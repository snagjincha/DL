[
  {
    "objectID": "posts/Predict_Underlying.html",
    "href": "posts/Predict_Underlying.html",
    "title": "Predict Underlying",
    "section": "",
    "text": "1. Imports\n\nimport torch\nimport matplotlib.pyplot as plt \n\n\n\n2. 회귀분석\n- 목적은 아래와 같이 데이터들에 잘 맞는 회귀선을 찾는 것\n\ntorch.manual_seed(21345)\nones= torch.ones(100).reshape(-1,1)\nx,_ = torch.randn(100).sort()\nx = x.reshape(-1,1)\nX = torch.concat([ones,x],axis=-1)\nW = torch.tensor([[2.5],[4]])\nϵ = torch.randn(100).reshape(-1,1)*0.5\ny = X@W + ϵ\n\n\nplt.plot(x,y,'o')\nplt.plot(x,2.5+4*x,'--')\n\n\n\n\n\n\n\n\n- 회귀모형에서 학습이란 위의 그림처럼 주황색 점선을 더 정확하게 추정하는 것\n\n\n3. 학습\n임의의 W값을 생각하자 그 다음 더 좋은 W값을 찾아서 업데이트 하면 되지 않을까?\n- Step1: Data\n데이터를 torch.tensor의 2차원 배열로 바꿔준다 .reshape(-1,1)을 사용 &lt;- 행렬계산을 해야하기에 2차원배열로 바꾸는 것\n- Step2: Update\n업데이트를 할 건데 뭘 어떻게 좋게 만들 것인지 기준을 잡아야한다. -&gt; Loss!(SSE)\n회귀직선이 정확할 수록 Loss가 작다.\n그렇다면 Loss값을 가장 작게 하는 W를 어떻게 찾을까?\nStep2-1: update의 정도를 파악하고 수정하는 과정\n\n임의의 점 \\(\\hat{w}\\) 를 찍는다.\n2.loss(W)의 기울기를 계산\n\\(\\hat{w}\\) - \\(\\alpha\\) \\(\\frac{d}{dW}loss(W)\\)\n\nStep2-2: \\(\\hat{w}\\) 수정\nWbefore = What.data\nWafter = What.data - \\(\\alpha\\) * What.grad\n- Step3: 미분\nloss.backward()를 사용하면 What.grad()에 값이 생긴다. 곧 미분값\n- Step4: iteration\nfor문을 사용하여 반복학습 해준다\n\n\n4. 각 Step의 변형\n- Step2의 변형\nMSELoss()를 이용한다.\n- Step1의 변형\nnet오브젝트를 이용하여 원활한 학습을 위한 데이터 정리를 해준다.\n- Step4의 변형\noptimizer 오브젝트를 이용하여 학습을 진행한다.\n\ntemp = [-2.4821, -2.3621, -1.9973, -1.6239, -1.4792, -1.4635, -1.4509, -1.4435,\n        -1.3722, -1.3079, -1.1904, -1.1092, -1.1054, -1.0875, -0.9469, -0.9319,\n        -0.8643, -0.7858, -0.7549, -0.7421, -0.6948, -0.6103, -0.5830, -0.5621,\n        -0.5506, -0.5058, -0.4806, -0.4738, -0.4710, -0.4676, -0.3874, -0.3719,\n        -0.3688, -0.3159, -0.2775, -0.2772, -0.2734, -0.2721, -0.2668, -0.2155,\n        -0.2000, -0.1816, -0.1708, -0.1565, -0.1448, -0.1361, -0.1057, -0.0603,\n        -0.0559, -0.0214,  0.0655,  0.0684,  0.1195,  0.1420,  0.1521,  0.1568,\n         0.2646,  0.2656,  0.3157,  0.3220,  0.3461,  0.3984,  0.4190,  0.5443,\n         0.5579,  0.5913,  0.6148,  0.6469,  0.6469,  0.6523,  0.6674,  0.7059,\n         0.7141,  0.7822,  0.8154,  0.8668,  0.9291,  0.9804,  0.9853,  0.9941,\n         1.0376,  1.0393,  1.0697,  1.1024,  1.1126,  1.1532,  1.2289,  1.3403,\n         1.3494,  1.4279,  1.4994,  1.5031,  1.5437,  1.6789,  2.0832,  2.2444,\n         2.3935,  2.6056,  2.6057,  2.6632]\nsales= [-8.5420, -6.5767, -5.9496, -4.4794, -4.2516, -3.1326, -4.0239, -4.1862,\n        -3.3403, -2.2027, -2.0262, -2.5619, -1.3353, -2.0466, -0.4664, -1.3513,\n        -1.6472, -0.1089, -0.3071, -0.6299, -0.0438,  0.4163,  0.4166, -0.0943,\n         0.2662,  0.4591,  0.8905,  0.8998,  0.6314,  1.3845,  0.8085,  1.2594,\n         1.1211,  1.9232,  1.0619,  1.3552,  2.1161,  1.1437,  1.6245,  1.7639,\n         1.6022,  1.7465,  0.9830,  1.7824,  2.1116,  2.8621,  2.1165,  1.5226,\n         2.5572,  2.8361,  3.3956,  2.0679,  2.8140,  3.4852,  3.6059,  2.5966,\n         2.8854,  3.9173,  3.6527,  4.1029,  4.3125,  3.4026,  3.2180,  4.5686,\n         4.3772,  4.3075,  4.4895,  4.4827,  5.3170,  5.4987,  5.4632,  6.0328,\n         5.2842,  5.0539,  5.4538,  6.0337,  5.7250,  5.7587,  6.2020,  6.5992,\n         6.4621,  6.5140,  6.6846,  7.3497,  8.0909,  7.0794,  6.8667,  7.4229,\n         7.2544,  7.1967,  9.5006,  9.0339,  7.4887,  9.0759, 11.0946, 10.3260,\n        12.2665, 13.0983, 12.5468, 13.8340]\n\nx = torch.tensor(temp).reshape(-1,1)\nones = torch.ones(100).reshape(-1,1)\nX = torch.concat([ones,x],axis=1)\ny = torch.tensor(sales).reshape(-1,1)\n\nnet = torch.nn.Linear(\n    in_features=2,\n    out_features=1,\n    bias=False\n)\nnet.weight.data = torch.tensor([[-5.0,  10.0]])\n\nloss_fn = torch.nn.MSELoss()\n\noptimizr = torch.optim.SGD(net.parameters(),lr=0.1)\nfor epoc in range(30):\n    # step1: yhat \n    yhat = net(X)\n    # step2: loss\n    loss = loss_fn(yhat,y)\n    # step3: 미분\n    loss.backward()\n    # step4: update\n    optimizr.step()\n    optimizr.zero_grad()\n\n\nplt.plot(x,y,'o')\nplt.plot(x,yhat.data,'--')\nplt.title(f'net.weight={net.weight.data.reshape(-1)}');\n\n\n\n\n\n\n\n\n\n\n5. 로지스틱\n\nx = torch.tensor([-6,-5,-4,-3,-2,-1, 0, 1, 2, 3, 4, 5, 6.0]).reshape(-1,1)\ny = torch.tensor([ 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1]).reshape(-1,1)\nplt.plot(x,y,'o')\n\n\n\n\n\n\n\n\n- 위와 같은 범주형 문제는 이전의 문제처럼 회귀문제로 생각하는 건 쉽지 않아 보인다. 로지스틱으로 해볼까?\n우리가 예측하고 싶은 건 underlying이다.\n\nx = torch.linspace(-1,1,2000).reshape(2000,1)\nw0 = -1\nw1 = 5\nu = w0 + x*w1 \nv = torch.exp(u) / (1+torch.exp(u)) # logistic\ny = torch.bernoulli(v) # 베르누이\n\nnet = torch.nn.Sequential(\n    torch.nn.Linear(in_features=1, out_features=1),\n    torch.nn.Sigmoid()\n)\nl1, a1 = net\nl1.weight.data = torch.tensor([[-0.3]])\nl1.bias.data = torch.tensor([-0.8])\noptimizr = torch.optim.SGD(net.parameters(),lr=0.25)\n#---#\nfor epoc in range(100):\n    ## 1\n    yhat = net(x) \n    ## 2 \n    loss = torch.mean((y-yhat)**2)\n    ## 3\n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\n\n\nplt.plot(x,y,'.',alpha=0.05)\nplt.plot(x,v,'--r')\nplt.plot(x,yhat.data,'--b')\nplt.title('after 100 epochs')\n\nText(0.5, 1.0, 'after 100 epochs')\n\n\n\n\n\n\n\n\n\n5000번 학습하면?\n\nx = torch.linspace(-1,1,2000).reshape(2000,1)\nw0 = -1\nw1 = 5\nu = w0 + x*w1 \nv = torch.exp(u) / (1+torch.exp(u)) # logistic\ny = torch.bernoulli(v) # 베르누이\n\nnet = torch.nn.Sequential(\n    torch.nn.Linear(in_features=1, out_features=1),\n    torch.nn.Sigmoid()\n)\nl1, a1 = net\nl1.weight.data = torch.tensor([[-0.3]])\nl1.bias.data = torch.tensor([-0.8])\noptimizr = torch.optim.SGD(net.parameters(),lr=0.25)\n#---#\nfor epoc in range(5000):\n    ## 1\n    yhat = net(x) \n    ## 2 \n    loss = torch.mean((y-yhat)**2)\n    ## 3\n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\n\n\nplt.plot(x,y,'.',alpha=0.05)\nplt.plot(x,v,'--r')\nplt.plot(x,yhat.data,'--b')\nplt.title('after 100 epochs')\n\nText(0.5, 1.0, 'after 100 epochs')\n\n\n\n\n\n\n\n\n\n\n\n6. 초기값의 중요성\n\nimport torch\nimport matplotlib.pyplot as plt \nimport numpy as np\nimport pandas as pd\n\ndef plot_loss(loss_fn, ax=None, Wstar=[-1,5]):\n    w0hat,w1hat =torch.meshgrid(torch.arange(-10,3,0.1),torch.arange(-1,10,0.1),indexing='ij')\n    w0hat = w0hat.reshape(-1)\n    w1hat = w1hat.reshape(-1)\n    def l(w0hat,w1hat):\n        yhat = torch.exp(w0hat+w1hat*x)/(1+torch.exp(w0hat+w1hat*x))\n        return loss_fn(yhat,y) \n    loss = list(map(l,w0hat,w1hat))\n    #---#\n    if ax is None: \n        fig = plt.figure()\n        ax = fig.add_subplot(1,1,1,projection='3d')\n    ax.scatter(w0hat,w1hat,loss,s=0.001) \n    ax.scatter(w0hat[::20],w1hat[::20],loss[::20],s=0.1,color='C0') \n    w0star,w1star = np.array(Wstar).reshape(-1)\n    ax.scatter(w0star,w1star,l(w0star,w1star),s=200,marker='*',color='red',label=f\"W=[{w0star:.1f},{w1star:.1f}]\")\n    #---#\n    ax.elev = 15\n    ax.dist = -20\n    ax.azim = 75    \n    ax.legend()\n    ax.set_xlabel(r'$w_0$')  # x축 레이블 설정\n    ax.set_ylabel(r'$w_1$')  # y축 레이블 설정\n    ax.set_xticks([-10,-5,0])  # x축 틱 간격 설정\n    ax.set_yticks([-10,0,10])  # y축 틱 간격 설정\n\n\ndef learn_and_record(net, loss_fn, optimizr):\n    yhat_history = [] \n    loss_history = []\n    What_history = []\n    Whatgrad_history = []\n    What_history.append([net[0].bias.data.item(), net[0].weight.data.item()])\n    for epoc in range(100): \n        ## step1 \n        yhat = net(x)\n        ## step2 \n        loss = loss_fn(yhat,y)\n        ## step3\n        loss.backward() \n        ## step4 \n        optimizr.step()\n        ## record \n        if epoc % 5 ==0: \n            yhat_history.append(yhat.reshape(-1).data.tolist())\n            loss_history.append(loss.item())\n            What_history.append([net[0].bias.data.item(), net[0].weight.data.item()])\n            Whatgrad_history.append([net[0].bias.grad.item(), net[0].weight.grad.item()])\n        optimizr.zero_grad() \n        \n    return yhat_history, loss_history, What_history, Whatgrad_history\n\n\ndef show_animation(net, loss_fn, optimizr):\n    yhat_history,loss_history,What_history,Whatgrad_history = learn_and_record(net,loss_fn,optimizr)\n    \n    fig = plt.figure(figsize=(7.5,3.5))\n    ax1 = fig.add_subplot(1, 2, 1)\n    ax2 = fig.add_subplot(1, 2, 2, projection='3d')\n    ## ax1: 왼쪽그림 \n    ax1.scatter(x,y,alpha=0.01)\n    ax1.scatter(x[0],y[0],color='C0',label=r\"observed data = $(x_i,y_i)$\")\n    ax1.plot(x,v,'--',label=r\"prob (true) = $(x_i,\\frac{exp(-1+5x_i)}{1+exp(-1+5x_i)})$\")    \n    line, = ax1.plot(x,yhat_history[0],'--',label=r\"prob (estimated) = $(x_i,\\hat{y}_i)$\") \n    ax1.legend()\n    ## ax2: 오른쪽그림 \n    plot_loss(loss_fn,ax2)\n    ax2.scatter(np.array(What_history)[0,0],np.array(What_history)[0,1],loss_history[0],color='blue',s=200,marker='*')    \n    def animate(epoc):\n        line.set_ydata(yhat_history[epoc])\n        w0hat = np.array(What_history)[epoc,0]\n        w1hat = np.array(What_history)[epoc,1]\n        w0hatgrad = np.array(Whatgrad_history)[epoc,0]\n        w1hatgrad = np.array(Whatgrad_history)[epoc,1]\n        ax2.scatter(w0hat,w1hat,loss_history[epoc],color='grey')\n        ax2.set_title(f\"What.grad=[{w0hatgrad:.4f},{w1hatgrad:.4f}]\",y=0.8)\n        fig.suptitle(f\"epoch={epoc*5} // What=[{w0hat:.2f},{w1hat:.2f}] // Loss={loss_fn.__class__.__name__} // Opt={optimizr.__class__.__name__}\")\n        return line\n    ani = animation.FuncAnimation(fig, animate, frames=20)    \n    plt.close()\n    return ani\n\n- 좋은 초기값\n\nfrom matplotlib import animation\nplt.rcParams[\"animation.html\"] = \"jshtml\"\n\nnet = torch.nn.Sequential(\n    torch.nn.Linear(1,1),\n    torch.nn.Sigmoid()\n) \nnet[0].bias.data = torch.tensor([-0.8])\nnet[0].weight.data = torch.tensor([[-0.3]])\nloss_fn = torch.nn.MSELoss()\noptimizr = torch.optim.SGD(net.parameters(),lr=0.25) \n#---#\nshow_animation(net,loss_fn,optimizr)\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n- 최악의 초기값\n\nnet = torch.nn.Sequential(\n    torch.nn.Linear(1,1),\n    torch.nn.Sigmoid()\n) \nnet[0].bias.data = torch.tensor([-10.0])\nnet[0].weight.data = torch.tensor([[-1.0]])\nloss_fn = torch.nn.MSELoss()\noptimizr = torch.optim.SGD(net.parameters(),lr=0.25) \n#---#\nshow_animation(net,loss_fn,optimizr)\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n\n\n7. 손실함수의 개선\nLoss 중 BCELoss라는 것이 있는데 이것은 반복을 했을 때 원하는 만큼 움직이지 않는다면 가속도를 붙혀서 그 다음 반복시에는 더 많이 움직이게끔 방식이다.\n여기서 움직이게 한다는 건 Loss를 줄여서 적합을 시킨다는 의미이다.\n사용방법은 loss를 선언할 때 BCELoss() 라고 쓰면 된다\n\nnet = torch.nn.Sequential(\n    torch.nn.Linear(in_features=1, out_features=1),\n    torch.nn.Sigmoid()\n)\nl1, a1 = net # 네트워크는 섭스크립터블 오브젝트이니까..\nl1.weight.data = torch.tensor([[-0.3]])\nl1.bias.data = torch.tensor([-0.8])\nloss_fn = torch.nn.BCELoss()\noptimizr = torch.optim.SGD(net.parameters(),lr=0.25)\n#---#\nfor epoc in range(100):\n    ## 1\n    yhat = net(x) \n    ## 2 \n    loss = loss_fn(yhat,y) # yhat부터 써야함\n    ## 3\n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\n\n\nplt.plot(x,y,'.',alpha=0.05)\nplt.plot(x,v,'--r')\nplt.plot(x,yhat.data,'--b')\nplt.title('after 100 epochs')\n\nText(0.5, 1.0, 'after 100 epochs')\n\n\n\n\n\n\n\n\n\n- MSELoss를 사용할 때보다 BCELoss를 사용했을 때 훨씬 더 적합이 되는 듯한 느낌이다.\n\nfig = plt.figure()\nax1 = fig.add_subplot(1,2,1,projection='3d')\nax2 = fig.add_subplot(1,2,2,projection='3d')\nplot_loss(torch.nn.MSELoss(),ax1)\nplot_loss(torch.nn.BCELoss(),ax2)\n\n\n\n\n\n\n\n\n뭔가 더 잘 떨어지게끔 오른쪽이 미끄럼틀같이 펴져있다.\n- MSELoss\n\nnet = torch.nn.Sequential(\n    torch.nn.Linear(1,1),\n    torch.nn.Sigmoid()\n) \nnet[0].bias.data = torch.tensor([-0.8])\nnet[0].weight.data = torch.tensor([[-0.3]])\nloss_fn = torch.nn.MSELoss()\noptimizr = torch.optim.SGD(net.parameters(),lr=0.25) \n#---#\nshow_animation(net,loss_fn,optimizr)\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n- BCELoss\n\nnet = torch.nn.Sequential(\n    torch.nn.Linear(1,1),\n    torch.nn.Sigmoid()\n) \nnet[0].bias.data = torch.tensor([-0.8])\nnet[0].weight.data = torch.tensor([[-0.3]])\nloss_fn = torch.nn.BCELoss()\noptimizr = torch.optim.SGD(net.parameters(),lr=0.25) \n#---#\nshow_animation(net,loss_fn,optimizr)\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n\n\n8. 옵티마이저의 개선\n- MSELoss + SGD\n\nnet = torch.nn.Sequential(\n    torch.nn.Linear(1,1),\n    torch.nn.Sigmoid()\n) \nnet[0].bias.data = torch.tensor([-10.0])\nnet[0].weight.data = torch.tensor([[-1.0]])\nloss_fn = torch.nn.MSELoss()\noptimizr = torch.optim.SGD(net.parameters(),lr=0.05) \n#---#\nshow_animation(net,loss_fn,optimizr)\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n꿈쩍도 하지 않는 모습…\n- MSELoss + Adam\n\nnet = torch.nn.Sequential(\n    torch.nn.Linear(1,1),\n    torch.nn.Sigmoid()\n) \nnet[0].bias.data = torch.tensor([-10.0])\nnet[0].weight.data = torch.tensor([[-1.0]])\nloss_fn = torch.nn.MSELoss()\noptimizr = torch.optim.Adam(net.parameters(),lr=0.25) \n#---#\nshow_animation(net,loss_fn,optimizr)\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n너무나도 뚝뚝 잘 내려온다\n- 그럼 BCELoss + Adam 쓰면 무적인가?\n\n\n9.로지스틱의 한계\n\ndf = pd.read_csv(\"https://raw.githubusercontent.com/guebin/DL2024/main/posts/dnnex.csv\")\nx = torch.tensor(df.x).float().reshape(-1,1)\ny = torch.tensor(df.y).float().reshape(-1,1)\nprob = torch.tensor(df.prob).float().reshape(-1,1)\nplt.plot(x,y,'o',alpha=0.02)\nplt.plot(x[0],y[0],'o',label= r\"observed data = $(x_i,y_i)$\",color=\"C0\")\nplt.plot(x,prob,'--b',label= r\"prob (true, unknown)\")\nplt.legend()\n\n\n\n\n\n\n\n\n- 이런 꺾인 선은 로지스틱으로 적합하기 힘들어…\n\nnet = torch.nn.Sequential(\n    torch.nn.Linear(1,1),\n    torch.nn.Sigmoid()\n)\nloss_fn = torch.nn.BCELoss()\noptimizr = torch.optim.Adam(net.parameters())\n#---# \nfor epoc in range(5000):\n    ## 1 \n    yhat = net(x)\n    ## 2 \n    loss = loss_fn(yhat,y)\n    ## 3 \n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\n\n\nplt.plot(x,y,'o',alpha=0.02)\nplt.plot(x[0],y[0],'o',label= r\"observed data = $(x_i,y_i)$\",color=\"C0\")\nplt.plot(x,prob,'--b',label= r\"prob (true, unknown)\")\nplt.plot(x,net(x).data, '--', label= r\"prob (estimated) = $(x_i,\\hat{y}_i)$\")\nplt.legend()\n\n\n\n\n\n\n\n\n\n\n10.로지스틱의 한계 극복\nReLU : 음수인 것은 0으로 만들고 양수는 그대로 둔다. 그럼 그래프가 꺾인다.\n\ny = x*0 \ny[x&lt;0] = (9*x+4.5)[x&lt;0]\ny[x&gt;0] = (-4.5*x+4.5)[x&gt;0]\n\nplt.plot(y,'--')\n\n\n\n\n\n\n\n\n\ndf = pd.read_csv(\"https://raw.githubusercontent.com/guebin/DL2024/main/posts/dnnex.csv\")\nx = torch.tensor(df.x).float().reshape(-1,1)\ny = torch.tensor(df.y).float().reshape(-1,1)\nprob = torch.tensor(df.prob).float().reshape(-1,1)\n\ntorch.manual_seed(43052)\nnet = torch.nn.Sequential(\n    torch.nn.Linear(1,2),\n    torch.nn.ReLU(),\n    torch.nn.Linear(2,1),\n    torch.nn.Sigmoid()\n)\nloss_fn = torch.nn.BCELoss()\noptimizr = torch.optim.Adam(net.parameters())\n#---#\nfor epoc in range(3000):\n    ## \n    yhat = net(x)\n    ## \n    loss = loss_fn(yhat,y)\n    ## \n    loss.backward()\n    ## \n    optimizr.step()\n    optimizr.zero_grad()\n\n\nplt.plot(x,y,'o',alpha=0.02)\nplt.plot(x[0],y[0],'o',label= r\"observed data = $(x_i,y_i)$\",color=\"C0\")\nplt.plot(x,prob,'--b',label= r\"prob (true, unknown)\")\nplt.plot(x,net(x).data,'--',label=\"prob (estimated) -- after 3000 epochs\")\nplt.legend()\n\n\n\n\n\n\n\n\n\nfor epoc in range(3000):\n    ## \n    yhat = net(x)\n    ## \n    loss = loss_fn(yhat,y)\n    ## \n    loss.backward()\n    ## \n    optimizr.step()\n    optimizr.zero_grad()\n\n\nplt.plot(x,y,'o',alpha=0.02)\nplt.plot(x[0],y[0],'o',label= r\"observed data = $(x_i,y_i)$\",color=\"C0\")\nplt.plot(x,prob,'--b',label= r\"prob (true, unknown)\")\nplt.plot(x,net(x).data,'--',label=\"prob (estimated) -- after 6000 epochs\")\nplt.legend()\n\n\n\n\n\n\n\n\nReLU에 대해서는 깊게 알기보단 그래프를 꺾을 수 있다\n그 꺾는다는 의미는 표현력을 높인다고 생각하는 것이 좋다.\n로지스틱의 한계를 마주했을 때 그저 증가함수는 표현을 못 하는 그래프들이 있기때문에 문제였는데\n이제 ReLU를 이용해서 꺾인 선 그래프도 그릴 수 있게 되었고 그것으로 적합해서 맞춰나가면 해결할 수 있게 되었다.\n즉 그래프의 표현력이 늘었다. 그렇다면 모든 그래프도 적합할 수 있을 거 같은데?\n\ntorch.manual_seed(21345)\nx = torch.linspace(0,2,2000).reshape(-1,1)\neps = torch.randn(2000).reshape(-1,1)*0.05\nfx = torch.exp(-1*x)* torch.abs(torch.cos(3*x))*(torch.sin(3*x))\ny = fx + eps\n\nplt.plot(x,y,label=r\"observed data (with error): $(x_i,y_i)$\", alpha=0.2)\nplt.plot(x,fx,'--',color=\"C0\",label=r\"underlying (true, unknown): $e^{-x}|\\cos(3x)|\\sin(3x)$\")\nplt.legend()\n\n\n\n\n\n\n\n\n위의 문제는 로지스틱이 아니라 회귀선을 맞춰보는 느낌이 강하기에 MSELoss를 사용\n\n#torch.manual_seed(43052)\nnet = torch.nn.Sequential(\n    torch.nn.Linear(1,1024),\n    torch.nn.ReLU(),\n    torch.nn.Linear(1024,1)\n)\nloss_fn = torch.nn.MSELoss()\noptimizr = torch.optim.Adam(net.parameters())\n#--#\nfor epoc in range(5000):\n    ## 1\n    yhat = net(x)\n    ## 2\n    loss = loss_fn(yhat,y)\n    ## 3\n    loss.backward()\n    ## 4\n    optimizr.step()\n    optimizr.zero_grad()\n\n\nplt.plot(x,y,label=r\"observed data: $(x_i,y_i)$\", alpha=0.2)\nplt.plot(x,fx,'--',color=\"C0\",label=r\"underlying (true, unkown): $e^{-x}|\\cos(3x)|\\sin(3x)$\")\nplt.plot(x,yhat.data,'--',color=\"C1\",label=r\"underlying (esimated): $(x_i,\\hat{y}_i)$\")\nplt.legend()\n\n\n\n\n\n\n\n\n- 이렇게 복잡한 것도 잘 맞춰준다.\n\n\n11. 그럼 무적인가? (시벤코정리)\n치명적인 문제 : Overfiiting\n표현력은 무한하지만 오차까지 적합해버리는 문제가 있다\n이러한 문제를 해결하기 위해 드랍아웃이라는 기법이 존재함\n\nu = torch.randn(20).reshape(10,2)\nu\n\ntensor([[ 0.9303, -1.4108],\n        [ 0.1033,  0.3010],\n        [-0.3732,  2.3353],\n        [ 0.2152,  0.1908],\n        [-0.1696,  1.1762],\n        [-1.4862, -0.0343],\n        [-0.8385,  0.3719],\n        [ 0.9422,  0.9081],\n        [-1.3018, -0.7977],\n        [ 0.1008,  0.2014]])\n\n\n\nd = torch.nn.Dropout(0.9)\nd(u)\n\ntensor([[ 0.0000, -0.0000],\n        [ 1.0332,  0.0000],\n        [-0.0000,  0.0000],\n        [ 0.0000,  0.0000],\n        [-1.6961,  0.0000],\n        [-0.0000, -0.0000],\n        [-8.3853,  0.0000],\n        [ 0.0000,  0.0000],\n        [-0.0000, -0.0000],\n        [ 0.0000,  2.0141]])\n\n\n- 드랍아웃\n90%의 드랍아웃: 드랍아웃층의 입력 중 임의로 90%를 골라서 결과를 0으로 만든다. + 그리고 0이 되지않고 살아남은 값들은 10배 만큼 값이 커진다.\n의미: each iteration (each epoch x) 마다 학습에 참여하는 노드가 랜덤으로 결정됨.\n느낌: 모든 노드가 골고루 학습가능 + 한 두개의 특화된 능력치가 개발되기 보다 평균적인 능력치가 전반적으로 개선됨\n배깅과 랜덤포레스트의 느낌으로 오버피팅을 해결함"
  },
  {
    "objectID": "posts/About_hidden_size.html",
    "href": "posts/About_hidden_size.html",
    "title": "About hidden size",
    "section": "",
    "text": "import torch\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n\nsoft = torch.nn.Softmax(dim=1)\nohot = torch.nn.functional.one_hot"
  },
  {
    "objectID": "posts/About_hidden_size.html#a.-data",
    "href": "posts/About_hidden_size.html#a.-data",
    "title": "About hidden size",
    "section": "A. Data",
    "text": "A. Data\n\ntxt = list('abc'*100)\ntxt[:10]\n\n['a', 'b', 'c', 'a', 'b', 'c', 'a', 'b', 'c', 'a']\n\n\n\ndf_train = pd.DataFrame({'x': txt[:-1], 'y': txt[1:]})\ndf_train[:5]\n\n\nx = torch.tensor(df_train.x.map({'a':0,'b':1,'c':2}))\ny = torch.tensor(df_train.y.map({'a':0,'b':1,'c':2}))"
  },
  {
    "objectID": "posts/About_hidden_size.html#b.-mlp---하나의-은닉노드",
    "href": "posts/About_hidden_size.html#b.-mlp---하나의-은닉노드",
    "title": "About hidden size",
    "section": "B. MLP - 하나의 은닉노드",
    "text": "B. MLP - 하나의 은닉노드\n\ntorch.manual_seed(43052)\nnet = torch.nn.Sequential(\n    torch.nn.Embedding(3,1),\n    torch.nn.Tanh(), # -1,1사이로 값을 눌러준다\n    torch.nn.Linear(1,3), # 피처 뻥튀기 &lt;- 3개 중에 하나를 골라야하기 떄문에.\n    # torch.nn.Softmax()\n)\nloss_fn = torch.nn.CrossEntropyLoss()\noptimizr = torch.optim.Adam(net.parameters(),lr=0.1)\n\n\nfor epoc in range(50):\n    ## 1 \n    netout = net(x) \n    ## 2 \n    loss = loss_fn(netout,y)\n    ## 3 \n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\n\n\nyhat = soft(netout)\nyhat.argmax(axis=1),y\n\n(tensor([1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0,\n         1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0,\n         1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0,\n         1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0,\n         1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0,\n         1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0,\n         1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0,\n         1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0,\n         1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0,\n         1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0,\n         1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0,\n         1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0,\n         1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2]),\n tensor([1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0,\n         1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0,\n         1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0,\n         1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0,\n         1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0,\n         1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0,\n         1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0,\n         1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0,\n         1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0,\n         1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0,\n         1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0,\n         1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0,\n         1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2]))"
  },
  {
    "objectID": "posts/About_hidden_size.html#c.-적합결과의-해석",
    "href": "posts/About_hidden_size.html#c.-적합결과의-해석",
    "title": "About hidden size",
    "section": "C. 적합결과의 해석",
    "text": "C. 적합결과의 해석\n- 네트워크 분해\n\nebdd , tanh , linr = net\n\n- ebdd 레이어 통과 직후\n\nebdd_x = ebdd(x).data[:9]\nebdd_x\n\ntensor([[ 0.0287],\n        [ 1.8616],\n        [-2.8092],\n        [ 0.0287],\n        [ 1.8616],\n        [-2.8092],\n        [ 0.0287],\n        [ 1.8616],\n        [-2.8092]])\n\n\n\nplt.plot(ebdd_x, '--o', color='gray')\nplt.title(r\"$ebdd({\\bf x})$\")\nplt.title(r\"$ebdd({\\bf x})$\")\nplt.xticks(range(9),[r'$a\\to b$', r'$b\\to c$', r'$c\\to a$']*3);\n\n\n\n\n\n\n\n\n- ebdd -&gt; tanh 레이어 통과 직후\n\nebdd_x = ebdd(x).data[:9]\nh = tanh(ebdd(x)).data[:9]\ntorch.concat([ebdd_x,h],axis=1)\n\ntensor([[ 0.0287,  0.0287],\n        [ 1.8616,  0.9528],\n        [-2.8092, -0.9928],\n        [ 0.0287,  0.0287],\n        [ 1.8616,  0.9528],\n        [-2.8092, -0.9928],\n        [ 0.0287,  0.0287],\n        [ 1.8616,  0.9528],\n        [-2.8092, -0.9928]])\n\n\n\nfig,ax = plt.subplots(2,1,figsize=(8,8))\nax[0].plot(ebdd_x, '--o', color='gray')\nax[0].set_title(r\"$ebdd({\\bf x})$\")\nax[0].set_xticks(range(9),[r'$a\\to b$', r'$b\\to c$', r'$c\\to a$']*3)\nax[1].plot(h, '--o', color='gray')\nax[1].set_title(r\"${\\bf h}:=(tanh \\circ ebdd)({\\bf x})$\")\nax[1].set_xticks(range(9),[r'$a\\to b$', r'$b\\to c$', r'$c\\to a$']*3);\n\n\n\n\n\n\n\n\n\n여기까지 2개는 세트로 보는 것이 좋겠다.\n결과를 \\(h\\)로 생각하자.\n\n- Ebdd -&gt; tanh -&gt; linr 통과직후 - 여기에서 차원이 3차원으로 된다.\n\nebdd_x = ebdd(x).data[:9]\nh = tanh(ebdd(x)).data[:9]\nnetout = linr(tanh(ebdd(x))).data[:9]\n#netout = net(x).data[:9]\ntorch.concat([ebdd_x,h,netout],axis=1)\n\ntensor([[ 0.0287,  0.0287, -1.3637,  0.8247, -1.3384],\n        [ 1.8616,  0.9528, -4.9139,  1.2273,  2.9339],\n        [-2.8092, -0.9928,  2.5602,  0.3797, -6.0603],\n        [ 0.0287,  0.0287, -1.3637,  0.8247, -1.3384],\n        [ 1.8616,  0.9528, -4.9139,  1.2273,  2.9339],\n        [-2.8092, -0.9928,  2.5602,  0.3797, -6.0603],\n        [ 0.0287,  0.0287, -1.3637,  0.8247, -1.3384],\n        [ 1.8616,  0.9528, -4.9139,  1.2273,  2.9339],\n        [-2.8092, -0.9928,  2.5602,  0.3797, -6.0603]])\n\n\n\nnetout_a = netout[:,[0]]\nnetout_b = netout[:,[1]]\nnetout_c = netout[:,[2]]\n\n\nlinr.weight, linr.bias\n\n(Parameter containing:\n tensor([[-3.8416],\n         [ 0.4357],\n         [ 4.6228]], requires_grad=True),\n Parameter containing:\n tensor([-1.2536,  0.8122, -1.4709], requires_grad=True))\n\n\n\nnetout_a, h*(-3.8416) + (-1.2536)\n\n(tensor([[-1.3637],\n         [-4.9139],\n         [ 2.5602],\n         [-1.3637],\n         [-4.9139],\n         [ 2.5602],\n         [-1.3637],\n         [-4.9139],\n         [ 2.5602]]),\n tensor([[-1.3637],\n         [-4.9140],\n         [ 2.5602],\n         [-1.3637],\n         [-4.9140],\n         [ 2.5602],\n         [-1.3637],\n         [-4.9140],\n         [ 2.5602]]))\n\n\n\nfig,ax = plt.subplots(2,1,figsize=(8,8))\nax[0].plot(h, '--o', color='gray')\nax[0].set_title(r\"${\\bf h}:=(tanh \\circ ebdd)({\\bf x})$\")\nax[0].set_xticks(range(9),[r'$a\\to b$', r'$b\\to c$', r'$c\\to a$']*3);\nax[1].plot(netout_a, '--o', label=r\"$netout_a =  (-3.8416)\\times {\\bf h} + (-1.2536)$\")\nax[1].plot(netout_b, '--o', label=r\"$netout_b =  ( 0.4357)\\times {\\bf h} + (0.8122)$\")\nax[1].plot(netout_c, '--o', label=r\"$netout_c =  (4.6228)\\times {\\bf h} + (-1.4709)$\")\nax[1].set_xticks(range(9),[r'$a\\to b$', r'$b\\to c$', r'$c\\to a$']*3);\nax[1].legend()\n\n\n\n\n\n\n\n\n- 이 네트워크는 b-&gt;c 인 맵핑과 c-&gt;a인 맵핑은 확실하게 학습한 것 같지만 a-&gt;b인 맵핑은 그다지 잘 학습한 느낌이 들지 않는다.\n- 전체과정 시각화\n\nonehot_x = ohot(x).data[:9]\nonehot_a = onehot_x[:,[0]]\nonehot_b = onehot_x[:,[1]]\nonehot_c = onehot_x[:,[2]]\n#--#\nebdd_x = ebdd(x).data[:9]\n#--#\nh = tanh(ebdd(x)).data[:9]\n#--#\nnetout = linr(tanh(ebdd(x))).data[:9]\nnetout_a = netout[:,[0]]\nnetout_b = netout[:,[1]]\nnetout_c = netout[:,[2]]\n#--#\nyhat = soft(net(x)).data[:9]\nyhat_a = yhat[:,[0]]\nyhat_b = yhat[:,[1]]\nyhat_c = yhat[:,[2]]\n#--#\ntorch.concat([ebdd_x,h,netout,yhat],axis=1).numpy().round(2)\n\narray([[ 0.03,  0.03, -1.36,  0.82, -1.34,  0.09,  0.81,  0.09],\n       [ 1.86,  0.95, -4.91,  1.23,  2.93,  0.  ,  0.15,  0.85],\n       [-2.81, -0.99,  2.56,  0.38, -6.06,  0.9 ,  0.1 ,  0.  ],\n       [ 0.03,  0.03, -1.36,  0.82, -1.34,  0.09,  0.81,  0.09],\n       [ 1.86,  0.95, -4.91,  1.23,  2.93,  0.  ,  0.15,  0.85],\n       [-2.81, -0.99,  2.56,  0.38, -6.06,  0.9 ,  0.1 ,  0.  ],\n       [ 0.03,  0.03, -1.36,  0.82, -1.34,  0.09,  0.81,  0.09],\n       [ 1.86,  0.95, -4.91,  1.23,  2.93,  0.  ,  0.15,  0.85],\n       [-2.81, -0.99,  2.56,  0.38, -6.06,  0.9 ,  0.1 ,  0.  ]],\n      dtype=float32)\n\n\n\nfig, ax = plt.subplots(5,1,figsize=(8,16))\nax[0].set_title(r\"$onehot({\\bf x})$\")\nax[0].set_xticks(range(9),[r'$a\\to b$', r'$b\\to c$', r'$c\\to a$']*3)\nax[0].plot(onehot_a,'--o',label=\"a\")\nax[0].plot(onehot_b,'--o',label=\"b\")\nax[0].plot(onehot_c,'--o',label=\"c\")\nax[0].legend()\n#--#\nax[1].set_title(r\"$ebdd({\\bf x}):=(linr_{3 \\to 1} \\circ onehot)({\\bf x})$\")\nax[1].set_xticks(range(9),[r'$a\\to b$', r'$b\\to c$', r'$c\\to a$']*3)\nax[1].plot(ebdd_x, '--o', color='gray')\n#--#\nax[2].set_title(r\"${\\bf h}:=(tanh \\circ ebdd)({\\bf x})$\")\nax[2].set_xticks(range(9),[r'$a\\to b$', r'$b\\to c$', r'$c\\to a$']*3)\nax[2].plot(h, '--o', color='gray')\n#--#\nax[3].set_title(r\"$net({\\bf x})=(linr_{1 \\to 3} \\circ tanh \\circ ebdd)({\\bf x})$\")\nax[3].set_xticks(range(9),[r'$a\\to b$', r'$b\\to c$', r'$c\\to a$']*3)\nax[3].plot(netout_a,'--o',label=r\"$netout_a =  (-3.8416)\\times {\\bf h} + (-1.2536)$\")\nax[3].plot(netout_b,'--o',label=r\"$netout_b =  ( 0.4357)\\times {\\bf h} + (0.8122)$\")\nax[3].plot(netout_c,'--o',label=r\"$netout_c =  (4.6228)\\times {\\bf h} + (-1.4709)$\")\nax[3].legend()\n#--#\nax[4].set_title(r\"$\\hat{\\bf y} = soft(net({\\bf x}))$\")\nax[4].set_xticks(range(9),[r'$a\\to b$', r'$b\\to c$', r'$c\\to a$']*3)\nax[4].plot(yhat_a,'--o',label=r\"$P_a$\")\nax[4].plot(yhat_b,'--o',label=r\"$P_b$\")\nax[4].plot(yhat_c,'--o',label=r\"$P_c$\")\nax[4].legend()\nplt.tight_layout()\n\n\n\n\n\n\n\n\n- 다른 방식의 시각화\n\nh = tanh(ebdd(x)).data[:9]\nnetout = linr(tanh(ebdd(x))).data[:9]\nyhat = soft(net(x)).data[:9]\nmat = torch.concat([h,netout,yhat],axis=1)\n#---#\nplt.matshow(mat,cmap=\"bwr\",vmin=-6,vmax=6)\nplt.colorbar()\nplt.axvline(0.5,color=\"lime\")\nplt.axvline(3.5,color=\"lime\")\nplt.xticks(ticks=[0,1,2,3,4,5,6],labels=[r\"$h$\",r\"$out_a$\",r\"$out_b$\",r\"$out_c$\",r\"$P_a$\",r\"$P_b$\",r\"$P_c$\"]);\n\n\n\n\n\n\n\n\n\n\\(h\\)에 해당하는 색이 하얀색인 것이 있는데 이것은 b가 선택될 때 이런 일이 발생함. 이것은 값이 0이라는 의미인데 linr(3,1)에서 weight값은 0으로 되고 bias만 남는다는 의미이다. 그러므로 특징을 잡기가 불리하다.\n따라서 \\(h\\)가 확실한 색을 가지고 있는 것이 유리하다. 그렇지만 확실한 색인 빨강과 파랑이 이미 차지된 상태라 어쩔 수 없이 0이 선택된 것이다."
  },
  {
    "objectID": "posts/About_hidden_size.html#d.-mlp---두개의-은닉노드",
    "href": "posts/About_hidden_size.html#d.-mlp---두개의-은닉노드",
    "title": "About hidden size",
    "section": "D. MLP - 두개의 은닉노드",
    "text": "D. MLP - 두개의 은닉노드\n- 적합\n\ntorch.manual_seed(43052)\nnet = torch.nn.Sequential(\n    torch.nn.Embedding(num_embeddings=3,embedding_dim=2), # 받아주는 차원이 2차원으로 늘어남\n    torch.nn.Tanh(),\n    torch.nn.Linear(in_features=2,out_features=3),\n    #torch.nn.Softmax(),\n)\nloss_fn = torch.nn.CrossEntropyLoss() \noptimizr = torch.optim.Adam(net.parameters(),lr=0.1)\n\n\nfor epoc in range(50):\n    ## 1 \n    netout = net(x)\n    ## 2 \n    loss = loss_fn(netout,y)\n    ## 3 \n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\n\n- 결과 시각화\n\nebdd,tanh,linr = net\nh = tanh(ebdd(x)).data[:9]\nnetout = linr(tanh(ebdd(x))).data[:9]\nyhat = soft(net(x)).data[:9]\nmat = torch.concat([h,netout,yhat],axis=1)\n#---#\nplt.matshow(mat,cmap=\"bwr\",vmin=-6,vmax=6)\nplt.colorbar()\nplt.axvline(1.5,color=\"lime\")\nplt.axvline(4.5,color=\"lime\")\nplt.xticks(ticks=[0,1,2,3,4,5,6,7],labels=[r\"$h_1$\",r\"$h_2$\",r\"$out_a$\",r\"$out_b$\",r\"$out_c$\",r\"$P_a$\",r\"$P_b$\",r\"$P_c$\"]);\n\n\n\n\n\n\n\n\n\nx=a -&gt; h = (파,빨) -&gt; y=b\nx=b -&gt; h = (빨,파) -&gt; y=c\nx=c -&gt; h = (빨,빨) -&gt; y=a\nh=(파,파)는 아직 사용되지 않음. 즉 d를 하나 더 쓸 수 있는 공간이 남아있다는 의미이다."
  },
  {
    "objectID": "posts/About_hidden_size.html#a.-data-1",
    "href": "posts/About_hidden_size.html#a.-data-1",
    "title": "About hidden size",
    "section": "A. Data",
    "text": "A. Data\n- d 하나 더 써보자.\n\ntxt = list('abcd'*100)\ntxt[:10]\n\n['a', 'b', 'c', 'd', 'a', 'b', 'c', 'd', 'a', 'b']\n\n\n\ndf_train = pd.DataFrame({'x':txt[:-1], 'y':txt[1:]})\ndf_train[:5]\n\n\n\n\n\n\n\n\n\nx\ny\n\n\n\n\n0\na\nb\n\n\n1\nb\nc\n\n\n2\nc\nd\n\n\n3\nd\na\n\n\n4\na\nb\n\n\n\n\n\n\n\n\n\nx = torch.tensor(df_train.x.map({'a':0, 'b':1, 'c':2, 'd':3}))\ny = torch.tensor(df_train.y.map({'a':0, 'b':1, 'c':2, 'd':3}))"
  },
  {
    "objectID": "posts/About_hidden_size.html#b.-mlp---하나의-은닉노드-1",
    "href": "posts/About_hidden_size.html#b.-mlp---하나의-은닉노드-1",
    "title": "About hidden size",
    "section": "B. MLP - 하나의 은닉노드",
    "text": "B. MLP - 하나의 은닉노드\n- 학습\n\nnet = torch.nn.Sequential(\n    torch.nn.Embedding(num_embeddings=4,embedding_dim=1),\n    torch.nn.Tanh(),\n    torch.nn.Linear(in_features=1,out_features=4)\n)\nebdd,tanh,linr = net \nebdd.weight.data = torch.tensor([[-0.3333],[-2.5000],[5.0000],[0.3333]])\nlinr.weight.data = torch.tensor([[1.5000],[-6.0000],[-2.0000],[6.0000]])\nlinr.bias.data = torch.tensor([0.1500, -2.0000,  0.1500, -2.000])\nloss_fn = torch.nn.CrossEntropyLoss()\noptimizr = torch.optim.Adam(net.parameters(),lr=0.1)\n\n\nfor epoc in range(50):\n    ## 1 \n    netout = net(x)\n    ## 2 \n    loss = loss_fn(netout,y)\n    ## 3 \n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\n\n- 결과 시각화1\n\nonehot_x = ohot(x).data[:8]\nonehot_a = onehot_x[:,[0]]\nonehot_b = onehot_x[:,[1]]\nonehot_c = onehot_x[:,[2]]\nonehot_d = onehot_x[:,[3]]\n#--#\nebdd_x = ebdd(x).data[:8]\n#--#\nh = tanh(ebdd(x)).data[:8]\n#--#\nnetout = linr(tanh(ebdd(x))).data[:8]\nnetout_a = netout[:,[0]]\nnetout_b = netout[:,[1]]\nnetout_c = netout[:,[2]]\nnetout_d = netout[:,[3]]\n#--#\nyhat = soft(net(x)).data[:8]\nyhat_a = yhat[:,[0]]\nyhat_b = yhat[:,[1]]\nyhat_c = yhat[:,[2]]\nyhat_d = yhat[:,[3]]\n#--#\n\n\nfig, ax = plt.subplots(5,1,figsize=(8,16))\nax[0].set_title(r\"$onehot({\\bf x})$\")\nax[0].set_xticks(range(8),[r'$a\\to b$', r'$b\\to c$', r'$c\\to d$', r'$d\\to a$']*2)\nax[0].plot(onehot_a,'--o',label=\"a\")\nax[0].plot(onehot_b,'--o',label=\"b\")\nax[0].plot(onehot_c,'--o',label=\"c\")\nax[0].plot(onehot_d,'--o',label=\"d\")\nax[0].legend()\n#--#\nax[1].set_title(r\"$ebdd({\\bf x})$\")\nax[1].set_xticks(range(8),[r'$a\\to b$', r'$b\\to c$', r'$c\\to d$', r'$d\\to a$']*2)\nax[1].plot(ebdd_x, '--o', color='gray')\n#--#\nax[2].set_title(r\"${\\bf h}:=(tanh \\circ ebdd)({\\bf x})$\")\nax[2].set_xticks(range(8),[r'$a\\to b$', r'$b\\to c$', r'$c\\to d$', r'$d\\to a$']*2)\nax[2].plot(h, '--o', color='gray')\n#--#\nax[3].set_title(r\"$net({\\bf x})=(linr \\circ tanh \\circ ebdd)({\\bf x})$\")\nax[3].set_xticks(range(8),[r'$a\\to b$', r'$b\\to c$', r'$c\\to d$', r'$d\\to a$']*2)\nax[3].plot(netout_a,'--o',label=r\"$netout_a$\")\nax[3].plot(netout_b,'--o',label=r\"$netout_b$\")\nax[3].plot(netout_c,'--o',label=r\"$netout_c$\")\nax[3].plot(netout_d,'--o',label=r\"$netout_d$\")\nax[3].legend()\n#--#\nax[4].set_title(r\"$\\hat{\\bf y} = soft(net({\\bf x}))$\")\nax[4].set_xticks(range(8),[r'$a\\to b$', r'$b\\to c$', r'$c\\to d$', r'$d\\to a$']*2)\nax[4].plot(yhat_a,'--o',label=r\"$P_a$\")\nax[4].plot(yhat_b,'--o',label=r\"$P_b$\")\nax[4].plot(yhat_c,'--o',label=r\"$P_c$\")\nax[4].plot(yhat_d,'--o',label=r\"$P_d$\")\nax[4].legend()\nplt.tight_layout()\n\n\n\n\n\n\n\n\n- 결과 시각화2\n\nebdd,tanh,linr = net\nh = tanh(ebdd(x)).data[:8]\nnetout = linr(tanh(ebdd(x))).data[:8]\nyhat = soft(net(x)).data[:8]\nmat = torch.concat([h,netout,yhat],axis=1)\n#---#\nplt.matshow(mat,cmap=\"bwr\",vmin=-6,vmax=6)\nplt.colorbar()\nplt.axvline(0.5,color=\"lime\")\nplt.axvline(4.5,color=\"lime\")\nplt.xticks(ticks=[0,1,2,3,4,5,6,7,8],labels=[r\"$h$\",r\"$out_a$\",r\"$out_b$\",r\"$out_c$\",r\"$out_d$\",r\"$P_a$\",r\"$P_b$\",r\"$P_c$\",r\"$P_d$\"]);"
  },
  {
    "objectID": "posts/About_hidden_size.html#c.-mlp---두-개의-은닉노드",
    "href": "posts/About_hidden_size.html#c.-mlp---두-개의-은닉노드",
    "title": "About hidden size",
    "section": "C. MLP - 두 개의 은닉노드",
    "text": "C. MLP - 두 개의 은닉노드\n- 학습\n\ntorch.manual_seed(43052)\nnet = torch.nn.Sequential(\n    torch.nn.Embedding(num_embeddings=4,embedding_dim=2),\n    torch.nn.Tanh(),\n    torch.nn.Linear(in_features=2,out_features=4)\n)\nloss_fn = torch.nn.CrossEntropyLoss()\noptimizr = torch.optim.Adam(net.parameters(),lr=0.1)\n\n\nfor epoc in range(50):\n    ## 1 \n    netout = net(x)\n    ## 2 \n    loss = loss_fn(netout,y)\n    ## 3 \n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\n\n- 결과 시각화\n\nebdd,tanh,linr = net\nh = tanh(ebdd(x)).data[:8]\nnetout = linr(tanh(ebdd(x))).data[:8]\nyhat = soft(net(x)).data[:8]\nmat = torch.concat([h,netout,yhat],axis=1)\n#---#\nplt.matshow(mat,cmap=\"bwr\",vmin=-6,vmax=6)\nplt.colorbar()\nplt.axvline(1.5,color=\"lime\")\nplt.axvline(5.5,color=\"lime\")\nplt.xticks(ticks=range(10),labels=[r\"$h_1$\",r\"$h_2$\",r\"$out_a$\",r\"$out_b$\",r\"$out_c$\",r\"$out_d$\",r\"$P_a$\",r\"$P_b$\",r\"$P_c$\",r\"$P_d$\"]);"
  },
  {
    "objectID": "posts/About_hidden_size.html#d.-비교실험",
    "href": "posts/About_hidden_size.html#d.-비교실험",
    "title": "About hidden size",
    "section": "D. 비교실험",
    "text": "D. 비교실험\n- 은닉노드 1개\n\nclass Net1(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        ## 우리가 yhat을 구할때 사용할 레이어를 정의 \n        self.ebdd = torch.nn.Embedding(4,1)\n        self.tanh = torch.nn.Tanh()\n        self.linr = torch.nn.Linear(1,4)\n        ## 정의 끝\n    def forward(self,X):\n        ## yhat을 어떻게 구할것인지 정의 \n        ebdd_x = self.ebdd(x)\n        h = self.tanh(ebdd_x)\n        netout = self.linr(h)\n        ## 정의 끝\n        return netout\n\nfig, ax = plt.subplots(5,5,figsize=(10,10))\nfor i in range(5):\n    for j in range(5):\n        net = Net1()\n        optimizr = torch.optim.Adam(net.parameters(),lr=0.1)\n        loss_fn = torch.nn.CrossEntropyLoss()\n        for epoc in range(50):\n            ## 1 \n            netout = net(x)\n            ## 2 \n            loss = loss_fn(netout,y)\n            ## 3 \n            loss.backward()\n            ## 4 \n            optimizr.step()\n            optimizr.zero_grad()\n        h = net.tanh(net.ebdd(x)).data[:6]\n        yhat = soft(net(x)).data[:6]\n        mat = torch.concat([h,yhat],axis=1)\n        ax[i][j].matshow(mat,cmap='bwr',vmin=-1,vmax=1)\n        ax[i][j].axvline(0.5,color='lime')\n        ax[i][j].set_xticks(ticks=[0,1,2,3,4],labels=[r\"$h$\",r\"$P_a$\",r\"$P_b$\",r\"$P_c$\",r\"$P_d$\"])\nfig.suptitle(\"# of hidden nodes = 1\", size=20)\nfig.tight_layout()\n\n\n\n\n\n\n\n\n- 은닉노드 2개\n\nclass Net2(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        ## 우리가 yhat을 구할때 사용할 레이어를 정의 \n        self.ebdd = torch.nn.Embedding(4,2)\n        self.tanh = torch.nn.Tanh()\n        self.linr = torch.nn.Linear(2,4)\n        ## 정의 끝\n    def forward(self,X):\n        ## yhat을 어떻게 구할것인지 정의 \n        ebdd_x = self.ebdd(x)\n        h = self.tanh(ebdd_x)\n        netout = self.linr(h)\n        ## 정의 끝\n        return netout\n\nfig, ax = plt.subplots(5,5,figsize=(10,10))\nfor i in range(5):\n    for j in range(5):\n        net = Net2()\n        optimizr = torch.optim.Adam(net.parameters(),lr=0.1)\n        loss_fn = torch.nn.CrossEntropyLoss()\n        for epoc in range(50):\n            ## 1 \n            netout = net(x)\n            ## 2 \n            loss = loss_fn(netout,y)\n            ## 3 \n            loss.backward()\n            ## 4 \n            optimizr.step()\n            optimizr.zero_grad()\n        h = net.tanh(net.ebdd(x)).data[:6]\n        yhat = soft(net(x)).data[:6]\n        mat = torch.concat([h,yhat],axis=1)\n        ax[i][j].matshow(mat,cmap='bwr',vmin=-1,vmax=1)\n        ax[i][j].axvline(1.5,color='lime')\n        ax[i][j].set_xticks(ticks=[0,1,2,3,4,5],labels=[r\"$h_1$\",r\"$h_2$\",r\"$P_a$\",r\"$P_b$\",r\"$P_c$\",r\"$P_d$\"])\nfig.suptitle(\"# of hidden nodes = 2\", size=20)\nfig.tight_layout()"
  },
  {
    "objectID": "posts/DL_Summary4.html",
    "href": "posts/DL_Summary4.html",
    "title": "Deep Learning 4",
    "section": "",
    "text": "import torch \nimport torchvision\nimport fastai.vision.all\nimport matplotlib.pyplot as plt\nimport requests"
  },
  {
    "objectID": "posts/DL_Summary4.html#a.-dls",
    "href": "posts/DL_Summary4.html#a.-dls",
    "title": "Deep Learning 4",
    "section": "A. DLS",
    "text": "A. DLS\n\npath = fastai.data.external.untar_data(fastai.data.external.URLs.PETS)\nfnames = [l for l in (path/'images').ls() if str(l).split('.')[-1] == 'jpg']\n\n\ndef label_func(fname):\n    if fname[0].isupper():\n        return 'cat'\n    else:\n        return 'dog'\n\n\ndls = fastai.vision.data.ImageDataLoaders.from_name_func(\n    path = path/'images',\n    fnames = fnames,\n    label_func = label_func,\n    valid_pct = 0.2,\n    item_tfms = fastai.vision.augment.Resize(512),\n    bs = 32\n)\n\n\ndls.show_batch()"
  },
  {
    "objectID": "posts/DL_Summary4.html#b.-이미지-자료-불러오기",
    "href": "posts/DL_Summary4.html#b.-이미지-자료-불러오기",
    "title": "Deep Learning 4",
    "section": "B. 이미지 자료 불러오기",
    "text": "B. 이미지 자료 불러오기\n- torchvision 사용\n\nx = torchvision.io.read_image('/root/.fastai/data/oxford-iiit-pet/images/Sphynx_14.jpg')\n# x를 찍어보면 int형의 tensor형태로 되어있다\n\n- fastai 사용\npath를 이용하는 string \\(\\to\\) PILImage \\(\\to\\) fastai.torch_core.TensorImage \\(\\to\\) torch.tensor\n\nx_pil = fastai.vision.core.PILImage.create('/root/.fastai/data/oxford-iiit-pet/images/Sphynx_14.jpg')\nx = next(iter(dls.test_dl([x_pil])))[0]\n# x를 찍어보면 float형의 tensor형태로 되어있다"
  },
  {
    "objectID": "posts/DL_Summary4.html#c.-이미지-시각화",
    "href": "posts/DL_Summary4.html#c.-이미지-시각화",
    "title": "Deep Learning 4",
    "section": "C. 이미지 시각화",
    "text": "C. 이미지 시각화\n\nx_pil = fastai.vision.core.PILImage.create('/root/.fastai/data/oxford-iiit-pet/images/Sphynx_14.jpg')\nx = next(iter(dls.test_dl([x_pil])))[0] \nplt.imshow(torch.einsum('ocij -&gt; ijc' , x.to('cpu'))) # cuda에 있으면 시각화가 안 됨\n\n\n\n\n\n\n\n\n- 아무런 사진이나 하나 가져와서 시각화 해보기\n\nx_pil = fastai.vision.core.PILImage.create(requests.get('https://i.ytimg.com/vi/vc0aaS83cRo/maxresdefault.jpg').content)\nx = next(iter(dls.test_dl([x_pil])))[0]\nplt.imshow(torch.einsum('ocij -&gt; ijc' , x.to('cpu')))\n\n\n\n\n\n\n\n\niamge가 잘리는 이유 : dls에서 그림의 size를 512로 정해놨기때문에 512 x 512 image로 출력된다"
  },
  {
    "objectID": "posts/DL_Summary4.html#d.-ap-layer",
    "href": "posts/DL_Summary4.html#d.-ap-layer",
    "title": "Deep Learning 4",
    "section": "D. AP layer",
    "text": "D. AP layer\n\nap = torch.nn.AdaptiveAvgPool2d(output_size=1)\nap\n\nAdaptiveAvgPool2d(output_size=1)\n\n\n\nX = torch.arange(1*3*4*4).reshape(1,3,4,4)*1.0 \nX\n\ntensor([[[[ 0.,  1.,  2.,  3.],\n          [ 4.,  5.,  6.,  7.],\n          [ 8.,  9., 10., 11.],\n          [12., 13., 14., 15.]],\n\n         [[16., 17., 18., 19.],\n          [20., 21., 22., 23.],\n          [24., 25., 26., 27.],\n          [28., 29., 30., 31.]],\n\n         [[32., 33., 34., 35.],\n          [36., 37., 38., 39.],\n          [40., 41., 42., 43.],\n          [44., 45., 46., 47.]]]])\n\n\n\nap(X) #채널별로 평균을 구해줌\n\ntensor([[[[ 7.5000]],\n\n         [[23.5000]],\n\n         [[39.5000]]]])\n\n\n\nr,g,b = ap(X)[0]\n\n\nr, g ,b\n\n(tensor([[7.5000]]), tensor([[23.5000]]), tensor([[39.5000]]))"
  },
  {
    "objectID": "posts/DL_Summary4.html#e.-aplinear의-교환-linear는-행렬곱이다",
    "href": "posts/DL_Summary4.html#e.-aplinear의-교환-linear는-행렬곱이다",
    "title": "Deep Learning 4",
    "section": "E. AP,Linear의 교환 (Linear는 행렬곱이다)",
    "text": "E. AP,Linear의 교환 (Linear는 행렬곱이다)\n\nap(r)*0.1 + ap(g)*0.2 + ap(b)*0.3\n\ntensor([[17.3000]])\n\n\n\nap(r*0.1 + g*0.2 + b*0.3)\n\ntensor([[17.3000]])\n\n\n- 똑같은데요?\n위와 같은 과정을 Linear를 태울 때 l.weight와 같이 가중치를 적용하여서 합한다고 생각할 수 있다.\nl.weight.data = [0.1,0.2,0.3] 로 놓으면 생각하기가 조금 쉽다\n\nflttn = torch.nn.Flatten()\nl = torch.nn.Linear(3,1,bias=False)\nl.weight.data = torch.tensor([[0.1,0.2,0.3]]) # 행렬계산이기에 2차원 배열로 해야함\n\n우리가 쉽게 생각할 수 있는 1번째 방법\n\nl(flttn(ap(X)))\n\ntensor([[17.3000]], grad_fn=&lt;MmBackward0&gt;)\n\n\nLinear와 ap의 교환\n\nflttn(ap(torch.einsum('ocij,kc -&gt; okij' , X , l.weight.data)))\n\ntensor([[17.3000]])\n\n\n! Linear는 가중치를 곱해서 더하는 과정 즉, 행렬곱으로 이해하는 것이 좋다."
  },
  {
    "objectID": "posts/DL_Summary4.html#a.-1단계---이미지분류-잘하는-네트워크-선택-후-학습",
    "href": "posts/DL_Summary4.html#a.-1단계---이미지분류-잘하는-네트워크-선택-후-학습",
    "title": "Deep Learning 4",
    "section": "A. 1단계 - 이미지분류 잘하는 네트워크 선택 후 학습",
    "text": "A. 1단계 - 이미지분류 잘하는 네트워크 선택 후 학습\n\nlrnr = fastai.vision.learner.vision_learner(\n    dls = dls,\n    arch = fastai.vision.models.resnet34,\n    metrics = [fastai.metrics.accuracy]\n)\n\n\nlrnr.fine_tune(1) #lrnr.model의 마지막 부분만 학습시키는 걸 fine_tune이라고 함\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.087281\n0.012028\n0.995940\n00:19\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.051851\n0.010260\n0.996617\n00:26"
  },
  {
    "objectID": "posts/DL_Summary4.html#b.-2단계---네트워크-부분-수정-후-재학습",
    "href": "posts/DL_Summary4.html#b.-2단계---네트워크-부분-수정-후-재학습",
    "title": "Deep Learning 4",
    "section": "B. 2단계 - 네트워크 부분 수정 후 재학습",
    "text": "B. 2단계 - 네트워크 부분 수정 후 재학습\n\nnet1 = lrnr.model[0]\nnet2 = lrnr.model[1]\n\n\nnet2 = torch.nn.Sequential(\n    torch.nn.AdaptiveAvgPool2d(output_size=1),\n    torch.nn.Flatten(),\n    torch.nn.Linear(512,2,bias=False)\n)\n\n\nnet = torch.nn.Sequential(\n    net1,\n    net2\n) # 내가 만든(수정한) net\n\n\nlrnr2 = fastai.learner.Learner(\n    dls = dls,\n    model = net,\n    metrics = [fastai.metrics.accuracy]\n) # 나만의 net으로 새롭게 학습할 lrnr2\n\n\nlrnr.loss_func , lrnr2.loss_func # 정의하지 않아도 알아서 잘 들어가 있다\n\n(FlattenedLoss of CrossEntropyLoss(), FlattenedLoss of CrossEntropyLoss())\n\n\n\nlrnr2.fine_tune(5)\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.392073\n0.295664\n0.876861\n00:26\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.192747\n0.190745\n0.916779\n00:26\n\n\n1\n0.180415\n0.234511\n0.912720\n00:26\n\n\n2\n0.136703\n0.168526\n0.926252\n00:26\n\n\n3\n0.057196\n0.052161\n0.975643\n00:26\n\n\n4\n0.033394\n0.049159\n0.979702\n00:26"
  },
  {
    "objectID": "posts/DL_Summary4.html#c.-3단계---수정된-net2에서-linear-와-ap의-순서를-바꿈",
    "href": "posts/DL_Summary4.html#c.-3단계---수정된-net2에서-linear-와-ap의-순서를-바꿈",
    "title": "Deep Learning 4",
    "section": "C. 3단계 - 수정된 net2에서 linear 와 AP의 순서를 바꿈",
    "text": "C. 3단계 - 수정된 net2에서 linear 와 AP의 순서를 바꿈\n\nx_pil = fastai.vision.core.PILImage.create('/root/.fastai/data/oxford-iiit-pet/images/Sphynx_14.jpg')\nx = next(iter(dls.test_dl([x_pil])))[0]\nx_dec = dls.decode([x])[0]\nplt.imshow(torch.einsum('ocij -&gt; ijc', x_dec))\n\n\n\n\n\n\n\n\nnet2 순서 바꾸기 전 네트워크 진행\n\nap = lrnr2.model[-1][0]\nfl = lrnr2.model[-1][1]\nl = lrnr2.model[-1][2]\n\n\nl(fl(ap(net1(x)))) # 고양이!!\n\nTensorImage([[ 2.5441, -2.8503]], device='cuda:0', grad_fn=&lt;AliasBackward0&gt;)\n\n\n- net2 순서 바꾼 후 네트워크 진행\n\nap = lrnr2.model[-1][0]\nfl = lrnr2.model[-1][1]\nl = lrnr2.model[-1][2]\n\n1. 일단 net1(x)을 진행\n2. Linear를 먼저 해야하니 행렬곱\n3. ap\n4. flatten\n\nflttn(ap(torch.einsum('ocij,kc -&gt; okij' , net1(x) , l.weight.data))) #고양이!!!\n\nTensorImage([[ 2.5441, -2.8503]], device='cuda:0', grad_fn=&lt;AliasBackward0&gt;)\n\n\n- 그런데 왜 순서 바꾸는 거지..? 그냥 하면 안돼?\n- 순서를 바꾸면 데이터의 차원이 [1,2,?,?] 이런식으로 바뀌는데 2인 부분을 고양이 or 강아지 이런식으로 데이터를 바라볼 수 있다\n\nEXTRA. 잠깐… 인공지능이 뭘 보고 고양이인지 강아지인지 구분하는 거지?\n\nWHY = torch.einsum('ocij,kc -&gt; okij',net1(x),l.weight.data)\nWHY[0,0,:,:].int()\n\nTensorImage([[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n             [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n             [ 0,  0,  0,  1,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0, -1, -2],\n             [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n             [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n             [ 0,  0,  0,  0,  0,  0,  3, 11, 16, 12,  4,  0,  0,  0,  0,  0],\n             [ 0,  0,  0,  0,  0,  0,  7, 26, 39, 29,  9,  0,  0,  0,  0,  0],\n             [ 0,  0,  0,  0,  0,  0,  9, 34, 53, 37, 10,  0,  0,  0,  0,  0],\n             [ 0,  0,  0,  0,  0,  0,  7, 27, 39, 28,  9,  0,  0,  0,  0,  0],\n             [ 0,  0,  0,  0,  0, -1,  3, 10, 15, 10,  3,  1,  1,  0,  0,  0],\n             [ 0,  0,  0,  0,  0,  0,  2,  4,  4,  2,  0,  0,  0,  0,  1,  0],\n             [ 0,  0,  0,  0,  0,  0,  4,  6,  8,  3,  0,  0,  1,  1,  1,  1],\n             [ 0,  0,  0,  0,  1,  1,  2,  8, 11,  4,  0,  0,  0,  1,  1,  0],\n             [ 0,  0,  0,  0,  0,  0,  2,  6,  7,  4,  0,  0,  0,  0,  0,  0],\n             [ 0,  0,  0,  0,  0,  0,  1,  2,  2,  1,  0,  0,  0,  0,  0,  0],\n             [ 0,  0,  0,  0,  0,  0,  0,  1,  0,  0,  0,  0,  0,  0,  0,  0]],\n            device='cuda:0', dtype=torch.int32)\n\n\n- 가운데 쪽에 숫자들이 크다. 저 의미는 인공지능이 그림의 저 부분을 보고 고양이라고 판단했다는 의미\n\nWHY[0,1,:,:].int()\n\nTensorImage([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n                0,   0,   0],\n             [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n                0,   0,   0],\n             [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n                0,   0,   0],\n             [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n                1,   1,   0],\n             [  0,   0,   0,   0,   0,   0,   3,   8,   5,   1,   0,   0,   0,\n                0,   0,   0],\n             [  0,   0,   0,   0,  -3,  -9,  -8,   6,   5,   1,   0,   2,   3,\n                1,   0,   0],\n             [  0,   0,   0,  -1, -14, -42, -59, -42, -15,  -1,   0,   7,   4,\n                0,   0,   0],\n             [  0,   0,   0,  -2, -20, -65, -97, -72, -25,  -3,   0,   1,   1,\n                0,   0,   0],\n             [  0,   0,   0,  -2, -21, -64, -93, -71, -28,  -4,  -1,   0,   0,\n                0,   0,   0],\n             [  0,   0,   0,  -1, -12, -33, -45, -36, -15,  -3,  -1,  -1,   0,\n                0,   0,   1],\n             [  0,   0,   0,   0,  -3,  -8, -12,  -8,  -3,  -1,  -1,  -1,   0,\n                0,   0,   0],\n             [  0,   0,   0,   0,   0,  -2,  -5,  -1,   0,   0,   0,   0,   0,\n                0,   0,   0],\n             [  0,   0,   0,   0,   0,  -2,  -3,  -1,   0,   0,   0,   0,   0,\n                0,   0,   0],\n             [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n                0,   0,   0],\n             [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n                0,   0,   0],\n             [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n                0,   0,   0]], device='cuda:0', dtype=torch.int32)\n\n\n- 가운데 쪽에 숫자들이 작다. 저 의미는 인공지능이 그림의 저 부분을 보고 고양이가 아니라고 판단했다는 의미\n시각화 해보자\n\nWHYCAT = WHY[0,0,:,:].to('cpu').detach()\nWHYDOG = WHY[0,1,:,:].to('cpu').detach()\nx_dec = dls.decode([x])[0]\nfig , ax = plt.subplots(1,3,figsize=(8,4))\nax[0].imshow(torch.einsum('ocij -&gt; ijc',x_dec))\nax[1].imshow(WHYCAT , cmap='magma')\nax[2].imshow(WHYDOG , cmap='magma')\n\n\n\n\n\n\n\n\n\nfig ,ax = plt.subplots(1,2,figsize=(8,6))\nax[0].imshow(torch.einsum('ocij -&gt; ijc',x_dec))\nax[0].imshow(WHYCAT , cmap='magma',extent = (0,511,511,0) , interpolation = 'bilinear',alpha=0.5)\nax[1].imshow(torch.einsum('ocij -&gt; ijc',x_dec))\nax[1].imshow(WHYDOG , cmap='magma',extent = (0,511,511,0) , interpolation = 'bilinear',alpha=0.5)\n\n\n\n\n\n\n\n\n- 주로 눈을 보면서 고양이라고 판단하고 (1번째 그림의 해석) 주로 눈을 보면서 강아지가 아니라고 판단했다(2번째 그림의 해석)\n- 하니로 해보자\n\nx_pil = fastai.vision.core.PILImage.create(requests.get('https://github.com/guebin/DL2024/blob/main/imgs/01wk-hani1.jpeg?raw=true').content)\nx = next(iter(dls.test_dl([x_pil])))[0]\nx_dec = dls.decode([x])\nWHY = torch.einsum('ocij,kc -&gt; okij', net1(x), l.weight.data)\nWHYCAT = WHY[0,0,:,:].to(\"cpu\").detach()\nWHYDOG = WHY[0,1,:,:].to(\"cpu\").detach()\nsoftmax = torch.nn.Softmax(dim=1)\ncat_prob, dog_prob = softmax(flttn(ap(WHY))).to(\"cpu\").detach().tolist()[0]\n\n\nfig ,ax = plt.subplots(1,2,figsize=(8,6))\nax[0].imshow(torch.einsum('ocij -&gt; ijc',x_dec))\nax[0].imshow(WHYCAT , cmap='magma',extent = (0,511,511,0) , interpolation = 'bilinear',alpha=0.5)\nax[0].set_title(f'cat prob = {cat_prob:.6f}')\nax[1].imshow(torch.einsum('ocij -&gt; ijc',x_dec))\nax[1].imshow(WHYDOG , cmap='magma',extent = (0,511,511,0) , interpolation = 'bilinear',alpha=0.5)\nax[1].set_title(f'dog prob = {dog_prob:.6f}')\n\nText(0.5, 1.0, 'dog prob = 0.999923')"
  },
  {
    "objectID": "posts/DL_Summary4.html#d.-4단계---cam-시각화",
    "href": "posts/DL_Summary4.html#d.-4단계---cam-시각화",
    "title": "Deep Learning 4",
    "section": "D. 4단계 - CAM 시각화",
    "text": "D. 4단계 - CAM 시각화\n- 0~25번 사진\n\nfig,ax = plt.subplots(5,5)\nk = 0\nfor i in range(5):\n    for j in range(5):\n        x_pil = fastai.vision.core.PILImage.create(fnames[k])\n        x = next(iter(dls.test_dl([x_pil])))[0]\n        x_dec = dls.decode([x])\n        WHY = torch.einsum('ocij,kc -&gt; okij', net1(x), l.weight.data)\n        WHYCAT = WHY[0,0,:,:].to(\"cpu\").detach()\n        WHYDOG = WHY[0,1,:,:].to(\"cpu\").detach()\n        cat_prob, dog_prob = softmax(flttn(ap(WHY))).to(\"cpu\").detach().tolist()[0]\n        if cat_prob &gt; dog_prob:\n            ax[i][j].imshow(torch.einsum('ocij -&gt; ijc', x_dec))\n            ax[i][j].imshow(WHYCAT,cmap='magma',extent=(0,511,511,0),interpolation='bilinear',alpha=0.5)\n            ax[i][j].set_title(f\"cat({cat_prob:.2f})\")\n            ax[i][j].set_xticks([])\n            ax[i][j].set_yticks([])\n        else:\n            ax[i][j].imshow(torch.einsum('ocij -&gt; ijc', x_dec))\n            ax[i][j].imshow(WHYDOG,cmap='magma',extent=(0,511,511,0),interpolation='bilinear',alpha=0.5)\n            ax[i][j].set_title(f'dog({dog_prob:.2f})')\n            ax[i][j].set_xticks([])\n            ax[i][j].set_yticks([])\n        k = k+1\nfig.set_figheight(16)\nfig.set_figwidth(16)\nfig.tight_layout()\n\n\n\n\n\n\n\n\n- 26~50번 사진\n\nfig,ax = plt.subplots(5,5)\n#---#\nk=25\nfor i in range(5):\n    for j in range(5):\n        x_pil = fastai.vision.core.PILImage.create(fastai.data.transforms.get_image_files(path/'images')[k])\n        x = next(iter(dls.test_dl([x_pil])))[0] # 이걸로 WHY를 만들어보자. \n        x_dec = dls.decode([x])[0] # 이걸로 시각화 \n        WHY = torch.einsum('ocij,kc -&gt; okij',net1(x),l.weight.data)\n        WHYCAT = WHY[0,0,:,:].to(\"cpu\").detach()\n        WHYDOG = WHY[0,1,:,:].to(\"cpu\").detach()\n        cat_prob, dog_prob = softmax(flttn(ap(WHY))).to(\"cpu\").detach().tolist()[0]\n        if cat_prob &gt; dog_prob: \n            ax[i][j].imshow(torch.einsum('ocij -&gt; ijc',x_dec))\n            ax[i][j].imshow(WHYCAT,cmap='magma',extent = (0,511,511,0), interpolation='bilinear',alpha=0.5)\n            ax[i][j].set_title(f\"cat({cat_prob:.2f})\")\n            ax[i][j].set_xticks([])\n            ax[i][j].set_yticks([])\n        else: \n            ax[i][j].imshow(torch.einsum('ocij -&gt; ijc',x_dec))\n            ax[i][j].imshow(WHYDOG,cmap='magma',extent = (0,511,511,0), interpolation='bilinear',alpha=0.5)\n            ax[i][j].set_title(f\"dog({dog_prob:.2f})\")\n            ax[i][j].set_xticks([])\n            ax[i][j].set_yticks([])            \n        k=k+1\nfig.set_figheight(16)\nfig.set_figwidth(16)\nfig.tight_layout() \n\n\n\n\n\n\n\n\n중요 WHY를 만드는 과정에서 Linear는 가중치를 곱하여 더하는 행위이고 행렬곱으로 계산된다.\n즉, einsum을 이용하여 계산해주면 된다.\nl.weight.data의 값만 바꿔준다면 다른 가중치로도 계산할 수 있다."
  },
  {
    "objectID": "posts/DL_Summary5.html",
    "href": "posts/DL_Summary5.html",
    "title": "Deep Learning 5",
    "section": "",
    "text": "import torch \nimport torchvision\nimport fastai.vision.all \nimport fastai.vision.gan\nimport matplotlib.pyplot as plt"
  },
  {
    "objectID": "posts/DL_Summary5.html#a-1.-생성모형이란-쉬운-설명",
    "href": "posts/DL_Summary5.html#a-1.-생성모형이란-쉬운-설명",
    "title": "Deep Learning 5",
    "section": "A-1. 생성모형이란? 쉬운 설명",
    "text": "A-1. 생성모형이란? 쉬운 설명\n- 사진 속에 있는 고양이와 개를 분류하는 것보다 고양이 이미지와 개 이미지를 만드는 것이 더 어렵지 않은가?\n- 만들수 없다면 이해하지 못한 것이다 - 리처드 파인만"
  },
  {
    "objectID": "posts/DL_Summary5.html#a-2.-수학적인-설명",
    "href": "posts/DL_Summary5.html#a-2.-수학적인-설명",
    "title": "Deep Learning 5",
    "section": "A-2. 수학적인 설명",
    "text": "A-2. 수학적인 설명\n- 이미지 \\(X\\)가 주어졌을 때 라벨을 \\(y\\)라고 한다면. 이미지를 보고 라벨을 맞추는 것은 \\(P(y|X)\\)이다\n- 그렇지만 이미지를 생성하는 것은 \\(P(X,y)\\)이다\n- 쉽게 말하면 고양이인지 개인지 맞추는 것은 \\(y\\)만 맞추면 됨. 하지만 이미지를 생성하는 것은 \\(X,y\\)를 모두 맞추어야 함\n###\n\n\\(P(y|X) = \\frac{P(X,y)}{P(X)} = \\frac{P(X,y)}{\\sum_{y}P(X,y)}\\)\n\n\n\n- \\(P(X,y)\\)를 알면 \\(P(y|X)\\)은 알 수 있음 하지만 역은 불가능\n- 한 마디로 이미지를 생성하는 것이 이미지를 분류하는 것보다 더 어렵다"
  },
  {
    "objectID": "posts/DL_Summary5.html#b.-gan은-생성모형-중-하나다",
    "href": "posts/DL_Summary5.html#b.-gan은-생성모형-중-하나다",
    "title": "Deep Learning 5",
    "section": "B. GAN은 생성모형 중 하나다",
    "text": "B. GAN은 생성모형 중 하나다\n- GAN의 원리는 예를 들어 설명하면 경찰과 위조지폐범이 경쟁을 통해서 서로 발전하는 모형으로 설명 가능하다.\n- 위조범은 경찰이 속게끔 위조지폐를 더 정교하게 만들고 경찰은 그 위조지폐를 잘 구별하게끔 학습을 한다.\n- 굉장히 우수한 경찰이라도 진짜와 가짜를 구분하지 못 할때(=진짜 이미지를 0.5의 확률로만 진짜라고 할 때= 가짜이미지를 0.5의 확률로 가짜라고 할 때) 학습을 멈춘다."
  },
  {
    "objectID": "posts/DL_Summary5.html#a.data",
    "href": "posts/DL_Summary5.html#a.data",
    "title": "Deep Learning 5",
    "section": "A.DATA",
    "text": "A.DATA\n\npath = fastai.data.external.untar_data(fastai.data.external.URLs.MNIST)\npath\n\nPath('/root/.fastai/data/mnist_png')\n\n\n\nX_real = torch.stack([torchvision.io.read_image(str(l)) for l in (path/'training/3').ls()],axis=0)/255\nX_real.shape\n\ntorch.Size([6131, 1, 28, 28])\n\n\n\nplt.imshow(X_real[0].reshape(28,28),cmap='grey')"
  },
  {
    "objectID": "posts/DL_Summary5.html#b.-faker-생성",
    "href": "posts/DL_Summary5.html#b.-faker-생성",
    "title": "Deep Learning 5",
    "section": "B. Faker 생성",
    "text": "B. Faker 생성\n- 네트워크 입력 : (n,??)\n- 네트워크 출력 : (n,1,28,28)\n\ntorch.randn(1,4)\n\ntensor([[ 0.0454, -0.7508, -0.9412, -1.6791]])\n\n\nreshape을 위한 class를 정의하자\n\nclass reshape2828(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self,X):\n        return X.reshape(-1,1,28,28)\n\n\nnet_faker = torch.nn.Sequential(\n    torch.nn.Linear(4,64),\n    torch.nn.ReLU(),\n    torch.nn.Linear(64,64),\n    torch.nn.ReLU(),\n    torch.nn.Linear(64,784),\n    torch.nn.Sigmoid(),\n    reshape2828()\n)\n\n\nnet_faker(torch.randn(1,4)).shape\n\ntorch.Size([1, 1, 28, 28])"
  },
  {
    "objectID": "posts/DL_Summary5.html#c.-경찰-생성",
    "href": "posts/DL_Summary5.html#c.-경찰-생성",
    "title": "Deep Learning 5",
    "section": "C. 경찰 생성",
    "text": "C. 경찰 생성\n- 네트워크의 입력 : (n,1,28,28)\n- 네트워크의 출력 : 0 or 1\n\nnet_police = torch.nn.Sequential(\n    torch.nn.Flatten(),\n    torch.nn.Linear(784,64),\n    torch.nn.ReLU(),\n    torch.nn.Linear(64,30),\n    torch.nn.ReLU(),\n    torch.nn.Linear(30,1),\n    torch.nn.Sigmoid()\n)"
  },
  {
    "objectID": "posts/DL_Summary5.html#d.-패트와-매트",
    "href": "posts/DL_Summary5.html#d.-패트와-매트",
    "title": "Deep Learning 5",
    "section": "D. 패트와 매트",
    "text": "D. 패트와 매트\n\nplt.imshow(X_real[0].reshape(28,28),cmap='grey')\n\n\n\n\n\n\n\n\n진짜 이미지 -&gt; 0 / 가짜 이미지 -&gt; 1\n\nnet_police(X_real[[0]])\n\ntensor([[0.5080]], grad_fn=&lt;SigmoidBackward0&gt;)\n\n\n\nnoise = torch.randn(1,4)\nplt.imshow(net_faker(noise).data.reshape(28,28),cmap='grey')\n\n\n\n\n\n\n\n\n\nyhat_fake = net_police(net_faker(noise))\nyhat_fake\n\ntensor([[0.5001]], grad_fn=&lt;SigmoidBackward0&gt;)\n\n\n1과는 매우 거리가 멈\n아무 학습도 안 했기때문에 당연… 하지만 무능!!\n\nfig , ax = plt.subplots(1,2)\nax[0].imshow(net_faker(noise).data.reshape(28,28),cmap='grey')\nax[1].imshow(X_real[0].reshape(28,28),cmap='grey')\n\n\n\n\n\n\n\n\n이걸 구분을 잘 못하고 있음…"
  },
  {
    "objectID": "posts/DL_Summary5.html#e.-경찰이-공부했다.",
    "href": "posts/DL_Summary5.html#e.-경찰이-공부했다.",
    "title": "Deep Learning 5",
    "section": "E. 경찰이 공부했다.",
    "text": "E. 경찰이 공부했다.\n진짜 이미지 -&gt; 0 , 가짜 이미지 -&gt; 1\n- step1\n\nnoise = torch.randn(6131,4)\nX_fake = net_faker(noise)\ny_real = torch.tensor([0]*6131).reshape(-1,1).float()\ny_fake = torch.tensor([1]*6131).reshape(-1,1).float()\n\n\nyhat_real = net_police(X_real) # 경찰이 진짜 이미지를 보고 판별한 결과\nyhat_fake = net_police(X_fake) # 경찰이 가짜 이미지를 보고 판별한 경과\n\n- step2\n\nbce = torch.nn.BCELoss()\nloss_police = bce(yhat_fake , y_fake) + bce(yhat_real , y_real)\nloss_police\n\ntensor(1.4071, grad_fn=&lt;AddBackward0&gt;)\n\n\n- step3,4는 앞에서 공부한 내용과 비슷하기에 생략하고 바로 epoch을 돌리자\n\nnet_police = torch.nn.Sequential(\n    torch.nn.Flatten(),\n    torch.nn.Linear(784,64),\n    torch.nn.ReLU(),\n    torch.nn.Linear(64,30),\n    torch.nn.ReLU(),\n    torch.nn.Linear(30,1),\n    torch.nn.Sigmoid()\n)\nbce = torch.nn.BCELoss()\noptimizr_police = torch.optim.Adam(net_police.parameters())\n\nfor epoc in range(30):\n    noise = torch.randn(6131,4) # epoc을 돌릴 때마다 새로운 noise를 뽑아야함\n    X_fake = net_faker(noise)\n    yhat_real = net_police(X_real)\n    yhat_fake = net_police(X_fake)\n    loss_police = bce(yhat_real,y_real) + bce(yhat_fake,y_fake)\n    loss_police.backward()\n    optimizr_police.step()\n    optimizr_police.zero_grad()\n\n\nnet_police(X_real).mean()\n\ntensor(0.0224, grad_fn=&lt;MeanBackward0&gt;)\n\n\n꽤 늘었다"
  },
  {
    "objectID": "posts/DL_Summary5.html#f.-발전하는-페이커",
    "href": "posts/DL_Summary5.html#f.-발전하는-페이커",
    "title": "Deep Learning 5",
    "section": "F. 발전하는 페이커",
    "text": "F. 발전하는 페이커\n- step1\n\nNoise = torch.randn(6131,4)\nX_fake = net_faker(Noise) \n\n- step2\n\nyhat_faker = net_police(X_fake)\nloss_faker = bce(yhat_faker,y_real)\n\n가짜를 보고 진짜라고 생각해야한다\n- step3~4는 별로 특별한게 없음. 그래서 바로 epoch을 진행시켜보자.\n\nnet_faker = torch.nn.Sequential(\n    torch.nn.Linear(in_features=4, out_features=30),\n    torch.nn.ReLU(),\n    torch.nn.Linear(in_features=30, out_features=64),\n    torch.nn.ReLU(),\n    torch.nn.Linear(in_features=64, out_features=784),\n    torch.nn.Sigmoid(),\n    reshape2828()\n)\n#bce = torch.nn.BCELoss()\noptimizr_faker = torch.optim.Adam(net_faker.parameters()) # 얘를 실행시키면 새로운 옵티마이저가 나와서 초기값으로 돌아감\n\n\nfor epoc in range(10):\n    noise = torch.randn(6131,4)\n    X_fake = net_faker(noise)\n    yhat_fake = net_police(X_fake)\n    loss_faker = bce(yhat_fake,y_real)\n    loss_faker.backward()\n    optimizr_faker.step()\n    optimizr_faker.zero_grad()\n\n\nfig,ax = plt.subplots(2,5,figsize=(10,4))\nk = 0 \nfor i in range(2):\n    for j in range(5):\n        ax[i][j].imshow(X_fake[k].reshape(28,28).data,cmap=\"gray\")\n        ax[i][j].set_title(f\"police hat = {yhat_fake[k].item():.4f}\")\n        k = k+1 \nfig.tight_layout()\n\n\n\n\n\n\n\n\n\n((yhat_fake &gt; 0.5) == 0).float().mean()\n\ntensor(0.)\n\n\n경찰이 가짜이미지를 진짜라고 생각한 비율"
  },
  {
    "objectID": "posts/DL_Summary5.html#g.-경쟁",
    "href": "posts/DL_Summary5.html#g.-경쟁",
    "title": "Deep Learning 5",
    "section": "G. 경쟁",
    "text": "G. 경쟁\n\ntorch.manual_seed(21345)\nnet_police = torch.nn.Sequential(\n    torch.nn.Flatten(),\n    torch.nn.Linear(784,64),\n    torch.nn.ReLU(),\n    torch.nn.Linear(64,30),\n    torch.nn.ReLU(),\n    torch.nn.Linear(30,1),\n    torch.nn.Sigmoid()\n)\nnet_faker = torch.nn.Sequential(\n    torch.nn.Linear(in_features=4, out_features=30),\n    torch.nn.ReLU(),\n    torch.nn.Linear(in_features=30, out_features=64),\n    torch.nn.ReLU(),\n    torch.nn.Linear(in_features=64, out_features=784),\n    torch.nn.Sigmoid(),\n    reshape2828()\n)\nbce = torch.nn.BCELoss()\noptimizr_police = torch.optim.Adam(net_police.parameters(),lr=0.001,betas=(0.5,0.999))\noptimizr_faker = torch.optim.Adam(net_faker.parameters(),lr=0.002,betas=(0.5,0.999))\n\n- police 네트워크 학습\n\nfor epoc in range(30):\n    noise = torch.randn(6131,4)\n    X_fake = net_faker(noise)\n    yhat_real = net_police(X_real)\n    yhat_fake = net_police(X_fake)\n    loss_police = bce(yhat_real,y_real) + bce(yhat_fake,y_fake)\n    loss_police.backward()\n    optimizr_police.step()\n    optimizr_police.zero_grad()\nprint(f\"\"\"\nyhat_real = {yhat_real[0].item():.4f} // 이건 0에 가까워야함 \nyhat_fake = {yhat_fake[0].item():.4f} // 이건 1에 가까워야함 \n\"\"\")\n\n\nyhat_real = 0.0002 // 이건 0에 가까워야함 \nyhat_fake = 0.9945 // 이건 1에 가까워야함 \n\n\n\n- faker 네트워크 학습\n\nfor epoc in range(10):\n    # step1\n    Noise = torch.randn(6131,4) \n    X_fake = net_faker(Noise) \n    # step2\n    yhat_fake = net_police(X_fake) \n    loss_faker = bce(yhat_fake,y_real)\n    # step3 \n    loss_faker.backward()\n    # step4 \n    optimizr_faker.step()\n    optimizr_faker.zero_grad()\n#---#\nfig,ax = plt.subplots(2,5,figsize=(10,4))\nk = 0 \nfor i in range(2):\n    for j in range(5):\n        ax[i][j].imshow(X_fake[k].reshape(28,28).data,cmap=\"gray\")\n        ax[i][j].set_title(f\"police hat = {yhat_fake[k].item():.4f}\")\n        k = k+1 \nfig.tight_layout()\nprint(f\"\"\"\nscam ratio = {((yhat_fake &gt; 0.5) == 0).float().mean():.4f} \nloss_faker = {loss_faker:.4f} \n\"\"\")\n\n\nscam ratio = 0.0000 \nloss_faker = 4.4663"
  },
  {
    "objectID": "posts/RNN.html",
    "href": "posts/RNN.html",
    "title": "Recurrent Neural Network",
    "section": "",
    "text": "0. Data, Preprocessing\n\nimport torch\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n\nsoft = torch.nn.Softmax(dim=1)\nsig = torch.nn.Sigmoid()\ntanh = torch.nn.Tanh()\n\n\ntxt = list('AbAcAd'*50)\ntxt[:10]\n\n['A', 'b', 'A', 'c', 'A', 'd', 'A', 'b', 'A', 'c']\n\n\n\ndf_train = pd.DataFrame({'x':txt[:-1], 'y':txt[1:]})\ndf_train[:5]\n\n\n\n\n\n\n\n\n\nx\ny\n\n\n\n\n0\nA\nb\n\n\n1\nb\nA\n\n\n2\nA\nc\n\n\n3\nc\nA\n\n\n4\nA\nd\n\n\n\n\n\n\n\n\n- input은 단어의 종류 개수인 4이고 hidden size는 3으로 설정\n\n\n1. RNNCell\n\nx = torch.tensor(df_train.x.map({'A':0,'b':1,'c':2,'d':3}))\ny = torch.tensor(df_train.y.map({'A':0,'b':1,'c':2,'d':3}))\nX = torch.nn.functional.one_hot(x).float()\ny = torch.nn.functional.one_hot(y).float()\n\n\nrnncell = torch.nn.RNNCell(4,3)\nlinr = torch.nn.Linear(3,4)\nloss_fn = torch.nn.CrossEntropyLoss()\noptimizr = torch.optim.Adam(list(rnncell.parameters()) + list(linr.parameters()),lr=0.1)\n\n\nL = len(X)\nfor l in range(200):\n    loss = 0\n    ht = torch.zeros(3)\n    for t in range(L):\n        Xt,yt = X[t],y[t]\n        ht = rnncell(Xt,ht)\n        netout = linr(ht)\n        loss = loss + loss_fn(netout,yt)\n    loss = loss/L\n    loss.backward()\n    optimizr.step()\n    optimizr.zero_grad()\n\n\nh = torch.zeros(L,3)\nwater = torch.zeros(3)\nh[0] = rnncell(X[0],water)\nfor t in range(1,L):\n    h[t] = rnncell(X[t],h[t-1])\nyhat = soft(linr(h))\nyhat\n\ntensor([[1.0115e-02, 7.5804e-01, 5.5738e-02, 1.7611e-01],\n        [9.9825e-01, 6.4732e-04, 1.0804e-03, 1.8182e-05],\n        [1.4760e-03, 9.1038e-04, 9.9757e-01, 4.3916e-05],\n        ...,\n        [1.1941e-03, 1.3800e-03, 9.9737e-01, 5.9494e-05],\n        [9.9993e-01, 4.1149e-07, 7.4368e-05, 6.4593e-08],\n        [3.9266e-06, 4.9567e-01, 7.3947e-04, 5.0359e-01]],\n       grad_fn=&lt;SoftmaxBackward0&gt;)\n\n\n\nmat = torch.concat([h,yhat],axis=1).data[:10]\nplt.matshow(mat,cmap='bwr',vmin=-1,vmax=1)\nplt.axvline(x=2.5,color='lime')\nplt.xticks(range(7),[r'$h_1$',r'$h_2$',r'$h_3$', r'$P_A$',r'$P_b$',r'$P_c$',r'$P_d$']);\n\n\n\n\n\n\n\n\n\n\n2. RNN\n\nx = torch.tensor(df_train.x.map({'A':0,'b':1,'c':2,'d':3}))\ny = torch.tensor(df_train.y.map({'A':0,'b':1,'c':2,'d':3}))\nX = torch.nn.functional.one_hot(x).float()\ny = torch.nn.functional.one_hot(y).float()\n\n\nrnn = torch.nn.RNN(4,3)\nlinr = torch.nn.Linear(3,4)\nloss_fn = torch.nn.CrossEntropyLoss() \noptimizr = torch.optim.Adam(list(rnn.parameters())+list(linr.parameters()),lr=0.1)\n\nwater = torch.zeros(1,3)\nfor l in range(200):\n    h,hL = rnn(X,water)\n    netout = linr(h)\n    loss = loss_fn(netout,y)\n    loss.backward()\n    optimizr.step()\n    optimizr.zero_grad()\n\n\nh,_ = rnn(X,water)\nyhat = soft(linr(h))\nyhat\n\ntensor([[2.3293e-02, 9.7478e-01, 6.4693e-06, 1.9182e-03],\n        [9.9996e-01, 3.9347e-05, 2.1711e-09, 2.8704e-08],\n        [1.7528e-08, 5.4821e-07, 9.9156e-01, 8.4434e-03],\n        ...,\n        [1.7383e-08, 5.4203e-07, 9.9162e-01, 8.3797e-03],\n        [9.9946e-01, 5.4431e-04, 1.2763e-08, 2.2101e-07],\n        [7.5180e-06, 1.0098e-02, 5.9897e-03, 9.8390e-01]],\n       grad_fn=&lt;SoftmaxBackward0&gt;)\n\n\n\nmat = torch.concat([h,yhat],axis=1).data[:10]\nplt.matshow(mat,cmap='bwr',vmin=-1,vmax=1)\nplt.axvline(x=2.5,color='lime')\nplt.xticks(range(7),[r'$h_1$',r'$h_2$',r'$h_3$', r'$P_A$',r'$P_b$',r'$P_c$',r'$P_d$']);\n\n\n\n\n\n\n\n\n\n\n3. LSTM\n\nx = torch.tensor(df_train.x.map({'A':0,'b':1,'c':2,'d':3}))\ny = torch.tensor(df_train.y.map({'A':0,'b':1,'c':2,'d':3}))\nX = torch.nn.functional.one_hot(x).float()\ny = torch.nn.functional.one_hot(y).float()\n\n\nlstm = torch.nn.LSTM(4,3)\nlinr = torch.nn.Linear(3,4)\nloss_fn = torch.nn.CrossEntropyLoss()\noptimizr = torch.optim.Adam(list(lstm.parameters()) + list(linr.parameters()),lr=0.1)\n\nfor l in range(500):\n    h,_ = lstm(X)\n    netout = linr(h)\n    loss = loss_fn(netout,y)\n    loss.backward()\n    optimizr.step()\n    optimizr.zero_grad()\n\n\nh,_ = lstm(X)\nyhat = soft(linr(h))\nyhat\n\ntensor([[7.0750e-04, 9.7673e-01, 1.4719e-08, 2.2562e-02],\n        [9.9934e-01, 1.9747e-09, 6.5815e-04, 7.6823e-07],\n        [7.2932e-06, 7.5534e-11, 9.9835e-01, 1.6416e-03],\n        ...,\n        [7.4841e-06, 7.9410e-11, 9.9831e-01, 1.6844e-03],\n        [9.9994e-01, 6.7138e-08, 5.9597e-05, 2.4941e-06],\n        [3.2600e-04, 1.7392e-03, 1.3645e-03, 9.9657e-01]],\n       grad_fn=&lt;SoftmaxBackward0&gt;)\n\n\n\nmat = torch.concat([h,yhat],axis=1).data[:10]\nplt.matshow(mat,cmap='bwr',vmin=-1,vmax=1)\nplt.axvline(x=2.5,color='lime')\nplt.xticks(range(7),[r'$h_1$',r'$h_2$',r'$h_3$', r'$P_A$',r'$P_b$',r'$P_c$',r'$P_d$']);\n\n\n\n\n\n\n\n\n\n\n4. LSTMCell\n\nx = torch.tensor(df_train.x.map({'A':0,'b':1,'c':2,'d':3}))\ny = torch.tensor(df_train.y.map({'A':0,'b':1,'c':2,'d':3}))\nX = torch.nn.functional.one_hot(x).float()\ny = torch.nn.functional.one_hot(y).float()\n\n\nlstmcell = torch.nn.LSTMCell(4,3)\nlinr = torch.nn.Linear(3,4)\nloss_fn = torch.nn.CrossEntropyLoss()\noptimizr = torch.optim.Adam(list(lstmcell.parameters())+list(linr.parameters()),lr=0.1)\n\n\nfor l in range(100):\n    loss = 0\n    ht = torch.zeros(3)\n    ct = torch.zeros(3)\n    for t in range(len(X)):\n        Xt,yt = X[t],y[t]\n        ht,ct = lstmcell(Xt,(ht,ct))\n        netout_t = linr(ht)\n        loss = loss + loss_fn(netout_t,yt)\n    loss = loss/L\n    loss.backward()\n    optimizr.step()\n    optimizr.zero_grad()\n\n\nh = torch.zeros(L,3)\nc = torch.zeros(L,3)\nht = torch.zeros(3)\nct = torch.zeros(3)\nh[0],c[0] = lstmcell(X[0],(ht,ct))\nfor t in range(1,L):\n    h[t],c[t] = lstmcell(X[t],(h[t-1],c[t-1]))\nyhat = soft(linr(h))\nyhat\n\ntensor([[2.0200e-03, 9.7144e-01, 1.5596e-02, 1.0939e-02],\n        [9.8657e-01, 3.4276e-08, 1.3420e-02, 5.1240e-06],\n        [1.6634e-02, 1.8627e-03, 9.6436e-01, 1.7148e-02],\n        ...,\n        [1.8804e-02, 4.0043e-03, 9.6906e-01, 8.1301e-03],\n        [9.9448e-01, 1.0317e-05, 3.7891e-03, 1.7218e-03],\n        [1.2681e-04, 7.6272e-04, 4.3521e-03, 9.9476e-01]],\n       grad_fn=&lt;SoftmaxBackward0&gt;)\n\n\n\nmat = torch.concat([h,yhat],axis=1).data[:10]\nplt.matshow(mat,cmap='bwr',vmin=-1,vmax=1)\nplt.axvline(x=2.5,color='lime')\nplt.xticks(range(7),[r'$h_1$',r'$h_2$',r'$h_3$','$P_A$',r'$P_b$',r'$P_c$',r'$P_d$']);\n\n\n\n\n\n\n\n\n\n\n5. torch.nn.Module + rNNCell\n\nx = torch.tensor(df_train.x.map({'A':0,'b':1,'c':2,'d':3}))\ny = torch.tensor(df_train.y.map({'A':0,'b':1,'c':2,'d':3}))\nX = torch.nn.functional.one_hot(x).float()\ny = torch.nn.functional.one_hot(y).float()\n\n\nclass rNNCell(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.i2h = torch.nn.Linear(4,2) \n        self.h2h = torch.nn.Linear(2,2) \n        self.tanh = torch.nn.Tanh()\n    def forward(self,Xt,ht):\n        ht = self.tanh(self.i2h(Xt)+self.h2h(ht))\n        return ht\ntorch.manual_seed(43052) # 시드고정해야만 답나옴 --&gt; 임베딩공간이 부족하다는 의미 (사실상 6개의 문자니까!)\nrnncell = rNNCell() \ncook = torch.nn.Linear(2,4) \n#\nloss_fn = torch.nn.CrossEntropyLoss()\noptimizr = torch.optim.Adam(list(rnncell.parameters())+ list(cook.parameters()),lr=0.1)\n#---#\nL = len(X)\nfor epoc in range(200):\n    ## 1~2 \n    ht = torch.zeros(2)\n    loss = 0\n    for t in range(L):\n        Xt, yt = X[t], y[t]\n        ht = rnncell(Xt, ht)\n        ot = cook(ht) \n        loss = loss + loss_fn(ot, yt)\n    loss = loss/L\n    ## 3 \n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\n\n\nh = torch.zeros(L,2)\nwater = torch.zeros(2)\nh[0] = rnncell(X[0],water)\nfor t in range(1,L):\n    h[t] = rnncell(X[t],h[t-1])\nyhat = soft(cook(h))\nyhat    \n\ntensor([[4.1978e-03, 9.4555e-01, 1.9557e-06, 5.0253e-02],\n        [9.9994e-01, 5.5569e-05, 8.4751e-10, 1.3143e-06],\n        [2.1349e-07, 1.1345e-06, 9.7019e-01, 2.9806e-02],\n        ...,\n        [2.1339e-07, 1.1339e-06, 9.7020e-01, 2.9798e-02],\n        [9.9901e-01, 9.6573e-04, 6.9303e-09, 2.1945e-05],\n        [7.2919e-04, 2.5484e-02, 3.3011e-02, 9.4078e-01]],\n       grad_fn=&lt;SoftmaxBackward0&gt;)\n\n\n\nmat = torch.concat([h,yhat],axis=1).data[:10]\nplt.matshow(mat,cmap='bwr',vmin=-1,vmax=1)\nplt.axvline(x=1.5,color='lime')\nplt.xticks(range(6),[r'$h_1$',r'$h_2$',r'$P_A$',r'$P_b$',r'$P_c$',r'$P_d$']);\n\n\n\n\n\n\n\n\n\nx = torch.tensor(df_train.x.map({'A':0,'b':1,'c':2,'d':3}))\ny = torch.tensor(df_train.y.map({'A':0,'b':1,'c':2,'d':3}))\nX = torch.nn.functional.one_hot(x).float()\ny = torch.nn.functional.one_hot(y).float()\n\n\nlstmcell = torch.nn.LSTMCell(4,3)\nlinr = torch.nn.Linear(3,4)\nloss_fn = torch.nn.CrossEntropyLoss()\noptimizr = torch.optim.Adam(list(lstmcell.parameters())+list(linr.parameters()),lr=0.1)\n\n\nfor l in range(100):\n    loss = 0\n    ht = torch.zeros(3)\n    ct = torch.zeros(3)\n    for t in range(len(X)):\n        Xt,yt = X[t],y[t]\n        ht,ct = lstmcell(Xt,(ht,ct))\n        netout_t = linr(ht)\n        loss = loss + loss_fn(netout_t,yt)\n    loss = loss/L\n    loss.backward()\n    optimizr.step()\n    optimizr.zero_grad()\n\n\nh = torch.zeros(L,3)\nc = torch.zeros(L,3)\nht = torch.zeros(3)\nct = torch.zeros(3)\nh[0],c[0] = lstmcell(X[0],(ht,ct))\nfor t in range(1,L):\n    h[t],c[t] = lstmcell(X[t],(h[t-1],c[t-1]))\nyhat = soft(linr(h))\nyhat\n\ntensor([[2.0200e-03, 9.7144e-01, 1.5596e-02, 1.0939e-02],\n        [9.8657e-01, 3.4276e-08, 1.3420e-02, 5.1240e-06],\n        [1.6634e-02, 1.8627e-03, 9.6436e-01, 1.7148e-02],\n        ...,\n        [1.8804e-02, 4.0043e-03, 9.6906e-01, 8.1301e-03],\n        [9.9448e-01, 1.0317e-05, 3.7891e-03, 1.7218e-03],\n        [1.2681e-04, 7.6272e-04, 4.3521e-03, 9.9476e-01]],\n       grad_fn=&lt;SoftmaxBackward0&gt;)\n\n\n\nmat = torch.concat([h,yhat],axis=1).data[:10]\nplt.matshow(mat,cmap='bwr',vmin=-1,vmax=1)\nplt.axvline(x=2.5,color='lime')\nplt.xticks(range(7),[r'$h_1$',r'$h_2$',r'$h_3$','$P_A$',r'$P_b$',r'$P_c$',r'$P_d$']);\n\n\n\n\n\n\n\n\n\n\n6. torch.nn.Module + LSTM\n\ntorch.manual_seed(43052)\nlstmcell = torch.nn.LSTMCell(4,2) # 숙성담당\ncook = torch.nn.Linear(2,4)\nlstm = torch.nn.LSTM(4,2) # &lt;-- 이거로 학습\nlstm.weight_ih_l0.data = lstmcell.weight_ih.data\nlstm.weight_hh_l0.data = lstmcell.weight_hh.data\nlstm.bias_ih_l0.data = lstmcell.bias_ih.data\nlstm.bias_hh_l0.data = lstmcell.bias_hh.data\nloss_fn = torch.nn.CrossEntropyLoss()\noptimizr = torch.optim.Adam(list(lstm.parameters())+list(cook.parameters()),lr=0.1) \n#---#\nL = len(X)\nWater = torch.zeros(1,2)\nfor epoc in range(1):\n    # 1 \n    h,(hL,cL) = lstm(X,(Water,Water))\n    netout = cook(h)\n    # 2 \n    loss = loss_fn(netout,y)\n    # 3\n    loss.backward()\n    # 4 \n    optimizr.step()\n    optimizr.zero_grad()\n\n\nhL,cL\n\n(tensor([[0.1745, 0.1610]], grad_fn=&lt;SqueezeBackward1&gt;),\n tensor([[0.2894, 0.4533]], grad_fn=&lt;SqueezeBackward1&gt;))"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Deep Learing",
    "section": "",
    "text": "Recurrent Neural Network\n\n\n\n\n\n\n\n\n\n\n\nMay 7, 2025\n\n\n차상진\n\n\n\n\n\n\n\n\n\n\n\n\nRL1\n\n\n\n\n\n\n\n\n\n\n\nJun 2, 2024\n\n\n차상진\n\n\n\n\n\n\n\n\n\n\n\n\nAbout hidden size\n\n\n\n\n\n\n\n\n\n\n\nMay 27, 2024\n\n\n차상진\n\n\n\n\n\n\n\n\n\n\n\n\nDeep Learning 7\n\n\n\n\n\n\n\n\n\n\n\nMay 24, 2024\n\n\n차상진\n\n\n\n\n\n\n\n\n\n\n\n\nDeep Learning 6\n\n\n\n\n\n\n\n\n\n\n\nMay 22, 2024\n\n\n차상진\n\n\n\n\n\n\n\n\n\n\n\n\nDeep Learning 5\n\n\n\n\n\n\n\n\n\n\n\nMay 6, 2024\n\n\n차상진\n\n\n\n\n\n\n\n\n\n\n\n\nDeep Learning 4\n\n\n\n\n\n\n\n\n\n\n\nApr 19, 2024\n\n\n차상진\n\n\n\n\n\n\n\n\n\n\n\n\nMNIST CIFAR10\n\n\n\n\n\n\n\n\n\n\n\nApr 18, 2024\n\n\n차상진\n\n\n\n\n\n\n\n\n\n\n\n\nStochastic Gradient Descent\n\n\n\n\n\n\n\n\n\n\n\nApr 9, 2024\n\n\n차상진\n\n\n\n\n\n\n\n\n\n\n\n\nPredict Underlying\n\n\n\n\n\n\n\n\n\n\n\nApr 8, 2024\n\n\n차상진\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/SGD.html",
    "href": "posts/SGD.html",
    "title": "Stochastic Gradient Descent",
    "section": "",
    "text": "import torch\nimport matplotlib.pyplot as plt\nfrom fastai.data.all import *\nimport torchvision"
  },
  {
    "objectID": "posts/SGD.html#a.-xy-데이터를-모두-굳이-gpu에-넘겨야-하는가",
    "href": "posts/SGD.html#a.-xy-데이터를-모두-굳이-gpu에-넘겨야-하는가",
    "title": "Stochastic Gradient Descent",
    "section": "A. X,y 데이터를 모두 굳이 GPU에 넘겨야 하는가?",
    "text": "A. X,y 데이터를 모두 굳이 GPU에 넘겨야 하는가?\n데이터셋을 짝홀로 나누어서 번갈아가면서 GPU에 올렸다 내렸다하면 안되나?\n- 아래의 알고리즘을 생각해보자.\n\n데이터를 반으로 나누고\n짝수 obs의 x,y,net의 모든 parameters을 GPU에 올린다\nyhat,loss,grad,update 수행\n홀수 obs의 x,y를 GPU메모리에서 내린다. 그리고 짝수 obs의 x,y를 GPU메모리에 올린다\nyhat,loss,grad,update 수행\n반복"
  },
  {
    "objectID": "posts/SGD.html#b.-미니배치-경사하강법",
    "href": "posts/SGD.html#b.-미니배치-경사하강법",
    "title": "Stochastic Gradient Descent",
    "section": "B. 미니배치 경사하강법",
    "text": "B. 미니배치 경사하강법\n그럼 홀수 짝수로 나누는 건 2로 나누는 건데 굳이 2로만 나누어야하나? 더 쪼갤 수 있지 않나?\n- gradient descent : 10개의 sample data가 있다고 할 때 모든 sample을 이용하여 slope계산\n- stochastic gradient descent with batch size = 1 : 10개의 smaple data를 하나씩으로 모두 쪼개서 slope계산\nstochastic gradient descent with batch size = 1의 경우는 epoc을 10번 하면 총 100번 epoc을 돌리는 것과 같다\n- stochastic gradient descent : m개의 sample을 이용하여 slope 계산\n그럼 stochastic gradient descent의 경우는 epoc을 10번 하면 총 40번 epoc을 돌리는 것과 같다."
  },
  {
    "objectID": "posts/SGD.html#c.-datasetds-dataloaderdl",
    "href": "posts/SGD.html#c.-datasetds-dataloaderdl",
    "title": "Stochastic Gradient Descent",
    "section": "C. Dataset(ds) , DataLoader(dl)",
    "text": "C. Dataset(ds) , DataLoader(dl)\nstochastic gradient descent를 수행하기 위해서 파이토치에서는 ds와 dl라는 오브젝트를 준비했다.\n\nx=torch.tensor(range(10)).float().reshape(-1,1)\ny=torch.tensor([1.0]*5+[0.0]*5).reshape(-1,1)\ntorch.concat([x,y],axis=1)\n\ntensor([[0., 1.],\n        [1., 1.],\n        [2., 1.],\n        [3., 1.],\n        [4., 1.],\n        [5., 0.],\n        [6., 0.],\n        [7., 0.],\n        [8., 0.],\n        [9., 0.]])\n\n\n\nds = torch.utils.data.TensorDataset(x,y)\n\ndir(ds)를 살펴보면 __getitem__이 있다 이러면 섭스크립터블하다는 것이다.\n\nds.tensors # 튜플 언패킹으로 뽑을 수 있을 거 같음\n\n(tensor([[0.],\n         [1.],\n         [2.],\n         [3.],\n         [4.],\n         [5.],\n         [6.],\n         [7.],\n         [8.],\n         [9.]]),\n tensor([[1.],\n         [1.],\n         [1.],\n         [1.],\n         [1.],\n         [0.],\n         [0.],\n         [0.],\n         [0.],\n         [0.]]))\n\n\n\nds[0] , (x,y)[0]\n\n((tensor([0.]), tensor([1.])),\n tensor([[0.],\n         [1.],\n         [2.],\n         [3.],\n         [4.],\n         [5.],\n         [6.],\n         [7.],\n         [8.],\n         [9.]]))\n\n\n그런데 일반적인 튜플의 인덱싱과는 다르게 동작함\n\ndl=torch.utils.data.DataLoader(ds,batch_size=3)\ndl\n\n&lt;torch.utils.data.dataloader.DataLoader at 0x7fdf9fc2fec0&gt;\n\n\ndl : 섭스크립터블하지 않지만 이터러블 함 즉, for문을 사용할 수 있음\n\nfor xi,yi in dl:\n    print(xi,yi)\n\ntensor([[0.],\n        [1.],\n        [2.]]) tensor([[1.],\n        [1.],\n        [1.]])\ntensor([[3.],\n        [4.],\n        [5.]]) tensor([[1.],\n        [1.],\n        [0.]])\ntensor([[6.],\n        [7.],\n        [8.]]) tensor([[0.],\n        [0.],\n        [0.]])\ntensor([[9.]]) tensor([[0.]])\n\n\n10을 3으로 나누면 마지막에 하나 남는데 그건 어떻게 해? -&gt; 그냥 하나 남으면 그것만 계산한다"
  },
  {
    "objectID": "posts/SGD.html#d.-dsdl을-이용한-mnist구현",
    "href": "posts/SGD.html#d.-dsdl을-이용한-mnist구현",
    "title": "Stochastic Gradient Descent",
    "section": "D. ds,dl을 이용한 MNIST구현",
    "text": "D. ds,dl을 이용한 MNIST구현\n- 목표 : 확률적경사하강법과 그냥 경사하강법의 성능을 ’동일 반복횟수’로 비교해보자\n- 그냥 경사하강법 - mini-batch쓰지 않는 학습\n\npath = untar_data(URLs.MNIST)\nX0 = torch.stack(([torchvision.io.read_image(str(fname)) for fname in (path/'training/0').ls()]))\nX1 = torch.stack(([torchvision.io.read_image(str(fname)) for fname in (path/'training/1').ls()]))\nX = torch.concat([X0,X1],axis=0).reshape(-1,1*28*28)/255\ny = torch.tensor([0.0]*len(X0) + [1.0]*len(X1)).reshape(-1,1)\n\ntorch.manual_seed(21345)\nnet = torch.nn.Sequential(\n    torch.nn.Linear(1*28*28,32),\n    torch.nn.ReLU(),\n    torch.nn.Linear(32,1),\n    torch.nn.Sigmoid()\n)\n\nloss_fn = torch.nn.BCELoss()\noptimizr = torch.optim.Adam(net.parameters())\n\nfor epoc in range(700):\n    yhat = net(X)\n    loss = loss_fn(yhat,y)\n    loss.backward()\n    optimizr.step()\n    optimizr.zero_grad()\n\n((yhat &gt; 0.5)*1.0 == y).float().mean()\n\ntensor(0.9998)\n\n\n\nplt.plot(y)\nplt.plot(yhat.data,'.')\n\n\n\n\n\n\n\n\n- ‘확률적’ 경사하강법 - mini-batch사용하는 학습\n\nds = torch.utils.data.TensorDataset(X,y)\ndl = torch.utils.data.DataLoader(ds,batch_size=2048)\n\npath = untar_data(URLs.MNIST)\nX0 = torch.stack(([torchvision.io.read_image(str(fname)) for fname in (path/'training/0').ls()]))\nX1 = torch.stack(([torchvision.io.read_image(str(fname)) for fname in (path/'training/1').ls()]))\nX = torch.concat([X0,X1],axis=0).reshape(-1,1*28*28)/255\ny = torch.tensor([0.0]*len(X0) + [1.0]*len(X1)).reshape(-1,1)\n\ntorch.manual_seed(21345)\nnet = torch.nn.Sequential(\n    torch.nn.Linear(1*28*28,32),\n    torch.nn.ReLU(),\n    torch.nn.Linear(32,1),\n    torch.nn.Sigmoid()\n)\n\nloss_fn = torch.nn.BCELoss()\noptimizr = torch.optim.Adam(net.parameters())\n\nfor epoc in range(100):\n    for xi,yi in dl:\n        netout = net(xi)\n        loss = loss_fn(netout,yi)\n        loss.backward()\n        optimizr.step()\n        optimizr.zero_grad()\n\n((net(X) &gt; 0.5)*1.0 == y).float().mean()\n\ntensor(0.9992)\n\n\n\nplt.plot(y)\nplt.plot(yhat.data,'.')\n\n\n\n\n\n\n\n\n- GPU를 활용하는 ‘확률적’ 경사하강법 - 실제로는 이게 최종 알고리즘\n\nds = torch.utils.data.TensorDataset(X,y)\ndl = torch.utils.data.DataLoader(ds,batch_size=2048)\n\npath = untar_data(URLs.MNIST)\nX0 = torch.stack(([torchvision.io.read_image(str(fname)) for fname in (path/'training/0').ls()]))\nX1 = torch.stack(([torchvision.io.read_image(str(fname)) for fname in (path/'training/1').ls()]))\nX = torch.concat([X0,X1],axis=0).reshape(-1,1*28*28)/255\ny = torch.tensor([0.0]*len(X0) + [1.0]*len(X1)).reshape(-1,1)\n\ntorch.manual_seed(21345)\nnet = torch.nn.Sequential(\n    torch.nn.Linear(1*28*28,32),\n    torch.nn.ReLU(),\n    torch.nn.Linear(32,1),\n    torch.nn.Sigmoid()\n).to(\"cuda:0\")\n\nloss_fn = torch.nn.BCELoss()\noptimizr = torch.optim.Adam(net.parameters())\n\nfor epoc in range(100):\n    for xi,yi in dl:\n        loss = loss_fn(net(xi.to(\"cuda:0\")),yi.to(\"cuda:0\"))\n        loss.backward()\n        optimizr.step()\n        optimizr.zero_grad()\n\nnet.to(\"cpu\")\n\n((net(X) &gt; 0.5)*1.0 == y).float().mean()\n\ntensor(0.9992)\n\n\n\nplt.plot(y)\nplt.plot(net(X).data.data,'.')"
  },
  {
    "objectID": "posts/SGD.html#a.-결론",
    "href": "posts/SGD.html#a.-결론",
    "title": "Stochastic Gradient Descent",
    "section": "A. 결론",
    "text": "A. 결론\n- 2개의 class를 구분하는 게 아니라 k개의 class를 구분해야 한다면?\ny의 형태 : (n,) vector + int형 // (n,k) one-hot encoded matrix + float형\n손실함수 : torch.nn.BCEWithLogitsLoss, -&gt; torch.nn.CrossEntropyLoss\n마지막층의 선형변환 : torch.nn.Linear(?,1) -&gt; torch.nn.Linear(?,k)\n마지막층의 활성화 : NONE -&gt; NONE (손실함수에 이미 포함되어있음)"
  },
  {
    "objectID": "posts/SGD.html#b.-실습-3개의-클래스를-구분",
    "href": "posts/SGD.html#b.-실습-3개의-클래스를-구분",
    "title": "Stochastic Gradient Descent",
    "section": "B. 실습 : 3개의 클래스를 구분",
    "text": "B. 실습 : 3개의 클래스를 구분\n\n## Step1: 데이터준비 \npath = untar_data(URLs.MNIST)\nX0 = torch.stack([torchvision.io.read_image(str(fname)) for fname in (path/'training/0').ls()])\nX1 = torch.stack([torchvision.io.read_image(str(fname)) for fname in (path/'training/1').ls()])\nX2 = torch.stack([torchvision.io.read_image(str(fname)) for fname in (path/'training/2').ls()])\nX = torch.concat([X0,X1,X2]).reshape(-1,1*28*28)/255\ny = torch.tensor([0]*len(X0) + [1]*len(X1)+ [2]*len(X2))\n## Step2: 학습가능한 오브젝트 생성\ntorch.manual_seed(43052)\nnet = torch.nn.Sequential(\n    torch.nn.Linear(784,32),\n    torch.nn.ReLU(),\n    torch.nn.Linear(32,3), # class = 3\n#    torch.nn.Softmax()\n)\nloss_fn = torch.nn.CrossEntropyLoss()\noptimizr = torch.optim.Adam(net.parameters())\n## Step3: 적합 \nfor epoc in range(100):\n    ## step1 \n    netout = net(X)\n    ## step2 \n    loss = loss_fn(netout,y)\n    ## step3 \n    loss.backward()\n    ## step4 \n    optimizr.step()\n    optimizr.zero_grad()\n## Step4: 적합 (혹은 적합결과확인)    \n(netout.argmax(axis=1) == y).float().mean()\n\ntensor(0.9827)\n\n\n파이토치에서 CrossEntropyLoss를 사용하면 one-hot 인코딩을 해준다. float형도 자동으로 맞춰줌"
  },
  {
    "objectID": "posts/SGD.html#d.정리",
    "href": "posts/SGD.html#d.정리",
    "title": "Stochastic Gradient Descent",
    "section": "D.정리",
    "text": "D.정리\n- 결론\n\n소프트맥스는 시그모이드의 확장이다.\n클래스의 수가 2개일 경우에는 (Sigmoid, BCEloss) 조합을 사용해야 하고 클래스의 수가 2개보다 클 경우에는 (Softmax, CrossEntropyLoss) 를 사용해야 한다.\n\n- 그런데 사실.. 클래스의 수가 2개일 경우일때 (Softmax, CrossEntropyLoss)를 사용해도 그렇게 큰일나는것은 아니다. (그냥 좀 비효율적인 느낌이 드는 것 뿐임. 흑백이미지를 칼라잉크로 출력하는 느낌)"
  },
  {
    "objectID": "posts/DL_Summary6.html",
    "href": "posts/DL_Summary6.html",
    "title": "Deep Learning 6",
    "section": "",
    "text": "import torch\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport random\nimport numpy as np"
  },
  {
    "objectID": "posts/DL_Summary6.html#a.-나는-solo",
    "href": "posts/DL_Summary6.html#a.-나는-solo",
    "title": "Deep Learning 6",
    "section": "A. 나는 SOLO",
    "text": "A. 나는 SOLO\n\ndf_view = pd.read_csv('https://raw.githubusercontent.com/guebin/DL2024/main/posts/solo.csv',index_col=0)\ndf_view\n\n\n\n\n\n\n\n\n\n영식(IN)\n영철(IN)\n영호(IS)\n광수(IS)\n상철(EN)\n영수(EN)\n규빈(ES)\n다호(ES)\n\n\n\n\n옥순(IN)\nNaN\n4.02\n3.45\n3.42\n0.84\n1.12\n0.43\n0.49\n\n\n영자(IN)\n3.93\n3.99\n3.63\n3.43\n0.98\n0.96\n0.52\nNaN\n\n\n정숙(IS)\n3.52\n3.42\n4.05\n4.06\n0.39\nNaN\n0.93\n0.99\n\n\n영숙(IS)\n3.43\n3.57\nNaN\n3.95\n0.56\n0.52\n0.89\n0.89\n\n\n순자(EN)\n1.12\nNaN\n0.59\n0.43\n4.01\n4.16\n3.52\n3.38\n\n\n현숙(EN)\n0.94\n1.05\n0.32\n0.45\n4.02\n3.78\nNaN\n3.54\n\n\n서연(ES)\n0.51\n0.56\n0.88\n0.89\n3.50\n3.64\n4.04\n4.10\n\n\n보람(ES)\n0.48\n0.51\n1.03\nNaN\n3.52\n4.00\n3.82\nNaN\n\n\n하니(I)\n4.85\n4.82\nNaN\n4.98\n4.53\n4.39\n4.45\n4.52\n\n\n\n\n\n\n\n\n- 데이터 이해를 위한 가정\n5점 만점으로 궁합이 좋을수록 점수가 높다. 최저점은 0점\nMBTI 성향에 따라서 궁합의 정도가 다르다. 특히 I/E의 성향일치가 중요하다.\n하니는 모든 사람들과 대체로 궁합이 좋다.\n하니는 I성향의 사람들과 좀 더 잘 맞는다."
  },
  {
    "objectID": "posts/DL_Summary6.html#b.-fit-predict",
    "href": "posts/DL_Summary6.html#b.-fit-predict",
    "title": "Deep Learning 6",
    "section": "B. Fit / Predict",
    "text": "B. Fit / Predict\n- 목표 : Nan을 추정\n(1) 옥순(IN)과 영식(IN)의 궁합\n\n옥순성향 = torch.tensor([1.22,0.49]).reshape(1,2)\n옥순매력 = torch.tensor(1.21)\n영식성향 = torch.tensor([1.20,0.5]).reshape(1,2)\n영식매력 = torch.tensor(1.2)\n((옥순성향*영식성향).sum() + 옥순매력 + 영식매력) # 옥순과 영식의 궁합: a ∘ b 로 내적구함 + 이후에 매력을 더함 \n(옥순성향 @ 영식성향.T + 옥순매력 + 영식매력) # 옥순과 영식의 궁합: a.T @ b 로 내적구함 + 이후에 매력을 더함\n\ntensor([[4.1190]])\n\n\n(2) 영자(IN)와 다호(ES)의 궁합\n\n영자성향 = torch.tensor([1.17,0.44]).reshape(1,2)\n영자매력 = torch.tensor(1.25).reshape(1,1)\n다호성향 = torch.tensor([-1.22,-0.6]).reshape(1,2)\n다호매력 = torch.tensor(1.15).reshape(1,1)\n((영자성향*다호성향).sum() + 영자성향 + 다호성향)\n(영자성향 @ 다호성향.T + 영자매력 + 다호매력)\n\ntensor([[0.7086]])\n\n\n(3) 하니(I)와 영호(IS)의 궁합\n\n하니성향 = torch.tensor([0.2,0]).reshape(1,2)\n하니매력 = torch.tensor(3.6)\n영호성향 = torch.tensor([1.23,-0.7]).reshape(1,2)\n영호매력 = torch.tensor(1.11)\n((하니성향 * 영호성향).sum() + 하니매력 + 영호매력)\n하니성향 @ 영호성향.T + 하니매력 + 영호매력\n\ntensor([[4.9560]])\n\n\n- 여자의 전체 설정값\n\n옥순성향 = torch.tensor([1.22,0.49]).reshape(1,2)\n영자성향 = torch.tensor([1.17,0.44]).reshape(1,2)\n정숙성향 = torch.tensor([1.21,-0.45]).reshape(1,2)\n영숙성향 = torch.tensor([1.20,-0.50]).reshape(1,2)\n순자성향 = torch.tensor([-1.20,0.51]).reshape(1,2)\n현숙성향 = torch.tensor([-1.23,0.48]).reshape(1,2)\n서연성향 = torch.tensor([-1.20,-0.48]).reshape(1,2)\n보람성향 = torch.tensor([-1.19,-0.49]).reshape(1,2)\n하니성향 = torch.tensor([0.2,0]).reshape(1,2)\nW = torch.concat([옥순성향,영자성향,정숙성향,영숙성향,순자성향,현숙성향,서연성향,보람성향,하니성향])\nb1 = torch.tensor([1.21,1.25,1.10,1.11,1.12,1.13,1.14,1.12,3.6]).reshape(-1,1) \nW,b1\n\n(tensor([[ 1.2200,  0.4900],\n         [ 1.1700,  0.4400],\n         [ 1.2100, -0.4500],\n         [ 1.2000, -0.5000],\n         [-1.2000,  0.5100],\n         [-1.2300,  0.4800],\n         [-1.2000, -0.4800],\n         [-1.1900, -0.4900],\n         [ 0.2000,  0.0000]]),\n tensor([[1.2100],\n         [1.2500],\n         [1.1000],\n         [1.1100],\n         [1.1200],\n         [1.1300],\n         [1.1400],\n         [1.1200],\n         [3.6000]]))\n\n\n- 남자의 전체 설정값\n\n영식성향 = torch.tensor([1.20,0.5]).reshape(1,2)\n영철성향 = torch.tensor([1.22,0.45]).reshape(1,2)\n영호성향 = torch.tensor([1.23,-0.7]).reshape(1,2)\n광수성향 = torch.tensor([1.21,-0.6]).reshape(1,2)\n상철성향 = torch.tensor([-1.28,0.6]).reshape(1,2)\n영수성향 = torch.tensor([-1.24,0.5]).reshape(1,2)\n규빈성향 = torch.tensor([-1.20,-0.5]).reshape(1,2)\n다호성향 = torch.tensor([-1.22,-0.6]).reshape(1,2)\nM = torch.concat([영식성향,영철성향,영호성향,광수성향,상철성향,영수성향,규빈성향,다호성향]) # 각 column은 남성출연자의 성향을 의미함\nb2 = torch.tensor([1.2,1.10,1.11,1.25,1.18,1.11,1.15,1.15]).reshape(-1,1)\nM,b2\n\n(tensor([[ 1.2000,  0.5000],\n         [ 1.2200,  0.4500],\n         [ 1.2300, -0.7000],\n         [ 1.2100, -0.6000],\n         [-1.2800,  0.6000],\n         [-1.2400,  0.5000],\n         [-1.2000, -0.5000],\n         [-1.2200, -0.6000]]),\n tensor([[1.2000],\n         [1.1000],\n         [1.1100],\n         [1.2500],\n         [1.1800],\n         [1.1100],\n         [1.1500],\n         [1.1500]]))\n\n\n\nW@M.T + (b1 + b2.T)\n\ntensor([[4.1190, 4.0189, 3.4776, 3.6422, 1.1224, 1.0522, 0.6510, 0.5776],\n        [4.0740, 3.9754, 3.4911, 3.6517, 1.1964, 1.1292, 0.7760, 0.7086],\n        [3.5270, 3.4737, 4.0133, 4.0841, 0.4612, 0.4846, 1.0230, 1.0438],\n        [3.5000, 3.4490, 4.0460, 4.1120, 0.4540, 0.4820, 1.0700, 1.0960],\n        [1.1350, 0.9855, 0.3970, 0.6120, 4.1420, 3.9730, 3.4550, 3.4280],\n        [1.0940, 0.9454, 0.3911, 0.6037, 4.1724, 4.0052, 3.5160, 3.4926],\n        [0.6600, 0.5600, 1.1100, 1.2260, 3.5680, 3.4980, 3.9700, 4.0420],\n        [0.6470, 0.5477, 1.1093, 1.2241, 3.5292, 3.4606, 3.9430, 4.0158],\n        [5.0400, 4.9440, 4.9560, 5.0920, 4.5240, 4.4620, 4.5100, 4.5060]])\n\n\n- 그렇다면 Nan값의 부분을 제외한 나머지 부분을 잘 맞춘다면 Nan부분도 잘 맞을테니까 그 부분에 Nan을 넣으면 되겠다.\n1 yhat을 어떻게 구하지? (여성특징 * 남성특징).sum() + 남성bias + 여성bias??\n2 그럼 여성특징, 남성특징, 여성매력, 남성매력은 어떻게 구하지? 생각해보니 데이터에서 주어진 것은 모든 것이 합쳐진 궁합의 점수가 나왔지 각각의 점수가 나오진 않았다\n3 그 전에는 어떻게 했지? W를 보고 적당히 특징을 찾거나 상상해서 여성특징, 여성bias의 값을 때려넣음 M를 보고 적당히 특징을 찾거나 상상해서 남성특징, 남성bias의 값을 때려넣음\n4 자동화하려면? W \\(\\to\\) 여성특징, 여성bias / M \\(\\to\\) 남성특징, 남성bias인 함수를 만들자\n5 y와 yhat의 차이를 loss로 잡고 loss.backward()후 더 나은 값들로 update를 해주면 된다.\n6 구상은 다 했고 이제 뭘 해야하지? 먼저 4에 해당하는 함수를 만들어야한다.\n옥순 \\(\\to\\) 옥순의 특징 = (1.22,0.49)\n옥순 \\(\\to\\) 옥순의 매력 = 1.22\n영철 \\(\\to\\) 영철의 특징 = (1.22,0.45)…\n7 epoc을 돌리기 위한 step1~4를 하면 된다."
  },
  {
    "objectID": "posts/DL_Summary6.html#c.-6의-구현---함수-생성",
    "href": "posts/DL_Summary6.html#c.-6의-구현---함수-생성",
    "title": "Deep Learning 6",
    "section": "C. 6의 구현 - 함수 생성",
    "text": "C. 6의 구현 - 함수 생성\n\ndf_view\n\n\n\n\n\n\n\n\n\n영식(IN)\n영철(IN)\n영호(IS)\n광수(IS)\n상철(EN)\n영수(EN)\n규빈(ES)\n다호(ES)\n\n\n\n\n옥순(IN)\nNaN\n4.02\n3.45\n3.42\n0.84\n1.12\n0.43\n0.49\n\n\n영자(IN)\n3.93\n3.99\n3.63\n3.43\n0.98\n0.96\n0.52\nNaN\n\n\n정숙(IS)\n3.52\n3.42\n4.05\n4.06\n0.39\nNaN\n0.93\n0.99\n\n\n영숙(IS)\n3.43\n3.57\nNaN\n3.95\n0.56\n0.52\n0.89\n0.89\n\n\n순자(EN)\n1.12\nNaN\n0.59\n0.43\n4.01\n4.16\n3.52\n3.38\n\n\n현숙(EN)\n0.94\n1.05\n0.32\n0.45\n4.02\n3.78\nNaN\n3.54\n\n\n서연(ES)\n0.51\n0.56\n0.88\n0.89\n3.50\n3.64\n4.04\n4.10\n\n\n보람(ES)\n0.48\n0.51\n1.03\nNaN\n3.52\n4.00\n3.82\nNaN\n\n\n하니(I)\n4.85\n4.82\nNaN\n4.98\n4.53\n4.39\n4.45\n4.52\n\n\n\n\n\n\n\n\n- dataframe의 변형\n\ndf_train = df_view.stack().reset_index().set_axis(['W','M','y'],axis=1)\ndf_train\n\n\n\n\n\n\n\n\n\nW\nM\ny\n\n\n\n\n0\n옥순(IN)\n영철(IN)\n4.02\n\n\n1\n옥순(IN)\n영호(IS)\n3.45\n\n\n2\n옥순(IN)\n광수(IS)\n3.42\n\n\n3\n옥순(IN)\n상철(EN)\n0.84\n\n\n4\n옥순(IN)\n영수(EN)\n1.12\n\n\n...\n...\n...\n...\n\n\n58\n하니(I)\n광수(IS)\n4.98\n\n\n59\n하니(I)\n상철(EN)\n4.53\n\n\n60\n하니(I)\n영수(EN)\n4.39\n\n\n61\n하니(I)\n규빈(ES)\n4.45\n\n\n62\n하니(I)\n다호(ES)\n4.52\n\n\n\n\n63 rows × 3 columns\n\n\n\n\n- 이름을 숫자화\n\nw = {'옥순(IN)':0, '영자(IN)':1, '정숙(IS)':2, '영숙(IS)':3, '순자(EN)':4, '현숙(EN)':5, '서연(ES)':6, '보람(ES)':7, '하니(I)':8}\nm = {'영식(IN)':0, '영철(IN)':1, '영호(IS)':2, '광수(IS)':3, '상철(EN)':4, '영수(EN)':5, '규빈(ES)':6, '다호(ES)':7}\n\n\ndf_train['X1'] = df_train['W'].map(w)\ndf_train['X2'] = df_train['M'].map(m)\ndf_train\n\n\n\n\n\n\n\n\n\nW\nM\ny\nX1\nX2\n\n\n\n\n0\n옥순(IN)\n영철(IN)\n4.02\n0\n1\n\n\n1\n옥순(IN)\n영호(IS)\n3.45\n0\n2\n\n\n2\n옥순(IN)\n광수(IS)\n3.42\n0\n3\n\n\n3\n옥순(IN)\n상철(EN)\n0.84\n0\n4\n\n\n4\n옥순(IN)\n영수(EN)\n1.12\n0\n5\n\n\n...\n...\n...\n...\n...\n...\n\n\n58\n하니(I)\n광수(IS)\n4.98\n8\n3\n\n\n59\n하니(I)\n상철(EN)\n4.53\n8\n4\n\n\n60\n하니(I)\n영수(EN)\n4.39\n8\n5\n\n\n61\n하니(I)\n규빈(ES)\n4.45\n8\n6\n\n\n62\n하니(I)\n다호(ES)\n4.52\n8\n7\n\n\n\n\n63 rows × 5 columns\n\n\n\n\n- 텐서화 + one_hot-encoding\n\ny = torch.tensor(df_train['y']).float()\nX1 = torch.tensor(df_train['X1'])\nX2 = torch.tensor(df_train['X2'])\nE1 = torch.nn.functional.one_hot(X1).float()\nE2 = torch.nn.functional.one_hot(X2).float()\n\n\nprint(f\"y.shape: {y.shape},\\t y.dtype: {y.dtype}\")\nprint(f\"X1.shape: {X1.shape},\\t X1.dtype: {X1.dtype} // X1.unique: {X1.unique()}\")\nprint(f\"X2.shape: {X2.shape},\\t X2.dtype: {X2.dtype} // X2.unique: {X2.unique()}\")\nprint(f\"E1.shape: {E1.shape},\\t E1.dtype: {E1.dtype} -- shape에서 9는 여성이 9명이라는 의미\")\nprint(f\"E2.shape: {E2.shape},\\t E2.dtype: {E2.dtype} -- shape에서 8은 남성이 8명이라는 의미\")\n\ny.shape: torch.Size([63]),   y.dtype: torch.float32\nX1.shape: torch.Size([63]),  X1.dtype: torch.int64 // X1.unique: tensor([0, 1, 2, 3, 4, 5, 6, 7, 8])\nX2.shape: torch.Size([63]),  X2.dtype: torch.int64 // X2.unique: tensor([0, 1, 2, 3, 4, 5, 6, 7])\nE1.shape: torch.Size([63, 9]),   E1.dtype: torch.float32 -- shape에서 9는 여성이 9명이라는 의미\nE2.shape: torch.Size([63, 8]),   E2.dtype: torch.float32 -- shape에서 8은 남성이 8명이라는 의미\n\n\n\nl1 = torch.nn.Linear(9,2,bias=False)\nb1 = torch.nn.Linear(9,1,bias=False)\nl2 = torch.nn.Linear(8,2,bias=False)\nb2 = torch.nn.Linear(8,1,bias=False)"
  },
  {
    "objectID": "posts/DL_Summary6.html#d.-7의-구현---step14-수행",
    "href": "posts/DL_Summary6.html#d.-7의-구현---step14-수행",
    "title": "Deep Learning 6",
    "section": "D. 7의 구현 - step1~4 수행",
    "text": "D. 7의 구현 - step1~4 수행\n- step1 : yhat을 구하자\n\ntorch.manual_seed(21345)\nl1 = torch.nn.Linear(9,2,bias=False)\nb1 = torch.nn.Linear(9,1,bias=False)\nl2 = torch.nn.Linear(8,2,bias=False)\nb2 = torch.nn.Linear(8,1,bias=False)\nW_feartures = l1(E1)\nW_bias = b1(E1)\nM_feartures = l2(E2)\nM_bias = b2(E2)\n\n\nsig = torch.nn.Sigmoid()\nscore = (W_feartures * M_feartures).sum(axis=1).reshape(-1,1) + W_bias + M_bias\nyhat = sig(score) * 5\n\n- step2 : 손실계산\n\nloss_fn = torch.nn.MSELoss()\nloss = loss_fn(yhat,y)\n\n/root/anaconda3/envs/torch/lib/python3.12/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([63])) that is different to the input size (torch.Size([63, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n  return F.mse_loss(input, target, reduction=self.reduction)\n\n\n- step3 : 미분\n(미분전)\n\nl1.weight.data , b1.weight.data , l2.weight.data , b2.weight.data\n\n(tensor([[ 0.2862,  0.0305,  0.2883,  0.1300,  0.0214,  0.1566, -0.2712, -0.2259,\n           0.1584],\n         [ 0.2416,  0.3247,  0.0731,  0.1431, -0.2673, -0.0753,  0.0428, -0.2460,\n           0.2979]]),\n tensor([[ 0.0574,  0.2875, -0.0112,  0.1881,  0.1786, -0.1876,  0.1947,  0.1514,\n           0.2904]]),\n tensor([[ 0.0118,  0.0754,  0.2837, -0.1152, -0.3053,  0.2444, -0.0010, -0.0854],\n         [-0.2651, -0.1892, -0.1229,  0.2027,  0.1869,  0.2188,  0.1728,  0.0379]]),\n tensor([[ 0.0776,  0.2288,  0.2265, -0.2496, -0.3322,  0.2682,  0.1394, -0.3488]]))\n\n\n\nl1.weight.grad , b1.weight.grad , l2.weight.grad , b2.weight.grad\n\n(None, None, None, None)\n\n\n(미분후)\n\nloss.backward()\n\n\nl1.weight.data , b1.weight.data , l2.weight.data , b2.weight.data\n\n(tensor([[ 0.2862,  0.0305,  0.2883,  0.1300,  0.0214,  0.1566, -0.2712, -0.2259,\n           0.1584],\n         [ 0.2416,  0.3247,  0.0731,  0.1431, -0.2673, -0.0753,  0.0428, -0.2460,\n           0.2979]]),\n tensor([[ 0.0574,  0.2875, -0.0112,  0.1881,  0.1786, -0.1876,  0.1947,  0.1514,\n           0.2904]]),\n tensor([[ 0.0118,  0.0754,  0.2837, -0.1152, -0.3053,  0.2444, -0.0010, -0.0854],\n         [-0.2651, -0.1892, -0.1229,  0.2027,  0.1869,  0.2188,  0.1728,  0.0379]]),\n tensor([[ 0.0776,  0.2288,  0.2265, -0.2496, -0.3322,  0.2682,  0.1394, -0.3488]]))\n\n\n\nl1.weight.grad , b1.weight.grad , l2.weight.grad , b2.weight.grad\n\n(tensor([[ 0.0175,  0.0141,  0.0139,  0.0106,  0.0152,  0.0152,  0.0121,  0.0122,\n           0.0091],\n         [-0.0030,  0.0006, -0.0103, -0.0007, -0.0039, -0.0091, -0.0027, -0.0055,\n           0.0025]]),\n tensor([[ 0.0216,  0.1110, -0.0177,  0.0512,  0.0416, -0.0646,  0.0722,  0.0682,\n           0.0850]]),\n tensor([[-0.0021,  0.0044,  0.0023, -0.0093, -0.0132,  0.0070,  0.0047, -0.0134],\n         [ 0.0035,  0.0157,  0.0029,  0.0026, -0.0002,  0.0176,  0.0136, -0.0030]]),\n tensor([[ 0.0780,  0.1257,  0.1090, -0.0452, -0.0912,  0.1570,  0.1201, -0.0850]]))\n\n\n- step4 : update\n\nparams = list(l1.parameters()) + list(l2.parameters()) + list(b1.parameters()) + list(b2.parameters())\noptimizr = torch.optim.Adam(params , lr=0.1)\n\nupdate전\n\nl1.weight.data, b1.weight.data, l2.weight.data, b2.weight.data\n\n(tensor([[ 0.2862,  0.0305,  0.2883,  0.1300,  0.0214,  0.1566, -0.2712, -0.2259,\n           0.1584],\n         [ 0.2416,  0.3247,  0.0731,  0.1431, -0.2673, -0.0753,  0.0428, -0.2460,\n           0.2979]]),\n tensor([[ 0.0574,  0.2875, -0.0112,  0.1881,  0.1786, -0.1876,  0.1947,  0.1514,\n           0.2904]]),\n tensor([[ 0.0118,  0.0754,  0.2837, -0.1152, -0.3053,  0.2444, -0.0010, -0.0854],\n         [-0.2651, -0.1892, -0.1229,  0.2027,  0.1869,  0.2188,  0.1728,  0.0379]]),\n tensor([[ 0.0776,  0.2288,  0.2265, -0.2496, -0.3322,  0.2682,  0.1394, -0.3488]]))\n\n\n\nl1.weight.grad, b1.weight.grad, l2.weight.grad, b2.weight.grad\n\n(tensor([[ 0.0175,  0.0141,  0.0139,  0.0106,  0.0152,  0.0152,  0.0121,  0.0122,\n           0.0091],\n         [-0.0030,  0.0006, -0.0103, -0.0007, -0.0039, -0.0091, -0.0027, -0.0055,\n           0.0025]]),\n tensor([[ 0.0216,  0.1110, -0.0177,  0.0512,  0.0416, -0.0646,  0.0722,  0.0682,\n           0.0850]]),\n tensor([[-0.0021,  0.0044,  0.0023, -0.0093, -0.0132,  0.0070,  0.0047, -0.0134],\n         [ 0.0035,  0.0157,  0.0029,  0.0026, -0.0002,  0.0176,  0.0136, -0.0030]]),\n tensor([[ 0.0780,  0.1257,  0.1090, -0.0452, -0.0912,  0.1570,  0.1201, -0.0850]]))\n\n\nupdate\n\noptimizr.step()\n\n\nl1.weight.data, b1.weight.data, l2.weight.data, b2.weight.data\n\n(tensor([[ 0.1862, -0.0695,  0.1883,  0.0300, -0.0786,  0.0566, -0.3712, -0.3259,\n           0.0584],\n         [ 0.3416,  0.2247,  0.1731,  0.2431, -0.1673,  0.0247,  0.1428, -0.1460,\n           0.1979]]),\n tensor([[-0.0426,  0.1875,  0.0888,  0.0881,  0.0786, -0.0876,  0.0947,  0.0514,\n           0.1904]]),\n tensor([[ 0.1118, -0.0246,  0.1837, -0.0152, -0.2053,  0.1444, -0.1010,  0.0146],\n         [-0.3651, -0.2892, -0.2229,  0.1027,  0.2869,  0.1188,  0.0728,  0.1379]]),\n tensor([[-0.0224,  0.1288,  0.1265, -0.1496, -0.2322,  0.1682,  0.0394, -0.2488]]))\n\n\n\nl1.weight.grad, b1.weight.grad, l2.weight.grad, b2.weight.grad\n\n(tensor([[ 0.0175,  0.0141,  0.0139,  0.0106,  0.0152,  0.0152,  0.0121,  0.0122,\n           0.0091],\n         [-0.0030,  0.0006, -0.0103, -0.0007, -0.0039, -0.0091, -0.0027, -0.0055,\n           0.0025]]),\n tensor([[ 0.0216,  0.1110, -0.0177,  0.0512,  0.0416, -0.0646,  0.0722,  0.0682,\n           0.0850]]),\n tensor([[-0.0021,  0.0044,  0.0023, -0.0093, -0.0132,  0.0070,  0.0047, -0.0134],\n         [ 0.0035,  0.0157,  0.0029,  0.0026, -0.0002,  0.0176,  0.0136, -0.0030]]),\n tensor([[ 0.0780,  0.1257,  0.1090, -0.0452, -0.0912,  0.1570,  0.1201, -0.0850]]))\n\n\nzero_grad\n\noptimizr.zero_grad()\n\n\nl1.weight.data, b1.weight.data, l2.weight.data, b2.weight.data\n\n(tensor([[ 0.1862, -0.0695,  0.1883,  0.0300, -0.0786,  0.0566, -0.3712, -0.3259,\n           0.0584],\n         [ 0.3416,  0.2247,  0.1731,  0.2431, -0.1673,  0.0247,  0.1428, -0.1460,\n           0.1979]]),\n tensor([[-0.0426,  0.1875,  0.0888,  0.0881,  0.0786, -0.0876,  0.0947,  0.0514,\n           0.1904]]),\n tensor([[ 0.1118, -0.0246,  0.1837, -0.0152, -0.2053,  0.1444, -0.1010,  0.0146],\n         [-0.3651, -0.2892, -0.2229,  0.1027,  0.2869,  0.1188,  0.0728,  0.1379]]),\n tensor([[-0.0224,  0.1288,  0.1265, -0.1496, -0.2322,  0.1682,  0.0394, -0.2488]]))\n\n\n\nl1.weight.grad, b1.weight.grad, l2.weight.grad, b2.weight.grad\n\n(None, None, None, None)"
  },
  {
    "objectID": "posts/DL_Summary6.html#e.-코드정리",
    "href": "posts/DL_Summary6.html#e.-코드정리",
    "title": "Deep Learning 6",
    "section": "E. 코드정리",
    "text": "E. 코드정리\n\ndf_view = pd.read_csv('https://raw.githubusercontent.com/guebin/DL2024/main/posts/solo.csv',index_col=0)\ndf_train = df_view.stack().reset_index().set_axis(['W','M','y'],axis=1)\nw = {'옥순(IN)':0, '영자(IN)':1, '정숙(IS)':2, '영숙(IS)':3, '순자(EN)':4, '현숙(EN)':5, '서연(ES)':6, '보람(ES)':7, '하니(I)':8}\nm = {'영식(IN)':0, '영철(IN)':1, '영호(IS)':2, '광수(IS)':3, '상철(EN)':4, '영수(EN)':5, '규빈(ES)':6, '다호(ES)':7}\nX1 = torch.tensor(df_train['W'].map(w)) # length-n vector \nX2 = torch.tensor(df_train['M'].map(m)) # length-n vector \nE1 = torch.nn.functional.one_hot(X1).float()\nE2 = torch.nn.functional.one_hot(X2).float()\ny = torch.tensor(df_train['y']).float().reshape(-1,1)\n#--#\ntorch.manual_seed(21345)\nl1 = torch.nn.Linear(in_features=9, out_features=2, bias=False)\nb1 = torch.nn.Linear(in_features=9, out_features=1, bias=False)\nl2 = torch.nn.Linear(in_features=8, out_features=2, bias=False)\nb2 = torch.nn.Linear(in_features=8, out_features=1, bias=False)\nsig = torch.nn.Sigmoid()\nloss_fn = torch.nn.MSELoss()\nparams = list(l1.parameters())+list(b1.parameters())+list(l2.parameters())+list(b2.parameters())\noptimizr = torch.optim.Adam(params, lr=0.1) \n#--#\nfor epoc in range(100):\n    ## step1 \n    W_features = l1(E1)\n    W_bias = b1(E1)\n    M_features = l2(E2) \n    M_bias = b2(E2)\n    score = (W_features * M_features).sum(axis=1).reshape(-1,1) + W_bias + M_bias\n    yhat = sig(score) * 5 \n    ## step2 \n    loss = loss_fn(yhat,y)\n    ## step3 \n    loss.backward()\n    ## step4 \n    optimizr.step()\n    optimizr.zero_grad()\n\n\ntorch.concat([yhat,y],axis=1)[::4] # 꽤 잘맞음\n\ntensor([[4.0965, 4.0200],\n        [0.9562, 1.1200],\n        [3.9986, 3.9900],\n        [0.9538, 0.9600],\n        [4.1112, 4.0500],\n        [0.9660, 0.9900],\n        [0.5756, 0.5600],\n        [1.1270, 1.1200],\n        [4.0830, 4.1600],\n        [1.0020, 1.0500],\n        [3.9902, 3.7800],\n        [0.9060, 0.8800],\n        [3.9527, 4.0400],\n        [0.9539, 1.0300],\n        [4.8095, 4.8500],\n        [4.4280, 4.3900]], grad_fn=&lt;SliceBackward0&gt;)"
  },
  {
    "objectID": "posts/DL_Summary6.html#f.-모형의-해석---쉬운-경우",
    "href": "posts/DL_Summary6.html#f.-모형의-해석---쉬운-경우",
    "title": "Deep Learning 6",
    "section": "F. 모형의 해석 - 쉬운 경우",
    "text": "F. 모형의 해석 - 쉬운 경우\n\ndf_match = pd.DataFrame((W_features*M_features).data).set_axis(['잠재특징궁합1','잠재특징궁합2'],axis=1)\ndf_bias = pd.DataFrame(torch.concat([W_bias, M_bias],axis=1).data).set_axis(['여성bias','남성bias'],axis=1)\ndf_features = pd.concat([df_train.loc[:,'W':'M'],df_match,df_bias],axis=1)\ndf_features[:56]\n\n\n\n\n\n\n\n\n\nW\nM\n잠재특징궁합1\n잠재특징궁합2\n여성bias\n남성bias\n\n\n\n\n0\n옥순(IN)\n영철(IN)\n1.715036\n-0.005762\n-0.516387\n0.318762\n\n\n1\n옥순(IN)\n영호(IS)\n1.449594\n-0.253216\n-0.516387\n0.139160\n\n\n2\n옥순(IN)\n광수(IS)\n1.433762\n-0.259794\n-0.516387\n0.062099\n\n\n3\n옥순(IN)\n상철(EN)\n-1.301104\n-0.058262\n-0.516387\n0.386921\n\n\n4\n옥순(IN)\n영수(EN)\n-1.338332\n-0.091116\n-0.516387\n0.503820\n\n\n5\n옥순(IN)\n규빈(ES)\n-1.587567\n-0.244833\n-0.516387\n0.233090\n\n\n6\n옥순(IN)\n다호(ES)\n-1.654613\n-0.271469\n-0.516387\n0.318994\n\n\n7\n영자(IN)\n영식(IN)\n1.740016\n-0.001178\n-0.620071\n0.235756\n\n\n8\n영자(IN)\n영철(IN)\n1.686631\n-0.000798\n-0.620071\n0.318762\n\n\n9\n영자(IN)\n영호(IS)\n1.425585\n-0.035055\n-0.620071\n0.139160\n\n\n10\n영자(IN)\n광수(IS)\n1.410015\n-0.035966\n-0.620071\n0.062099\n\n\n11\n영자(IN)\n상철(EN)\n-1.279555\n-0.008066\n-0.620071\n0.386921\n\n\n12\n영자(IN)\n영수(EN)\n-1.316166\n-0.012614\n-0.620071\n0.503820\n\n\n13\n영자(IN)\n규빈(ES)\n-1.561273\n-0.033895\n-0.620071\n0.233090\n\n\n14\n정숙(IS)\n영식(IN)\n1.840118\n0.040885\n-1.332004\n0.235756\n\n\n15\n정숙(IS)\n영철(IN)\n1.783662\n0.027690\n-1.332004\n0.318762\n\n\n16\n정숙(IS)\n영호(IS)\n1.507598\n1.216796\n-1.332004\n0.139160\n\n\n17\n정숙(IS)\n광수(IS)\n1.491133\n1.248404\n-1.332004\n0.062099\n\n\n18\n정숙(IS)\n상철(EN)\n-1.353167\n0.279969\n-1.332004\n0.386921\n\n\n19\n정숙(IS)\n규빈(ES)\n-1.651093\n1.176513\n-1.332004\n0.233090\n\n\n20\n정숙(IS)\n다호(ES)\n-1.720822\n1.304508\n-1.332004\n0.318994\n\n\n21\n영숙(IS)\n영식(IN)\n1.850272\n0.036922\n-1.318590\n0.235756\n\n\n22\n영숙(IS)\n영철(IN)\n1.793504\n0.025007\n-1.318590\n0.318762\n\n\n23\n영숙(IS)\n광수(IS)\n1.499361\n1.127417\n-1.318590\n0.062099\n\n\n24\n영숙(IS)\n상철(EN)\n-1.360634\n0.252837\n-1.318590\n0.386921\n\n\n25\n영숙(IS)\n영수(EN)\n-1.399565\n0.395411\n-1.318590\n0.503820\n\n\n26\n영숙(IS)\n규빈(ES)\n-1.660204\n1.062494\n-1.318590\n0.233090\n\n\n27\n영숙(IS)\n다호(ES)\n-1.730318\n1.178084\n-1.318590\n0.318994\n\n\n28\n순자(EN)\n영식(IN)\n-1.603807\n-0.036773\n0.170387\n0.235756\n\n\n29\n순자(EN)\n영호(IS)\n-1.313990\n-1.094429\n0.170387\n0.139160\n\n\n30\n순자(EN)\n광수(IS)\n-1.299639\n-1.122858\n0.170387\n0.062099\n\n\n31\n순자(EN)\n상철(EN)\n1.179391\n-0.251814\n0.170387\n0.386921\n\n\n32\n순자(EN)\n영수(EN)\n1.213136\n-0.393812\n0.170387\n0.503820\n\n\n33\n순자(EN)\n규빈(ES)\n1.439057\n-1.058197\n0.170387\n0.233090\n\n\n34\n순자(EN)\n다호(ES)\n1.499831\n-1.173319\n0.170387\n0.318994\n\n\n35\n현숙(EN)\n영식(IN)\n-1.682481\n-0.032957\n-0.049426\n0.235756\n\n\n36\n현숙(EN)\n영철(IN)\n-1.630861\n-0.022321\n-0.049426\n0.318762\n\n\n37\n현숙(EN)\n영호(IS)\n-1.378447\n-0.980869\n-0.049426\n0.139160\n\n\n38\n현숙(EN)\n광수(IS)\n-1.363392\n-1.006349\n-0.049426\n0.062099\n\n\n39\n현숙(EN)\n상철(EN)\n1.237246\n-0.225686\n-0.049426\n0.386921\n\n\n40\n현숙(EN)\n영수(EN)\n1.272646\n-0.352950\n-0.049426\n0.503820\n\n\n41\n현숙(EN)\n다호(ES)\n1.573404\n-1.051574\n-0.049426\n0.318994\n\n\n42\n서연(ES)\n영식(IN)\n-1.607879\n0.017862\n-0.861638\n0.235756\n\n\n43\n서연(ES)\n영철(IN)\n-1.558548\n0.012098\n-0.861638\n0.318762\n\n\n44\n서연(ES)\n영호(IS)\n-1.317326\n0.531613\n-0.861638\n0.139160\n\n\n45\n서연(ES)\n광수(IS)\n-1.302939\n0.545423\n-0.861638\n0.062099\n\n\n46\n서연(ES)\n상철(EN)\n1.182386\n0.122317\n-0.861638\n0.386921\n\n\n47\n서연(ES)\n영수(EN)\n1.216216\n0.191292\n-0.861638\n0.503820\n\n\n48\n서연(ES)\n규빈(ES)\n1.442710\n0.514014\n-0.861638\n0.233090\n\n\n49\n서연(ES)\n다호(ES)\n1.503639\n0.569934\n-0.861638\n0.318994\n\n\n50\n보람(ES)\n영식(IN)\n-1.590716\n0.015779\n-0.750431\n0.235756\n\n\n51\n보람(ES)\n영철(IN)\n-1.541911\n0.010687\n-0.750431\n0.318762\n\n\n52\n보람(ES)\n영호(IS)\n-1.303264\n0.469614\n-0.750431\n0.139160\n\n\n53\n보람(ES)\n상철(EN)\n1.169764\n0.108052\n-0.750431\n0.386921\n\n\n54\n보람(ES)\n영수(EN)\n1.203233\n0.168983\n-0.750431\n0.503820\n\n\n55\n보람(ES)\n규빈(ES)\n1.427310\n0.454067\n-0.750431\n0.233090\n\n\n\n\n\n\n\n\n\ndf_features[56:]\n\n\n\n\n\n\n\n\n\nW\nM\n잠재특징궁합1\n잠재특징궁합2\n여성bias\n남성bias\n\n\n\n\n56\n하니(I)\n영식(IN)\n1.007862\n0.032956\n1.95212\n0.235756\n\n\n57\n하니(I)\n영철(IN)\n0.976940\n0.022320\n1.95212\n0.318762\n\n\n58\n하니(I)\n광수(IS)\n0.816717\n1.006302\n1.95212\n0.062099\n\n\n59\n하니(I)\n상철(EN)\n-0.741151\n0.225675\n1.95212\n0.386921\n\n\n60\n하니(I)\n영수(EN)\n-0.762357\n0.352934\n1.95212\n0.503820\n\n\n61\n하니(I)\n규빈(ES)\n-0.904330\n0.948353\n1.95212\n0.233090\n\n\n62\n하니(I)\n다호(ES)\n-0.942522\n1.051526\n1.95212\n0.318994"
  },
  {
    "objectID": "posts/DL_Summary6.html#g.-모형의-해석---어려운-경우",
    "href": "posts/DL_Summary6.html#g.-모형의-해석---어려운-경우",
    "title": "Deep Learning 6",
    "section": "G. 모형의 해석 - 어려운 경우",
    "text": "G. 모형의 해석 - 어려운 경우\n\ndf_view = pd.read_csv('https://raw.githubusercontent.com/guebin/DL2024/main/posts/solo.csv',index_col=0)\ndf_train = df_view.stack().reset_index().set_axis(['W','M','y'],axis=1)\nw = {'옥순(IN)':0, '영자(IN)':1, '정숙(IS)':2, '영숙(IS)':3, '순자(EN)':4, '현숙(EN)':5, '서연(ES)':6, '보람(ES)':7, '하니(I)':8}\nm = {'영식(IN)':0, '영철(IN)':1, '영호(IS)':2, '광수(IS)':3, '상철(EN)':4, '영수(EN)':5, '규빈(ES)':6, '다호(ES)':7}\nX1 = torch.tensor(df_train['W'].map(w)) # length-n vector \nX2 = torch.tensor(df_train['M'].map(m)) # length-n vector \nE1 = torch.nn.functional.one_hot(X1).float()\nE2 = torch.nn.functional.one_hot(X2).float()\ny = torch.tensor(df_train['y']).float().reshape(-1,1)\n#--#\ntorch.manual_seed(8)\nl1 = torch.nn.Linear(in_features=9, out_features=2, bias=False)\nb1 = torch.nn.Linear(in_features=9, out_features=1, bias=False)\nl2 = torch.nn.Linear(in_features=8, out_features=2, bias=False)\nb2 = torch.nn.Linear(in_features=8, out_features=1, bias=False)\nsig = torch.nn.Sigmoid()\nloss_fn = torch.nn.MSELoss()\nparams = list(l1.parameters())+list(b1.parameters())+list(l2.parameters())+list(b2.parameters())\noptimizr = torch.optim.Adam(params, lr=0.1) \n#--#\nfor epoc in range(100):\n    ## step1 \n    W_features = l1(E1)\n    W_bias = b1(E1)\n    M_features = l2(E2) \n    M_bias = b2(E2)\n    score = (W_features * M_features).sum(axis=1).reshape(-1,1) + W_bias + M_bias\n    yhat = sig(score) * 5 \n    ## step2 \n    loss = loss_fn(yhat,y)\n    ## step3 \n    loss.backward()\n    ## step4 \n    optimizr.step()\n    optimizr.zero_grad()\n\n\ntorch.concat([yhat,y],axis=1)[::4] # 꽤 잘맞음\n\ntensor([[4.0386, 4.0200],\n        [0.9623, 1.1200],\n        [3.9990, 3.9900],\n        [0.9883, 0.9600],\n        [4.0799, 4.0500],\n        [0.9734, 0.9900],\n        [0.5064, 0.5600],\n        [1.0738, 1.1200],\n        [4.1248, 4.1600],\n        [0.9569, 1.0500],\n        [4.0001, 3.7800],\n        [0.8943, 0.8800],\n        [4.0257, 4.0400],\n        [0.8481, 1.0300],\n        [4.8558, 4.8500],\n        [4.5425, 4.3900]], grad_fn=&lt;SliceBackward0&gt;)\n\n\n\ndf_match = pd.DataFrame((W_features*M_features).data).set_axis(['잠재특징궁합1','잠재특징궁합2'],axis=1)\ndf_bias = pd.DataFrame(torch.concat([W_bias, M_bias],axis=1).data).set_axis(['여성bias','남성bias'],axis=1)\ndf_features = pd.concat([df_train.loc[:,'W':'M'],df_match,df_bias],axis=1)\ndf_features[:56]\n\n\n\n\n\n\n\n\n\nW\nM\n잠재특징궁합1\n잠재특징궁합2\n여성bias\n남성bias\n\n\n\n\n0\n옥순(IN)\n영철(IN)\n0.302724\n1.411065\n-0.983853\n0.705386\n\n\n1\n옥순(IN)\n영호(IS)\n0.743988\n0.439198\n-0.983853\n0.618197\n\n\n2\n옥순(IN)\n광수(IS)\n0.776415\n0.382274\n-0.983853\n0.545838\n\n\n3\n옥순(IN)\n상철(EN)\n-0.992729\n-0.430939\n-0.983853\n0.886856\n\n\n4\n옥순(IN)\n영수(EN)\n-0.942706\n-0.539754\n-0.983853\n1.032223\n\n\n5\n옥순(IN)\n규빈(ES)\n-0.569901\n-1.434720\n-0.983853\n0.853764\n\n\n6\n옥순(IN)\n다호(ES)\n-0.556357\n-1.467226\n-0.983853\n0.944640\n\n\n7\n영자(IN)\n영식(IN)\n0.323436\n1.356050\n-0.930421\n0.651731\n\n\n8\n영자(IN)\n영철(IN)\n0.324665\n1.285471\n-0.930421\n0.705386\n\n\n9\n영자(IN)\n영호(IS)\n0.797912\n0.400107\n-0.930421\n0.618197\n\n\n10\n영자(IN)\n광수(IS)\n0.832689\n0.348249\n-0.930421\n0.545838\n\n\n11\n영자(IN)\n상철(EN)\n-1.064682\n-0.392583\n-0.930421\n0.886856\n\n\n12\n영자(IN)\n영수(EN)\n-1.011034\n-0.491713\n-0.930421\n1.032223\n\n\n13\n영자(IN)\n규빈(ES)\n-0.611208\n-1.307021\n-0.930421\n0.853764\n\n\n14\n정숙(IS)\n영식(IN)\n0.659914\n0.304652\n-0.846774\n0.651731\n\n\n15\n정숙(IS)\n영철(IN)\n0.662421\n0.288796\n-0.846774\n0.705386\n\n\n16\n정숙(IS)\n영호(IS)\n1.627999\n0.089888\n-0.846774\n0.618197\n\n\n17\n정숙(IS)\n광수(IS)\n1.698955\n0.078238\n-0.846774\n0.545838\n\n\n18\n정숙(IS)\n상철(EN)\n-2.172296\n-0.088198\n-0.846774\n0.886856\n\n\n19\n정숙(IS)\n규빈(ES)\n-1.247061\n-0.293637\n-0.846774\n0.853764\n\n\n20\n정숙(IS)\n다호(ES)\n-1.217423\n-0.300290\n-0.846774\n0.944640\n\n\n21\n영숙(IS)\n영식(IN)\n0.628118\n0.407719\n-0.884238\n0.651731\n\n\n22\n영숙(IS)\n영철(IN)\n0.630504\n0.386498\n-0.884238\n0.705386\n\n\n23\n영숙(IS)\n광수(IS)\n1.617095\n0.104707\n-0.884238\n0.545838\n\n\n24\n영숙(IS)\n상철(EN)\n-2.067629\n-0.118037\n-0.884238\n0.886856\n\n\n25\n영숙(IS)\n영수(EN)\n-1.963442\n-0.147842\n-0.884238\n1.032223\n\n\n26\n영숙(IS)\n규빈(ES)\n-1.186974\n-0.392978\n-0.884238\n0.853764\n\n\n27\n영숙(IS)\n다호(ES)\n-1.158764\n-0.401881\n-0.884238\n0.944640\n\n\n28\n순자(EN)\n영식(IN)\n-0.553524\n-0.133968\n-1.260765\n0.651731\n\n\n29\n순자(EN)\n영호(IS)\n-1.365535\n-0.039528\n-1.260765\n0.618197\n\n\n30\n순자(EN)\n광수(IS)\n-1.425051\n-0.034405\n-1.260765\n0.545838\n\n\n31\n순자(EN)\n상철(EN)\n1.822081\n0.038784\n-1.260765\n0.886856\n\n\n32\n순자(EN)\n영수(EN)\n1.730267\n0.048578\n-1.260765\n1.032223\n\n\n33\n순자(EN)\n규빈(ES)\n1.046011\n0.129124\n-1.260765\n0.853764\n\n\n34\n순자(EN)\n다호(ES)\n1.021151\n0.132050\n-1.260765\n0.944640\n\n\n35\n현숙(EN)\n영식(IN)\n-0.521171\n-0.265791\n-1.371327\n0.651731\n\n\n36\n현숙(EN)\n영철(IN)\n-0.523151\n-0.251958\n-1.371327\n0.705386\n\n\n37\n현숙(EN)\n영호(IS)\n-1.285721\n-0.078422\n-1.371327\n0.618197\n\n\n38\n현숙(EN)\n광수(IS)\n-1.341758\n-0.068258\n-1.371327\n0.545838\n\n\n39\n현숙(EN)\n상철(EN)\n1.715582\n0.076948\n-1.371327\n0.886856\n\n\n40\n현숙(EN)\n영수(EN)\n1.629135\n0.096378\n-1.371327\n1.032223\n\n\n41\n현숙(EN)\n다호(ES)\n0.961466\n0.261986\n-1.371327\n0.944640\n\n\n42\n서연(ES)\n영식(IN)\n-0.218469\n-1.394457\n-1.191920\n0.651731\n\n\n43\n서연(ES)\n영철(IN)\n-0.219299\n-1.321879\n-1.191920\n0.705386\n\n\n44\n서연(ES)\n영호(IS)\n-0.538959\n-0.411439\n-1.191920\n0.618197\n\n\n45\n서연(ES)\n광수(IS)\n-0.562449\n-0.358112\n-1.191920\n0.545838\n\n\n46\n서연(ES)\n상철(EN)\n0.719152\n0.403702\n-1.191920\n0.886856\n\n\n47\n서연(ES)\n영수(EN)\n0.682914\n0.505639\n-1.191920\n1.032223\n\n\n48\n서연(ES)\n규빈(ES)\n0.412847\n1.344040\n-1.191920\n0.853764\n\n\n49\n서연(ES)\n다호(ES)\n0.403035\n1.374491\n-1.191920\n0.944640\n\n\n50\n보람(ES)\n영식(IN)\n-0.269182\n-1.233859\n-1.178461\n0.651731\n\n\n51\n보람(ES)\n영철(IN)\n-0.270204\n-1.169639\n-1.178461\n0.705386\n\n\n52\n보람(ES)\n영호(IS)\n-0.664067\n-0.364054\n-1.178461\n0.618197\n\n\n53\n보람(ES)\n상철(EN)\n0.886088\n0.357208\n-1.178461\n0.886856\n\n\n54\n보람(ES)\n영수(EN)\n0.841439\n0.447405\n-1.178461\n1.032223\n\n\n55\n보람(ES)\n규빈(ES)\n0.508681\n1.189248\n-1.178461\n0.853764\n\n\n\n\n\n\n\n\n\ndf_features[56:]\n\n\n\n\n\n\n\n\n\nW\nM\n잠재특징궁합1\n잠재특징궁합2\n여성bias\n남성bias\n\n\n\n\n56\n하니(I)\n영식(IN)\n0.209820\n0.540235\n2.115016\n0.651731\n\n\n57\n하니(I)\n영철(IN)\n0.210617\n0.512117\n2.115016\n0.705386\n\n\n58\n하니(I)\n광수(IS)\n0.540183\n0.138738\n2.115016\n0.545838\n\n\n59\n하니(I)\n상철(EN)\n-0.690681\n-0.156401\n2.115016\n0.886856\n\n\n60\n하니(I)\n영수(EN)\n-0.655878\n-0.195893\n2.115016\n1.032223\n\n\n61\n하니(I)\n규빈(ES)\n-0.396503\n-0.520703\n2.115016\n0.853764\n\n\n62\n하니(I)\n다호(ES)\n-0.387079\n-0.532500\n2.115016\n0.944640\n\n\n\n\n\n\n\n\n\ndf_features[df_features.W.str.contains('IN') & df_features.M.str.contains('ES')]\n\n\n\n\n\n\n\n\n\nW\nM\n잠재특징궁합1\n잠재특징궁합2\n여성bias\n남성bias\n\n\n\n\n5\n옥순(IN)\n규빈(ES)\n-0.569901\n-1.434720\n-0.983853\n0.853764\n\n\n6\n옥순(IN)\n다호(ES)\n-0.556357\n-1.467226\n-0.983853\n0.944640\n\n\n13\n영자(IN)\n규빈(ES)\n-0.611208\n-1.307021\n-0.930421\n0.853764"
  },
  {
    "objectID": "posts/RL1.html",
    "href": "posts/RL1.html",
    "title": "RL1",
    "section": "",
    "text": "import numpy as np"
  },
  {
    "objectID": "posts/RL1.html#a.-게임설명-및-원시코드",
    "href": "posts/RL1.html#a.-게임설명-및-원시코드",
    "title": "RL1",
    "section": "A. 게임설명 및 원시코드",
    "text": "A. 게임설명 및 원시코드\n- 버튼0 -&gt; 1의 보상 , 버튼2 -&gt; 10의 보상\n- 처음에는 아는 것이 없으니 아무거나 눌러봐야겠다.\n\naction_space = [0,1]\naction = np.random.choice(action_space)\naction\n\n1\n\n\n\naction_space와 action이라는 용어를 기억\n\n- 버튼을 누른 행위에 대한 보상 구현\n\nreward = 1 if action == 0 else 10\nreward\n\n10\n\n\n- 아무 버튼이나 10번 눌러보며 데이터 쌓기\n\naction_space = [0,1]\nfor l in range(10):\n    action = np.random.choice(action_space)\n    reward = 1 if action == 0 else 10\n    print(action , reward)\n\n1 10\n0 1\n0 1\n0 1\n1 10\n0 1\n0 1\n1 10\n0 1\n1 10\n\n\n- 사람이라면 1을 눌러야겠다는 생각을 함. 그런데 컴퓨터가 이렇게 생각하게 어떻게 만들지? 생각하는 과정을 연구한 것이 강화학습이다\n\n# 꺠달은 사람\naction_space = ['버튼0','버튼1']\nfor _ in range(10):\n    action = '버튼1'\n    reward = 1 if action == \"버튼0\" else 10\n    print(action,reward)\n\n버튼1 10\n버튼1 10\n버튼1 10\n버튼1 10\n버튼1 10\n버튼1 10\n버튼1 10\n버튼1 10\n버튼1 10\n버튼1 10\n\n\n- 강화학습 : 환경(enviornment)를 이해 -&gt; 에이전트(agent)가 행동(action)을 결정\n\n게임클리어조건: (1) 20번은 그냥 진행 (2) 최근 20번의 보상의 평균이 9.5점 이상이면 게임이 클리어 되었다고 생각하자.\n\n- 원시코드1 : 환경을 이해하지 못한 에이전트 - 게임 클리어 불가능\n\naction_space = [0,1]\nactions = []\nrewards = []\nfor t in range(1,51):\n    action = np.random.choice(action_space)\n    reward = 1 if action == 0 else 10\n    actions.append(action)\n    rewards.append(reward)\n\n    print(\n        f\"시도:{t}\\t\"\n        f\"행동:{action}\\t\"\n        f\"보상:{reward}\\t\"\n        f\"최근20번보상평균:{np.mean(rewards[-20:]):.4f}\\t\"\n    )\n\n    if t&lt;20:\n        pass\n    elif t==20:\n        print('--')\n    else:\n        if np.mean(rewards[-20:]) &gt; 9.5:\n            print('Game Clear')\n            break\n\n시도:1    행동:1    보상:10   최근20번보상평균:10.0000   \n시도:2    행동:0    보상:1    최근20번보상평균:5.5000    \n시도:3    행동:1    보상:10   최근20번보상평균:7.0000    \n시도:4    행동:0    보상:1    최근20번보상평균:5.5000    \n시도:5    행동:0    보상:1    최근20번보상평균:4.6000    \n시도:6    행동:1    보상:10   최근20번보상평균:5.5000    \n시도:7    행동:0    보상:1    최근20번보상평균:4.8571    \n시도:8    행동:0    보상:1    최근20번보상평균:4.3750    \n시도:9    행동:0    보상:1    최근20번보상평균:4.0000    \n시도:10   행동:1    보상:10   최근20번보상평균:4.6000    \n시도:11   행동:0    보상:1    최근20번보상평균:4.2727    \n시도:12   행동:0    보상:1    최근20번보상평균:4.0000    \n시도:13   행동:0    보상:1    최근20번보상평균:3.7692    \n시도:14   행동:0    보상:1    최근20번보상평균:3.5714    \n시도:15   행동:0    보상:1    최근20번보상평균:3.4000    \n시도:16   행동:1    보상:10   최근20번보상평균:3.8125    \n시도:17   행동:0    보상:1    최근20번보상평균:3.6471    \n시도:18   행동:0    보상:1    최근20번보상평균:3.5000    \n시도:19   행동:0    보상:1    최근20번보상평균:3.3684    \n시도:20   행동:1    보상:10   최근20번보상평균:3.7000    \n--\n시도:21   행동:1    보상:10   최근20번보상평균:3.7000    \n시도:22   행동:0    보상:1    최근20번보상평균:3.7000    \n시도:23   행동:1    보상:10   최근20번보상평균:3.7000    \n시도:24   행동:0    보상:1    최근20번보상평균:3.7000    \n시도:25   행동:0    보상:1    최근20번보상평균:3.7000    \n시도:26   행동:0    보상:1    최근20번보상평균:3.2500    \n시도:27   행동:0    보상:1    최근20번보상평균:3.2500    \n시도:28   행동:0    보상:1    최근20번보상평균:3.2500    \n시도:29   행동:1    보상:10   최근20번보상평균:3.7000    \n시도:30   행동:1    보상:10   최근20번보상평균:3.7000    \n시도:31   행동:1    보상:10   최근20번보상평균:4.1500    \n시도:32   행동:0    보상:1    최근20번보상평균:4.1500    \n시도:33   행동:0    보상:1    최근20번보상평균:4.1500    \n시도:34   행동:0    보상:1    최근20번보상평균:4.1500    \n시도:35   행동:1    보상:10   최근20번보상평균:4.6000    \n시도:36   행동:0    보상:1    최근20번보상평균:4.1500    \n시도:37   행동:1    보상:10   최근20번보상평균:4.6000    \n시도:38   행동:1    보상:10   최근20번보상평균:5.0500    \n시도:39   행동:0    보상:1    최근20번보상평균:5.0500    \n시도:40   행동:0    보상:1    최근20번보상평균:4.6000    \n시도:41   행동:0    보상:1    최근20번보상평균:4.1500    \n시도:42   행동:0    보상:1    최근20번보상평균:4.1500    \n시도:43   행동:0    보상:1    최근20번보상평균:3.7000    \n시도:44   행동:0    보상:1    최근20번보상평균:3.7000    \n시도:45   행동:1    보상:10   최근20번보상평균:4.1500    \n시도:46   행동:0    보상:1    최근20번보상평균:4.1500    \n시도:47   행동:0    보상:1    최근20번보상평균:4.1500    \n시도:48   행동:1    보상:10   최근20번보상평균:4.6000    \n시도:49   행동:1    보상:10   최근20번보상평균:4.6000    \n시도:50   행동:0    보상:1    최근20번보상평균:4.1500    \n\n\n- 원시코드2 : 환경을 깨달은 에이전트 - 게임 클리어\n\naction_space = [0,1]\nactions = []\nrewards = []\nfor t in range(1,51):\n    action = 1\n    reward = 1 if action == 0 else 10\n    actions.append(action)\n    rewards.append(reward)\n\n    print(\n        f\"시도:{t}\\t\"\n        f\"행동:{action}\\t\"\n        f\"보상:{reward}\\t\"\n        f\"최근20번보상평균:{np.mean(rewards[-20:]):.4f}\\t\"\n    )\n\n    if t&lt;20:\n        pass\n    elif t==20:\n        print('--')\n    else:\n        if np.mean(rewards[-20:]) &gt; 9.5:\n            print('Game Clear')\n            break\n\n시도:1    행동:1    보상:10   최근20번보상평균:10.0000   \n시도:2    행동:1    보상:10   최근20번보상평균:10.0000   \n시도:3    행동:1    보상:10   최근20번보상평균:10.0000   \n시도:4    행동:1    보상:10   최근20번보상평균:10.0000   \n시도:5    행동:1    보상:10   최근20번보상평균:10.0000   \n시도:6    행동:1    보상:10   최근20번보상평균:10.0000   \n시도:7    행동:1    보상:10   최근20번보상평균:10.0000   \n시도:8    행동:1    보상:10   최근20번보상평균:10.0000   \n시도:9    행동:1    보상:10   최근20번보상평균:10.0000   \n시도:10   행동:1    보상:10   최근20번보상평균:10.0000   \n시도:11   행동:1    보상:10   최근20번보상평균:10.0000   \n시도:12   행동:1    보상:10   최근20번보상평균:10.0000   \n시도:13   행동:1    보상:10   최근20번보상평균:10.0000   \n시도:14   행동:1    보상:10   최근20번보상평균:10.0000   \n시도:15   행동:1    보상:10   최근20번보상평균:10.0000   \n시도:16   행동:1    보상:10   최근20번보상평균:10.0000   \n시도:17   행동:1    보상:10   최근20번보상평균:10.0000   \n시도:18   행동:1    보상:10   최근20번보상평균:10.0000   \n시도:19   행동:1    보상:10   최근20번보상평균:10.0000   \n시도:20   행동:1    보상:10   최근20번보상평균:10.0000   \n--\n시도:21   행동:1    보상:10   최근20번보상평균:10.0000   \nGame Clear"
  },
  {
    "objectID": "posts/RL1.html#b.-수정1-env구현",
    "href": "posts/RL1.html#b.-수정1-env구현",
    "title": "RL1",
    "section": "B. 수정1 : Env구현",
    "text": "B. 수정1 : Env구현\n- Bandit 클래스 선언 + .step()구현\n\nclass Bandit:\n    def step(self,agent_action):\n        reward = 1 if agent_action == 0 else 10\n        return reward\n\n\nenv = Bandit()\naction_space = [0,1]\nactions = []\nrewards = []\nfor t in range(1,51):\n    action = np.random.choice(action_space)\n    reward = env.step(action)\n    actions.append(action)\n    rewards.append(reward)\n\n    print(\n        f\"시도:{t}\\t\"\n        f\"행동:{action}\\t\"\n        f\"보상:{reward}\\t\"\n        f\"최근20번보상평균:{np.mean(rewards[-20:]):.4f}\\t\"\n    )\n    if t&lt;20:\n        pass \n    elif t==20:\n        print(\"--\")\n    else: \n        if np.mean(rewards[-20:]) &gt; 9.5:\n            print(\"Game Clear\")\n            break\n\n시도:1    행동:1    보상:10   최근20번보상평균:10.0000   \n시도:2    행동:1    보상:10   최근20번보상평균:10.0000   \n시도:3    행동:1    보상:10   최근20번보상평균:10.0000   \n시도:4    행동:1    보상:10   최근20번보상평균:10.0000   \n시도:5    행동:1    보상:10   최근20번보상평균:10.0000   \n시도:6    행동:0    보상:1    최근20번보상평균:8.5000    \n시도:7    행동:0    보상:1    최근20번보상평균:7.4286    \n시도:8    행동:0    보상:1    최근20번보상평균:6.6250    \n시도:9    행동:1    보상:10   최근20번보상평균:7.0000    \n시도:10   행동:0    보상:1    최근20번보상평균:6.4000    \n시도:11   행동:1    보상:10   최근20번보상평균:6.7273    \n시도:12   행동:0    보상:1    최근20번보상평균:6.2500    \n시도:13   행동:1    보상:10   최근20번보상평균:6.5385    \n시도:14   행동:0    보상:1    최근20번보상평균:6.1429    \n시도:15   행동:1    보상:10   최근20번보상평균:6.4000    \n시도:16   행동:0    보상:1    최근20번보상평균:6.0625    \n시도:17   행동:1    보상:10   최근20번보상평균:6.2941    \n시도:18   행동:1    보상:10   최근20번보상평균:6.5000    \n시도:19   행동:0    보상:1    최근20번보상평균:6.2105    \n시도:20   행동:0    보상:1    최근20번보상평균:5.9500    \n--\n시도:21   행동:1    보상:10   최근20번보상평균:5.9500    \n시도:22   행동:1    보상:10   최근20번보상평균:5.9500    \n시도:23   행동:1    보상:10   최근20번보상평균:5.9500    \n시도:24   행동:0    보상:1    최근20번보상평균:5.5000    \n시도:25   행동:1    보상:10   최근20번보상평균:5.5000    \n시도:26   행동:0    보상:1    최근20번보상평균:5.5000    \n시도:27   행동:1    보상:10   최근20번보상평균:5.9500    \n시도:28   행동:0    보상:1    최근20번보상평균:5.9500    \n시도:29   행동:1    보상:10   최근20번보상평균:5.9500    \n시도:30   행동:0    보상:1    최근20번보상평균:5.9500    \n시도:31   행동:1    보상:10   최근20번보상평균:5.9500    \n시도:32   행동:1    보상:10   최근20번보상평균:6.4000    \n시도:33   행동:1    보상:10   최근20번보상평균:6.4000    \n시도:34   행동:0    보상:1    최근20번보상평균:6.4000    \n시도:35   행동:0    보상:1    최근20번보상평균:5.9500    \n시도:36   행동:1    보상:10   최근20번보상평균:6.4000    \n시도:37   행동:1    보상:10   최근20번보상평균:6.4000    \n시도:38   행동:1    보상:10   최근20번보상평균:6.4000    \n시도:39   행동:0    보상:1    최근20번보상평균:6.4000    \n시도:40   행동:0    보상:1    최근20번보상평균:6.4000    \n시도:41   행동:0    보상:1    최근20번보상평균:5.9500    \n시도:42   행동:0    보상:1    최근20번보상평균:5.5000    \n시도:43   행동:1    보상:10   최근20번보상평균:5.5000    \n시도:44   행동:1    보상:10   최근20번보상평균:5.9500    \n시도:45   행동:1    보상:10   최근20번보상평균:5.9500    \n시도:46   행동:0    보상:1    최근20번보상평균:5.9500    \n시도:47   행동:0    보상:1    최근20번보상평균:5.5000    \n시도:48   행동:0    보상:1    최근20번보상평균:5.5000    \n시도:49   행동:0    보상:1    최근20번보상평균:5.0500    \n시도:50   행동:0    보상:1    최근20번보상평균:5.0500"
  },
  {
    "objectID": "posts/RL1.html#c.-수정2-agent구현-인간지능",
    "href": "posts/RL1.html#c.-수정2-agent구현-인간지능",
    "title": "RL1",
    "section": "C. 수정2 : Agent구현 (인간지능)",
    "text": "C. 수정2 : Agent구현 (인간지능)\n- Agent 클래스 설계\n\n액션을 하고, 본인의 행동과 환경에서 받은 reward를 기억\n.act()함수와 .save_experience()함수 구현\n\n\nclass Agent:\n    def __init__(self):\n        self.action_space = [0,1]\n        self.action = None\n        self.reward = None\n        self.actions = []\n        self.rewards = []\n    def act(self):\n        self.action = 1\n    def save_experience(self):\n        self.actions.append(self.action)\n        self.rewards.append(self.reward)\n\n시점0 : init\n\nagent = Agent()\nenv = Bandit()\n\n\nagent.action, agent.reward, agent.actions, agent.rewards\n\n(None, None, [], [])\n\n\n시점1 : agent가 action을 선택\n\nagent.act()\n\n\nagent.action, agent.reward, agent.actions, agent.rewards\n\n(1, None, [], [])\n\n\n시점2: env가 agent에게 보상을 줌\n\nagent.reward = env.step(agent.action)\n\n\nagent.action, agent.reward, agent.actions, agent.rewards\n\n(1, 10, [], [])\n\n\n시점3: 경험을 저장\n\nagent.save_experience()\n\n\nagent.action, agent.reward, agent.actions, agent.rewards\n\n(1, 10, [1], [10])\n\n\n-전체코드-\n\nenv = Bandit()\nagent = Agent()\nfor t in range(1,51):\n    agent.act()\n    agent.reward = env.step(agent.action)\n    agent.save_experience()\n\n    print(\n        f\"시도:{t}\\t\"\n        f\"행동:{agent.action}\\t\"\n        f\"보상:{agent.reward}\\t\"\n        f\"최근20번보상평균:{np.mean(agent.rewards[-20:]):.4f}\\t\"\n    )\n    if t&lt;20:\n        pass \n    elif t==20:\n        print(\"--\")\n    else: \n        if np.mean(agent.rewards[-20:]) &gt; 9.5:\n            print(\"Game Clear\")\n            break    \n\n시도:1    행동:1    보상:10   최근20번보상평균:10.0000   \n시도:2    행동:1    보상:10   최근20번보상평균:10.0000   \n시도:3    행동:1    보상:10   최근20번보상평균:10.0000   \n시도:4    행동:1    보상:10   최근20번보상평균:10.0000   \n시도:5    행동:1    보상:10   최근20번보상평균:10.0000   \n시도:6    행동:1    보상:10   최근20번보상평균:10.0000   \n시도:7    행동:1    보상:10   최근20번보상평균:10.0000   \n시도:8    행동:1    보상:10   최근20번보상평균:10.0000   \n시도:9    행동:1    보상:10   최근20번보상평균:10.0000   \n시도:10   행동:1    보상:10   최근20번보상평균:10.0000   \n시도:11   행동:1    보상:10   최근20번보상평균:10.0000   \n시도:12   행동:1    보상:10   최근20번보상평균:10.0000   \n시도:13   행동:1    보상:10   최근20번보상평균:10.0000   \n시도:14   행동:1    보상:10   최근20번보상평균:10.0000   \n시도:15   행동:1    보상:10   최근20번보상평균:10.0000   \n시도:16   행동:1    보상:10   최근20번보상평균:10.0000   \n시도:17   행동:1    보상:10   최근20번보상평균:10.0000   \n시도:18   행동:1    보상:10   최근20번보상평균:10.0000   \n시도:19   행동:1    보상:10   최근20번보상평균:10.0000   \n시도:20   행동:1    보상:10   최근20번보상평균:10.0000   \n--\n시도:21   행동:1    보상:10   최근20번보상평균:10.0000   \nGame Clear"
  },
  {
    "objectID": "posts/RL1.html#d.-수정3-agent-구현-인공지능",
    "href": "posts/RL1.html#d.-수정3-agent-구현-인공지능",
    "title": "RL1",
    "section": "D. 수정3: Agent 구현 (인공지능)",
    "text": "D. 수정3: Agent 구현 (인공지능)\n- 지금까지 풀이의 한계 - 사실 강화학습은 환경을 이해 -&gt; 행동을 결정 과정에서 -&gt;의 과정을 수식화 한 것 - 그런데 지금까지는 환경을 파악하면 인간의 지능으로 코드를 수정했으므로 기계가 스스로 생각했다고 할 수 없다.\n- Agent가 데이터를 보고 스스로 학습할 수 있도록 설계 - 부제: agent.learn()을 설계하자\n\n데이터를 모아서 q_table을 만든다.\nq_table을 바탕으로 적절한 정책(=policy)를 설정한다.\n\n\n\\((\\frac{q_0}{q_0+q_1} , \\frac{q_1}{q_0+q_1})\\)으로 생각하면 될 것 같다.\n\n- q_table을 계산하는 코드 예시\n\nagent.actions = [0, 1, 1,  0, 1,   0, 0] \nagent.rewards = [1, 9, 10, 1, 9.5, 1, 1.2] \nactions = np.array(agent.actions)\nrewards = np.array(agent.rewards)\n\n\nq0 , q1 = np.mean(rewards[actions == 0]) , np.mean(rewards[actions == 1])\n\n\nq_table = np.array([q0,q1])\nq_table\n\narray([1.05, 9.5 ])\n\n\n\nprob = q_table / q_table.sum()\nagent.action = np.random.choice(action_space , p = prob)\nagent.action\n\n0\n\n\n- 최종코드 정리\n\nclass Agent:\n    def __init__(self):\n        self.action_space = [0,1]\n        self.action = None \n        self.reward = None\n        self.actions = []\n        self.rewards = []\n        self.q_table = np.array([0.001,0.001])\n        self.n_experience = 0 \n    def act(self):\n        if self.n_experience &lt;= 20:\n            self.action = np.random.choice(self.action_space)\n        else:\n            prob = q_table / q_table.sum()\n            self.action = np.random.choice(self.action_space , p=prob)\n    def save_experience(self):\n        self.actions.append(self.action)\n        self.rewards.append(self.reward)\n        self.n_experience = self.n_experience + 1 \n    def learn(self):\n        if self.n_experience &lt; 20:\n            pass \n        else: \n            actions = np.array(self.actions)\n            rewards = np.array(self.rewards)      \n            q0,q1 = rewards[actions==0].mean(), rewards[actions==1].mean()\n            self.q_table = np.array([q0,q1])\n\n\nenv = Bandit()\nagent = Agent()\nfor t in range(1,51):\n    # step1: 행동\n    agent.act()\n    # step2: 보상\n    agent.reward = env.step(agent.action)\n    # step3: 저장 & 학습\n    agent.save_experience()\n    agent.learn()    \n    #--#\n    print(\n        f\"시도:{t}\\t\"\n        f\"행동:{agent.action}\\t\"\n        f\"보상:{agent.reward}\\t\"\n        f\"최근20번보상평균:{np.mean(agent.rewards[-20:]):.4f}\\t\"\n    )\n    if t&lt;20:\n        pass \n    elif t==20:\n        print(\"--\")\n    else: \n        if np.mean(agent.rewards[-20:]) &gt; 9.5:\n            print(\"Game Clear\")\n            break    \n\n시도:1    행동:1    보상:10   최근20번보상평균:10.0000   \n시도:2    행동:1    보상:10   최근20번보상평균:10.0000   \n시도:3    행동:1    보상:10   최근20번보상평균:10.0000   \n시도:4    행동:1    보상:10   최근20번보상평균:10.0000   \n시도:5    행동:1    보상:10   최근20번보상평균:10.0000   \n시도:6    행동:0    보상:1    최근20번보상평균:8.5000    \n시도:7    행동:1    보상:10   최근20번보상평균:8.7143    \n시도:8    행동:0    보상:1    최근20번보상평균:7.7500    \n시도:9    행동:0    보상:1    최근20번보상평균:7.0000    \n시도:10   행동:1    보상:10   최근20번보상평균:7.3000    \n시도:11   행동:0    보상:1    최근20번보상평균:6.7273    \n시도:12   행동:0    보상:1    최근20번보상평균:6.2500    \n시도:13   행동:0    보상:1    최근20번보상평균:5.8462    \n시도:14   행동:0    보상:1    최근20번보상평균:5.5000    \n시도:15   행동:0    보상:1    최근20번보상평균:5.2000    \n시도:16   행동:0    보상:1    최근20번보상평균:4.9375    \n시도:17   행동:1    보상:10   최근20번보상평균:5.2353    \n시도:18   행동:1    보상:10   최근20번보상평균:5.5000    \n시도:19   행동:0    보상:1    최근20번보상평균:5.2632    \n시도:20   행동:1    보상:10   최근20번보상평균:5.5000    \n--\n시도:21   행동:0    보상:1    최근20번보상평균:5.0500    \n시도:22   행동:1    보상:10   최근20번보상평균:5.0500    \n시도:23   행동:1    보상:10   최근20번보상평균:5.0500    \n시도:24   행동:1    보상:10   최근20번보상평균:5.0500    \n시도:25   행동:1    보상:10   최근20번보상평균:5.0500    \n시도:26   행동:1    보상:10   최근20번보상평균:5.5000    \n시도:27   행동:1    보상:10   최근20번보상평균:5.5000    \n시도:28   행동:1    보상:10   최근20번보상평균:5.9500    \n시도:29   행동:1    보상:10   최근20번보상평균:6.4000    \n시도:30   행동:1    보상:10   최근20번보상평균:6.4000    \n시도:31   행동:1    보상:10   최근20번보상평균:6.8500    \n시도:32   행동:1    보상:10   최근20번보상평균:7.3000    \n시도:33   행동:1    보상:10   최근20번보상평균:7.7500    \n시도:34   행동:1    보상:10   최근20번보상평균:8.2000    \n시도:35   행동:0    보상:1    최근20번보상평균:8.2000    \n시도:36   행동:1    보상:10   최근20번보상평균:8.6500    \n시도:37   행동:1    보상:10   최근20번보상평균:8.6500    \n시도:38   행동:1    보상:10   최근20번보상평균:8.6500    \n시도:39   행동:0    보상:1    최근20번보상평균:8.6500    \n시도:40   행동:1    보상:10   최근20번보상평균:8.6500    \n시도:41   행동:1    보상:10   최근20번보상평균:9.1000    \n시도:42   행동:1    보상:10   최근20번보상평균:9.1000    \n시도:43   행동:1    보상:10   최근20번보상평균:9.1000    \n시도:44   행동:1    보상:10   최근20번보상평균:9.1000    \n시도:45   행동:1    보상:10   최근20번보상평균:9.1000    \n시도:46   행동:1    보상:10   최근20번보상평균:9.1000    \n시도:47   행동:1    보상:10   최근20번보상평균:9.1000    \n시도:48   행동:1    보상:10   최근20번보상평균:9.1000    \n시도:49   행동:0    보상:1    최근20번보상평균:8.6500    \n시도:50   행동:1    보상:10   최근20번보상평균:8.6500"
  },
  {
    "objectID": "posts/MNIST_CIFAR10.html",
    "href": "posts/MNIST_CIFAR10.html",
    "title": "MNIST CIFAR10",
    "section": "",
    "text": "import torch\nimport torchvision\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom fastai.data.all import *\nfrom fastai.vision.all import * \nimport fastai"
  },
  {
    "objectID": "posts/MNIST_CIFAR10.html#a.-yn3---float형",
    "href": "posts/MNIST_CIFAR10.html#a.-yn3---float형",
    "title": "MNIST CIFAR10",
    "section": "A. y:(n,3) - float형",
    "text": "A. y:(n,3) - float형\n\nds = torch.utils.data.TensorDataset(X,y)\ndl = torch.utils.data.DataLoader(ds,batch_size=128)\n\nnet1 = torch.nn.Sequential(\n    torch.nn.Conv2d(1,16,(5,5)),\n    torch.nn.ReLU(),\n    torch.nn.MaxPool2d((2,2)),\n    torch.nn.Flatten()\n)\n\nnet2 = torch.nn.Sequential(\n    torch.nn.Linear(2304,3)\n)\nnet = torch.nn.Sequential(\n    net1,\n    net2\n)\nloss_fn = torch.nn.CrossEntropyLoss()\noptimizr = torch.optim.Adam(net.parameters())\nnet.to('cuda:0')\nfor epoc in range(10):\n    for xi,yi in dl:\n        loss = loss_fn(net(xi.to('cuda:0')),yi.to('cuda:0'))\n        loss.backward()\n        optimizr.step()\n        optimizr.zero_grad()\n\nnet.to('cpu')\n\nprint(f'train : {(net(X).data.argmax(axis=1) == y.argmax(axis=1)).float().mean():.4f}')\nprint(f'val : {(net(XX).data.argmax(axis=1) == yy.argmax(axis=1)).float().mean():.4f}')\n\ntrain : 0.9846\nval : 0.9914\n\n\n- 항상 하던 것."
  },
  {
    "objectID": "posts/MNIST_CIFAR10.html#b.-yn---int형",
    "href": "posts/MNIST_CIFAR10.html#b.-yn---int형",
    "title": "MNIST CIFAR10",
    "section": "B. y:(n,) - int형",
    "text": "B. y:(n,) - int형\n\nX0 = torch.stack([torchvision.io.read_image(str(fname)) for fname in list(Path('/root/.fastai/data/mnist_png/training/0').ls())])\nX1 = torch.stack([torchvision.io.read_image(str(fname)) for fname in list(Path('/root/.fastai/data/mnist_png/training/1').ls())])\nX2 = torch.stack([torchvision.io.read_image(str(fname)) for fname in list(Path('/root/.fastai/data/mnist_png/training/2').ls())])\nX = torch.concat([X0,X1,X2],axis=0)/255\ny = torch.nn.functional.one_hot(torch.tensor([0]*len(X0) + [1]*len(X1) + [2]*len(X2))).float()\nXX0 = torch.stack([torchvision.io.read_image(str(fname)) for fname in list(Path('/root/.fastai/data/mnist_png/testing/0').ls())])\nXX1 = torch.stack([torchvision.io.read_image(str(fname)) for fname in list(Path('/root/.fastai/data/mnist_png/testing/1').ls())])\nXX2 = torch.stack([torchvision.io.read_image(str(fname)) for fname in list(Path('/root/.fastai/data/mnist_png/testing/2').ls())])\nXX = torch.concat([XX0,XX1,XX2],axis=0)/255\nyy = torch.nn.functional.one_hot(torch.tensor([0]*len(XX0) + [1]*len(XX1) + [2]*len(XX2))).float()\n\n\ny = y.argmax(axis=-1)\nyy = yy.argmax(axis=-1)\n\ny와 yy를 int형으로 바꿔야하기에 argmax함수를 이용했다.\n\nprint(X.shape)\nprint(y.shape)\nprint(XX.shape)\nprint(yy.shape)\n\ntorch.Size([18623, 1, 28, 28])\ntorch.Size([18623])\ntorch.Size([3147, 1, 28, 28])\ntorch.Size([3147])\n\n\n\nds = torch.utils.data.TensorDataset(X,y)\ndl = torch.utils.data.DataLoader(ds,batch_size=128)\n\nnet1 =  torch.nn.Sequential(\n    torch.nn.Conv2d(1,16,(5,5)),\n    torch.nn.ReLU(),\n    torch.nn.MaxPool2d((2,2)),\n    torch.nn.Flatten()\n)\nnet2 = torch.nn.Sequential(\n    torch.nn.Linear(2304,3)\n)\nnet = torch.nn.Sequential(\n    net1,\n    net2\n)\nloss_fn = torch.nn.CrossEntropyLoss()\noptimizr = torch.optim.Adam(net.parameters())\nnet.to('cuda:0')\nfor epoc in range(10):\n    for xi,yi in dl:\n        # netout = net(xi.to('cuda:0'))\n        loss = loss_fn(net(xi.to('cuda:0')),yi.to('cuda:0'))\n        loss.backward()\n        optimizr.step()\n        optimizr.zero_grad()\nnet.to(\"cpu\")\nprint(f'train : {(net(X).data.argmax(axis=1) == y).float().mean():.4f}')\nprint(f'val : {(net(XX).data.argmax(axis=1) == yy).float().mean():.4f}')\n\ntrain : 0.9792\nval : 0.9857\n\n\n- 손실함수로 torch.nn.CrossEntropyLoss()를 사용하면 one_hot_encoding , float형 전처리 모두 필요없다 알아서 다 해줌\n- 받아야하는 class가 1보다 크면 CrossEntropyLoss()를 사용"
  },
  {
    "objectID": "posts/MNIST_CIFAR10.html#a.-torch",
    "href": "posts/MNIST_CIFAR10.html#a.-torch",
    "title": "MNIST CIFAR10",
    "section": "A. torch",
    "text": "A. torch\n\nds = torch.utils.data.TensorDataset(X,y)\ndl = torch.utils.data.DataLoader(ds,batch_size=128)\n\nnet1 = torch.nn.Sequential(\n    torch.nn.Conv2d(1,16,(5,5)),\n    torch.nn.ReLU(),\n    torch.nn.MaxPool2d((2,2)),\n    torch.nn.Flatten()\n)\nnet2 = torch.nn.Sequential(\n    torch.nn.Linear(2304,10)\n)\nnet = torch.nn.Sequential(\n    net1,\n    net2\n)\nloss_fn = torch.nn.CrossEntropyLoss()\noptimizr = torch.optim.Adam(net.parameters())\nnet.to('cuda:0')\nfor epoc in range(10):\n    for xi,yi in dl:\n        netout = net(xi.to('cuda:0'))\n        loss = loss_fn(netout,yi.to('cuda:0'))\n        loss.backward()\n        optimizr.step()\n        optimizr.zero_grad()\nnet.to('cpu')\n\nprint(f'train: {(net(X).data.argmax(axis=1) == y).float().mean():.4f}')\nprint(f'val: {(net(XX).data.argmax(axis=1) == yy).float().mean():.4f}')\n\ntrain: 0.9073\nval: 0.8723\n\n\n- 항상 하던 것\n- 받아야하는 class가 10개니까 torch.nn.CrossEntropyLoss() 사용하고 어자피 float형 안 맞춰도 되니까 y를 int형으로 설정"
  },
  {
    "objectID": "posts/MNIST_CIFAR10.html#b.-fastai",
    "href": "posts/MNIST_CIFAR10.html#b.-fastai",
    "title": "MNIST CIFAR10",
    "section": "B. fastai",
    "text": "B. fastai\n\n# Step1: 데이터정리 (dls생성)\nds1 = torch.utils.data.TensorDataset(X,y)\nds2 = torch.utils.data.TensorDataset(XX,yy)\ndl1 = torch.utils.data.DataLoader(ds1,batch_size=128)\ndl2 = torch.utils.data.DataLoader(ds2,batch_size=128)\ndls = fastai.data.core.DataLoaders(dl1,dl2)\n# Step2: 적합에 필요한 오브젝트 생성\nnet1 = torch.nn.Sequential(\n    torch.nn.Conv2d(1,16,(5,5)),\n    torch.nn.ReLU(),\n    torch.nn.MaxPool2d((2,2)),\n    torch.nn.Flatten()\n)\nnet2 = torch.nn.Sequential(\n    torch.nn.Linear(2304,10),\n)\nnet = torch.nn.Sequential(\n    net1, # 2d-part\n    net2, # 1d-part \n)\nloss_fn = torch.nn.CrossEntropyLoss()\n# optimizr = torch.optim.Adam(net.parameters())\n# Step3: 적합 \nlrnr = fastai.learner.Learner(\n    dls=dls,\n    model = net,\n    loss_func = loss_fn,\n    metrics = [fastai.metrics.accuracy]\n)\n\nlrnr.fit(10)\n\n# Step4: 예측 및 평가 \nlrnr.model.to('cpu')\n\nprint(f'train: {(net(X).data.argmax(axis=1) == y).float().mean():.4f}')\nprint(f'val: {(net(XX).data.argmax(axis=1) == yy).float().mean():.4f}')\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.554820\n0.531753\n0.838000\n00:01\n\n\n1\n0.393958\n0.425235\n0.858600\n00:01\n\n\n2\n0.352178\n0.419405\n0.868800\n00:01\n\n\n3\n0.317096\n0.415394\n0.873600\n00:01\n\n\n4\n0.295001\n0.433857\n0.870100\n00:01\n\n\n5\n0.279118\n0.443609\n0.870300\n00:01\n\n\n6\n0.271758\n0.461911\n0.870000\n00:01\n\n\n7\n0.262299\n0.472799\n0.866900\n00:01\n\n\n8\n0.251994\n0.489210\n0.867200\n00:01\n\n\n9\n0.247002\n0.501473\n0.868800\n00:01\n\n\n\n\n\n\ntrain: 0.9114\nval: 0.8688\n\n\n- 조금 새롭게 fastai 이용\noptimizr 사용 안 해도 된다. lrnr 새롭게 정의해서 손실함수 넣어주고 원하는 적합기준 정해주면 된다.\nfor epoc 귀찮게 길게 쓸 필요 없이 fit하면 바로 학습\nlrnr 사용하면 to.(‘cuda:0’) 사용 할 필요없이 바로 GPU로 연산해준다."
  },
  {
    "objectID": "posts/MNIST_CIFAR10.html#a.-데이터-불러오기-및-전처리",
    "href": "posts/MNIST_CIFAR10.html#a.-데이터-불러오기-및-전처리",
    "title": "MNIST CIFAR10",
    "section": "A. 데이터 불러오기 및 전처리",
    "text": "A. 데이터 불러오기 및 전처리\n\npath = fastai.data.external.untar_data(fastai.data.external.URLs.CIFAR)\npath.ls()\n\n(#3) [Path('/root/.fastai/data/cifar10/train'),Path('/root/.fastai/data/cifar10/labels.txt'),Path('/root/.fastai/data/cifar10/test')]\n\n\n\nlabels = [str(l).split('/')[-1] for l in (path/'train').ls()]\nlabels\n\n['ship',\n 'horse',\n 'bird',\n 'cat',\n 'truck',\n 'deer',\n 'frog',\n 'dog',\n 'automobile',\n 'airplane']\n\n\n\nX = torch.stack([torchvision.io.read_image(str(fname)) for l in labels for fname in (path/f'train/{l}').ls()],axis=0).float()/255\nXX = torch.stack([torchvision.io.read_image(str(fname)) for l in labels for fname in (path/f'test/{l}').ls()],axis=0).float()/255\ny = torch.tensor([i for i,l in enumerate(labels) for fname in (path/f'train/{l}').ls()])\nyy = torch.tensor([i for i,l in enumerate(labels) for fname in (path/f'test/{l}').ls()])\n\n\nprint(X.shape,'\\t',X.dtype)\nprint(y.shape,'\\t\\t\\t',y.dtype)\nprint(XX.shape,'\\t',XX.dtype)\nprint(yy.shape,'\\t\\t\\t',yy.dtype)\n\ntorch.Size([50000, 3, 32, 32])   torch.float32\ntorch.Size([50000])              torch.int64\ntorch.Size([10000, 3, 32, 32])   torch.float32\ntorch.Size([10000])              torch.int64\n\n\n\nylabel = [l for l in labels for fname in (path/f'train/{l}').ls()]\ni = 30002\nplt.imshow(torch.einsum('cij-&gt;ijc',X[i]))\nplt.title(f'{ylabel[i]},{y[i]}')\n\nText(0.5, 1.0, 'frog,6')"
  },
  {
    "objectID": "posts/MNIST_CIFAR10.html#b.-수제네트워크로-학습",
    "href": "posts/MNIST_CIFAR10.html#b.-수제네트워크로-학습",
    "title": "MNIST CIFAR10",
    "section": "B. 수제네트워크로 학습",
    "text": "B. 수제네트워크로 학습\n- shuffle 적용 전\n\n# Step1:\nds1 = torch.utils.data.TensorDataset(X,y)\nds2 = torch.utils.data.TensorDataset(XX,yy)\ndl1 = torch.utils.data.DataLoader(ds1,batch_size=256)\ndl2 = torch.utils.data.DataLoader(ds2,batch_size=100)\ndls = fastai.data.core.DataLoaders(dl1,dl2)\n# Step2:\nnet1 = torch.nn.Sequential(\n    torch.nn.Conv2d(3,16,(5,5)),\n    torch.nn.ReLU(),\n    torch.nn.MaxPool2d((2,2)),\n    torch.nn.Flatten()\n)\nnet2 = torch.nn.Sequential(\n    torch.nn.Linear(3136,10),\n)\nnet = torch.nn.Sequential(\n    net1, # 2d-part\n    net2, # 1d-part \n)\nloss_fn = torch.nn.CrossEntropyLoss()\nlrnr = fastai.learner.Learner(\n    dls=dls,\n    model=net,\n    loss_func=loss_fn,\n    #--#\n    metrics=[fastai.metrics.accuracy]\n)\n# Step3:\nlrnr.fit(10)\n# Step4: \nlrnr.model.to(\"cpu\")\nprint(f'train: {(lrnr.model(X).data.argmax(axis=1) == y).float().mean():.4f}')\nprint(f'val: {(lrnr.model(XX).data.argmax(axis=1) == yy).float().mean():.4f}')\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n2.423931\n2.302204\n0.095300\n00:01\n\n\n1\n2.060858\n7.854949\n0.100000\n00:01\n\n\n2\n2.388210\n2.297860\n0.103100\n00:01\n\n\n3\n2.465129\n2.278522\n0.103700\n00:01\n\n\n4\n2.647360\n2.273736\n0.141000\n00:01\n\n\n5\n2.334776\n2.246390\n0.158400\n00:01\n\n\n6\n2.566397\n2.217799\n0.150400\n00:01\n\n\n7\n2.194528\n3.984030\n0.105600\n00:01\n\n\n8\n2.700539\n2.256594\n0.154400\n00:01\n\n\n9\n2.189913\n3.454185\n0.127800\n00:01\n\n\n\n\n\n\ntrain: 0.1267\nval: 0.1278\n\n\n- shuffle 적용 후\n\n# Step1:\nds1 = torch.utils.data.TensorDataset(X,y)\nds2 = torch.utils.data.TensorDataset(XX,yy)\ndl1 = torch.utils.data.DataLoader(ds1,batch_size=256,shuffle=True)\ndl2 = torch.utils.data.DataLoader(ds2,batch_size=100)\ndls = fastai.data.core.DataLoaders(dl1,dl2)\n# Step2:\nnet1 = torch.nn.Sequential(\n    torch.nn.Conv2d(3,16,(5,5)),\n    torch.nn.ReLU(),\n    torch.nn.MaxPool2d((2,2)),\n    torch.nn.Flatten()\n)\nnet2 = torch.nn.Sequential(\n    torch.nn.Linear(3136,10),\n)\nnet = torch.nn.Sequential(\n    net1, # 2d-part\n    net2, # 1d-part \n)\nloss_fn = torch.nn.CrossEntropyLoss()\nlrnr = fastai.learner.Learner(\n    dls=dls,\n    model=net,\n    loss_func=loss_fn,\n    #--#\n    metrics=[fastai.metrics.accuracy]\n)\n# Step3:\nlrnr.fit(10)\n# Step4: \nlrnr.model.to(\"cpu\")\nprint(f'train: {(lrnr.model(X).data.argmax(axis=1) == y).float().mean():.4f}')\nprint(f'val: {(lrnr.model(XX).data.argmax(axis=1) == yy).float().mean():.4f}')\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n1.666294\n1.589550\n0.446000\n00:01\n\n\n1\n1.476124\n1.465711\n0.491000\n00:01\n\n\n2\n1.367271\n1.363849\n0.534000\n00:01\n\n\n3\n1.274095\n1.274507\n0.552500\n00:01\n\n\n4\n1.222440\n1.225671\n0.574500\n00:01\n\n\n5\n1.177622\n1.223371\n0.574400\n00:01\n\n\n6\n1.149504\n1.184314\n0.594200\n00:01\n\n\n7\n1.120652\n1.159286\n0.593100\n00:01\n\n\n8\n1.106153\n1.153125\n0.602100\n00:01\n\n\n9\n1.081379\n1.126705\n0.608200\n00:01\n\n\n\n\n\n\ntrain: 0.6408\nval: 0.6082\n\n\n- shuffle 하나로 이렇게 상승한다고?"
  },
  {
    "objectID": "posts/MNIST_CIFAR10.html#c.-transferlearning",
    "href": "posts/MNIST_CIFAR10.html#c.-transferlearning",
    "title": "MNIST CIFAR10",
    "section": "C. TransferLearning",
    "text": "C. TransferLearning\n- 남들이 만들어놓은 좋은 model을 가져와서 써보자\n\nnet = torchvision.models.resnet18()\nnet\n\nResNet(\n  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (relu): ReLU(inplace=True)\n  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n  (layer1): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (layer2): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (downsample): Sequential(\n        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (layer3): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (downsample): Sequential(\n        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (layer4): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (downsample): Sequential(\n        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n  (fc): Linear(in_features=512, out_features=1000, bias=True)\n)\n\n\n- 마지막만 바꾸는 건 받아야하는 class의 개수가 원래 net과 다르니 이 부분을 바꿔준다\n\nnet.fc = torch.nn.Linear(512,10)\n\n\n# Step1:\nds1 = torch.utils.data.TensorDataset(X,y)\nds2 = torch.utils.data.TensorDataset(XX,yy)\ndl1 = torch.utils.data.DataLoader(ds1,batch_size=64,shuffle=True)\ndl2 = torch.utils.data.DataLoader(ds2,batch_size=100)\ndls = fastai.data.core.DataLoaders(dl1,dl2)\n# Step2:\nnet = torchvision.models.resnet18()\nnet.fc = torch.nn.Linear(512,10)\nloss_fn = torch.nn.CrossEntropyLoss()\nlrnr = fastai.learner.Learner(\n    dls=dls,\n    model=net,\n    loss_func=loss_fn,\n    #--#\n    metrics=[fastai.metrics.accuracy]\n)\n# Step3:\nlrnr.fit(10)\n# Step4: \n\nlrnr.model.to(\"cpu\")\nprint(f'train: {(lrnr.model(X).data.argmax(axis=1) == y).float().mean():.4f}') # \nprint(f'val: {(lrnr.model(XX).data.argmax(axis=1) == yy).float().mean():.4f}')\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n1.118602\n1.336943\n0.533500\n00:12\n\n\n1\n0.947132\n1.220068\n0.598500\n00:11\n\n\n2\n0.771734\n1.346856\n0.577400\n00:11\n\n\n3\n0.699828\n1.194723\n0.614800\n00:11\n\n\n4\n0.571979\n0.946209\n0.684400\n00:11\n\n\n5\n0.506702\n0.967242\n0.682800\n00:11\n\n\n6\n0.434684\n0.887218\n0.732100\n00:11\n\n\n7\n0.338026\n0.915448\n0.733100\n00:11\n\n\n8\n0.274322\n1.014980\n0.720900\n00:11\n\n\n9\n0.243159\n0.961551\n0.741300\n00:11\n\n\n\n\n\n\ntrain: 0.9260\nval: 0.7413\n\n\n- 오버피팅이 좀 있지만 꽤 잘 맞춘다\n- 결론 : 남들이 쓰는 거 가져다가 살짝 바꿔서 쓰는 게 잘 나오긴 한다…"
  },
  {
    "objectID": "posts/DL_Summary7.html",
    "href": "posts/DL_Summary7.html",
    "title": "Deep Learning 7",
    "section": "",
    "text": "import torch\nimport pandas as pd\nimport matplotlib.pyplot as plt"
  },
  {
    "objectID": "posts/DL_Summary7.html#a.-임베딩-레이어",
    "href": "posts/DL_Summary7.html#a.-임베딩-레이어",
    "title": "Deep Learning 7",
    "section": "A. 임베딩 레이어",
    "text": "A. 임베딩 레이어\n\n`-` motive : torch.nn.functional.one_hot + torch.nn.Linear를 매번 쓰는 건 너무 귀찮은데??\n\n\ntorch.manual_seed(43052)\n#x = ['옥순','영숙','하니','옥순','영숙']\nx = torch.tensor([0,1,2,0,1])\nE = torch.nn.functional.one_hot(x).float()\nlinr = torch.nn.Linear(3,1,bias=False) \nlf = linr(E)\nlf\n\ntensor([[-0.2002],\n        [-0.4890],\n        [ 0.2081],\n        [-0.2002],\n        [-0.4890]], grad_fn=&lt;MmBackward0&gt;)\n\n\n\nlinr.weight\n\nParameter containing:\ntensor([[-0.2002, -0.4890,  0.2081]], requires_grad=True)\n\n\n\ntorch.concat([linr(E) , E @ linr.weight.T],axis=1)\n\ntensor([[-0.2002, -0.2002],\n        [-0.4890, -0.4890],\n        [ 0.2081,  0.2081],\n        [-0.2002, -0.2002],\n        [-0.4890, -0.4890]], grad_fn=&lt;CatBackward0&gt;)\n\n\n- torch.nn.functional.one_hot + torch.nn.Linear를 함께 처리해주는 레이어 torch.nn.Embedding 존재\n\ntorch.manual_seed(43052)\nebdd = torch.nn.Embedding(3,1) \nebdd.weight.data = linr.weight.data.T\nebdd(x)\n\ntensor([[-0.2002],\n        [-0.4890],\n        [ 0.2081],\n        [-0.2002],\n        [-0.4890]], grad_fn=&lt;EmbeddingBackward0&gt;)\n\n\n? 굳이 one_hot인코딩을 해야하나? 안하고 바로 linear하면 안되나?\n- linear.weight가 각각의 데이터에 개별마다 서로 다른 weight가 적용되어야하는데 one_hot인코딩을 하지 않으면 개별적으로 적용되지 않음."
  },
  {
    "objectID": "posts/DL_Summary7.html#b.-mf-based-추천시스템-재설계",
    "href": "posts/DL_Summary7.html#b.-mf-based-추천시스템-재설계",
    "title": "Deep Learning 7",
    "section": "B. MF-based 추천시스템 재설계",
    "text": "B. MF-based 추천시스템 재설계\n\ndf_view = pd.read_csv('https://raw.githubusercontent.com/guebin/DL2024/main/posts/solo.csv',index_col=0)\ndf_view\n\n\n\n\n\n\n\n\n\n영식(IN)\n영철(IN)\n영호(IS)\n광수(IS)\n상철(EN)\n영수(EN)\n규빈(ES)\n다호(ES)\n\n\n\n\n옥순(IN)\nNaN\n4.02\n3.45\n3.42\n0.84\n1.12\n0.43\n0.49\n\n\n영자(IN)\n3.93\n3.99\n3.63\n3.43\n0.98\n0.96\n0.52\nNaN\n\n\n정숙(IS)\n3.52\n3.42\n4.05\n4.06\n0.39\nNaN\n0.93\n0.99\n\n\n영숙(IS)\n3.43\n3.57\nNaN\n3.95\n0.56\n0.52\n0.89\n0.89\n\n\n순자(EN)\n1.12\nNaN\n0.59\n0.43\n4.01\n4.16\n3.52\n3.38\n\n\n현숙(EN)\n0.94\n1.05\n0.32\n0.45\n4.02\n3.78\nNaN\n3.54\n\n\n서연(ES)\n0.51\n0.56\n0.88\n0.89\n3.50\n3.64\n4.04\n4.10\n\n\n보람(ES)\n0.48\n0.51\n1.03\nNaN\n3.52\n4.00\n3.82\nNaN\n\n\n하니(I)\n4.85\n4.82\nNaN\n4.98\n4.53\n4.39\n4.45\n4.52\n\n\n\n\n\n\n\n\n\ndf_train = df_view.stack().reset_index().set_axis(['W','M','y'],axis=1)\nw = {'옥순(IN)':0, '영자(IN)':1, '정숙(IS)':2, '영숙(IS)':3, '순자(EN)':4, '현숙(EN)':5, '서연(ES)':6, '보람(ES)':7, '하니(I)':8}\nm = {'영식(IN)':0, '영철(IN)':1, '영호(IS)':2, '광수(IS)':3, '상철(EN)':4, '영수(EN)':5, '규빈(ES)':6, '다호(ES)':7}\nX1 = torch.tensor(df_train['W'].map(w)) # length-n int vector \nX2 = torch.tensor(df_train['M'].map(m)) # length-n int vector \ny = torch.tensor(df_train['y']).float().reshape(-1,1) # (n,1) float vector\n\n임베딩레이어를 활용하여 MF-based 추천시스템을 설계하라.\n\ntorch.manual_seed(43052)\nebdd1 = torch.nn.Embedding(9,2)\nb1 = torch.nn.Embedding(9,1)\nebdd2 = torch.nn.Embedding(8,2)\nb2 = torch.nn.Embedding(8,1)\nsig = torch.nn.Sigmoid()\nloss_fn = torch.nn.MSELoss()\nparams = list(ebdd1.parameters()) + list(b1.parameters()) + list(ebdd2.parameters()) + list(b2.parameters())\noptimizr = torch.optim.Adam(params , lr=0.1)\n#--#\nfor epoc in range(100):\n    # 1\n    W_features = ebdd1(X1)\n    W_bias = b1(X1)\n    M_features = ebdd2(X2)\n    M_bias = b2(X2)\n    score = (W_features * M_features).sum(axis=1).reshape(-1,1) + W_bias + M_bias\n    yhat = sig(score) * 5\n    # 2\n    loss = loss_fn(yhat,y)\n    # 3\n    loss.backward()\n    # 4\n    optimizr.step()\n    optimizr.zero_grad()\n\n\ntorch.concat([yhat,y],axis=1)[::4]\n\ntensor([[4.1083, 4.0200],\n        [0.9388, 1.1200],\n        [4.0483, 3.9900],\n        [0.9707, 0.9600],\n        [4.2264, 4.0500],\n        [0.9518, 0.9900],\n        [0.5124, 0.5600],\n        [1.1198, 1.1200],\n        [4.0588, 4.1600],\n        [1.0596, 1.0500],\n        [3.9666, 3.7800],\n        [0.9472, 0.8800],\n        [3.9194, 4.0400],\n        [1.0346, 1.0300],\n        [4.8851, 4.8500],\n        [4.5387, 4.3900]], grad_fn=&lt;SliceBackward0&gt;)\n\n\n- Nan값들을 추출하고 싶다면? 아래와 같이 진행\n\nw = {'옥순(IN)':0, '영자(IN)':1, '정숙(IS)':2, '영숙(IS)':3, '순자(EN)':4, '현숙(EN)':5, '서연(ES)':6, '보람(ES)':7, '하니(I)':8}\nm = {'영식(IN)':0, '영철(IN)':1, '영호(IS)':2, '광수(IS)':3, '상철(EN)':4, '영수(EN)':5, '규빈(ES)':6, '다호(ES)':7}\ndf_test = pd.DataFrame({'W':['옥순(IN)','보람(ES)','하니(I)'],'M':['영식(IN)','다호(ES)','영호(IS)']})\nXX1 = torch.tensor(df_test['W'].map(w))\nXX2 = torch.tensor(df_test['M'].map(m))\n\nebdd1 = torch.nn.Embedding(9,2)\nb1 = torch.nn.Embedding(9,1)\nebdd2 = torch.nn.Embedding(8,2)\nb2 = torch.nn.Embedding(8,1)\n\nW_features = ebdd1(XX1)\nW_bias = b1(XX1)\nM_features = ebdd2(XX2)\nM_bias = b2(XX2)\nscore = (W_features * M_features).sum(axis=1).reshape(-1,1) + W_bias + M_bias\nyhat = sig(score) * 5\nyhat\n\ntensor([[1.8270],\n        [2.6058],\n        [4.5784]], grad_fn=&lt;MulBackward0&gt;)"
  },
  {
    "objectID": "posts/DL_Summary7.html#a.-사용자-정의-네트워크-사용법",
    "href": "posts/DL_Summary7.html#a.-사용자-정의-네트워크-사용법",
    "title": "Deep Learning 7",
    "section": "A. 사용자 정의 네트워크 사용법",
    "text": "A. 사용자 정의 네트워크 사용법\n- 예비학습1 : net(x)와 net.forward(x)는 사실 같다.\n\nnet = torch.nn.Sequential(\n    torch.nn.Linear(1,1),\n    torch.nn.Sigmoid()\n)\n\n\nX = torch.randn(5,1)\nX\n\ntensor([[-1.2804],\n        [-0.0382],\n        [-0.8600],\n        [-1.3291],\n        [ 0.0696]])\n\n\n\nnet(X)\n\ntensor([[0.1890],\n        [0.3183],\n        [0.2277],\n        [0.1849],\n        [0.3315]], grad_fn=&lt;SigmoidBackward0&gt;)\n\n\n\nnet.forward(X)\n\ntensor([[0.1890],\n        [0.3183],\n        [0.2277],\n        [0.1849],\n        [0.3315]], grad_fn=&lt;SigmoidBackward0&gt;)\n\n\n\nnet.forward = lambda x : '메롱'\n\n\nnet.forward(X)\n\n'메롱'\n\n\n\nnet(X)\n\n'메롱'\n\n\n- 예비학습2 : torch.nn.Module을 상속받아서 네트워크를 만들면 (= class XXX(torch.nn.Module): 과 같은 방식으로 클래스를 선언하면) 약속된 아키텍처를 가진 네트워크를 찍어내는 함수를 만들 수 있다.\n예시1)\n\nclass Mynet1(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.l1 = torch.nn.Linear(1,1)\n        self.a1 = torch.nn.Sigmoid()\n        self.l2 = torch.nn.Linear(1,1)\n    def forward(self,x):\n        yhat = self.l2(self.al(self.l1(x)))\n        return yhat\n\nnet을 내가 만든 네트워크로 받아주기\n\nnet = Mynet1()\n\n예시2)\n\nclass Mynet2(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.l1 = torch.nn.Linear(in_features=1,out_features=1,bias=True)\n        self.a1 = torch.nn.ReLU()\n        self.l2 = torch.nn.Linear(in_features=1,out_features=1,bias=False)\n    def forward(self,x):\n        yhat = self.l2(self.a1(self.l1(x)))\n        return yhat\n\n\nnet = Mynet2()\n\n실습)\n\ntorch.manual_seed(43052)\nx,_ = torch.randn(100).sort()\nx = x.reshape(-1,1)\nϵ = torch.randn(100).reshape(-1,1)*0.5\ny = 2.5+ 4*x + ϵ\n\n\nplt.plot(x,y,'o')\n\n\n\n\n\n\n\n\n\nclass Net(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.l1 = torch.nn.Linear(1,1)\n    def forward(self,x):\n        yhat = self.l1(x)\n        return yhat\n\n\nplt.plot(x,y,'o')\nplt.plot(x,net(x).data,'--') # 최초의 직선\n\n\n\n\n\n\n\n\n\nnet = Net()\nloss_fn = torch.nn.MSELoss()\noptimizr = torch.optim.Adam(net.parameters(),lr=0.1)\n\nfor epoc in range(100):\n    yhat = net(x)\n    loss = loss_fn(yhat,y)\n    loss.backward()\n    optimizr.step()\n    optimizr.zero_grad()\n\n\nplt.plot(x,y,'o')\nplt.plot(x,net(x).data,'--') # 최초의 직선"
  },
  {
    "objectID": "posts/DL_Summary7.html#b.-mf-based-추천시스템-재설계-1",
    "href": "posts/DL_Summary7.html#b.-mf-based-추천시스템-재설계-1",
    "title": "Deep Learning 7",
    "section": "B. MF-based 추천시스템 재설계",
    "text": "B. MF-based 추천시스템 재설계\n\ndf_view = pd.read_csv('https://raw.githubusercontent.com/guebin/DL2024/main/posts/solo.csv',index_col=0)\ndf_view\n\n\n\n\n\n\n\n\n\n영식(IN)\n영철(IN)\n영호(IS)\n광수(IS)\n상철(EN)\n영수(EN)\n규빈(ES)\n다호(ES)\n\n\n\n\n옥순(IN)\nNaN\n4.02\n3.45\n3.42\n0.84\n1.12\n0.43\n0.49\n\n\n영자(IN)\n3.93\n3.99\n3.63\n3.43\n0.98\n0.96\n0.52\nNaN\n\n\n정숙(IS)\n3.52\n3.42\n4.05\n4.06\n0.39\nNaN\n0.93\n0.99\n\n\n영숙(IS)\n3.43\n3.57\nNaN\n3.95\n0.56\n0.52\n0.89\n0.89\n\n\n순자(EN)\n1.12\nNaN\n0.59\n0.43\n4.01\n4.16\n3.52\n3.38\n\n\n현숙(EN)\n0.94\n1.05\n0.32\n0.45\n4.02\n3.78\nNaN\n3.54\n\n\n서연(ES)\n0.51\n0.56\n0.88\n0.89\n3.50\n3.64\n4.04\n4.10\n\n\n보람(ES)\n0.48\n0.51\n1.03\nNaN\n3.52\n4.00\n3.82\nNaN\n\n\n하니(I)\n4.85\n4.82\nNaN\n4.98\n4.53\n4.39\n4.45\n4.52\n\n\n\n\n\n\n\n\n\ndf_train = df_view.stack().reset_index().set_axis(['W','M','y'],axis=1)\nw = {'옥순(IN)':0, '영자(IN)':1, '정숙(IS)':2, '영숙(IS)':3, '순자(EN)':4, '현숙(EN)':5, '서연(ES)':6, '보람(ES)':7, '하니(I)':8}\nm = {'영식(IN)':0, '영철(IN)':1, '영호(IS)':2, '광수(IS)':3, '상철(EN)':4, '영수(EN)':5, '규빈(ES)':6, '다호(ES)':7}\nX1 = torch.tensor(df_train['W'].map(w)) # length-n int vector \nX2 = torch.tensor(df_train['M'].map(m)) # length-n int vector \ny = torch.tensor(df_train['y']).float().reshape(-1,1) # (n,1) float vector\n\n사용자 정의 네트워크를 이용하여 MF-based 추천시스템을 설계하라\n- 풀이1\n\nclass Net(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.ebdd1 = torch.nn.Embedding(9,2)\n        self.ebdd2 = torch.nn.Embedding(8,2)\n        self.b1 = torch.nn.Embedding(9,1)\n        self.b2 = torch.nn.Embedding(8,1)\n        self.sig = torch.nn.Sigmoid()\n    def forward(self,X1,X2):\n        W_features = self.ebdd1(X1)\n        M_features = self.ebdd2(X2)\n        W_bias = self.b1(X1)\n        M_bias = self.b2(X2)\n        score = (W_features * M_features).sum(axis=1).reshape(-1,1) + W_bias + M_bias\n        yhat = sig(score) * 5\n        return yhat\n\n\nnet = Net()\nloss_fn = torch.nn.MSELoss()\noptimizr = torch.optim.Adam(net.parameters(),lr=0.1)\n\nfor epoc in range(100):\n    yhat = net(X1,X2)\n    \n    loss = loss_fn(yhat,y)\n    \n    loss.backward()\n    \n    optimizr.step()\n    optimizr.zero_grad()\n\n\ntorch.concat([yhat.data , y],axis=1)[::5]\n\ntensor([[4.0761, 4.0200],\n        [0.5779, 0.4300],\n        [3.5008, 3.4300],\n        [3.4501, 3.4200],\n        [0.9958, 0.9900],\n        [0.7196, 0.5200],\n        [0.4822, 0.4300],\n        [0.8808, 0.9400],\n        [3.9193, 3.7800],\n        [0.8878, 0.8900],\n        [0.5020, 0.4800],\n        [4.0130, 3.8200],\n        [4.7589, 4.3900]])\n\n\n- 풀이2\n\nX = torch.stack([X1,X2],axis=1)\nX[:5]\n\ntensor([[0, 1],\n        [0, 2],\n        [0, 3],\n        [0, 4],\n        [0, 5]])\n\n\n\nX[:,0], X[:,1]\n\n(tensor([0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3,\n         3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6,\n         6, 6, 7, 7, 7, 7, 7, 7, 8, 8, 8, 8, 8, 8, 8]),\n tensor([1, 2, 3, 4, 5, 6, 7, 0, 1, 2, 3, 4, 5, 6, 0, 1, 2, 3, 4, 6, 7, 0, 1, 3,\n         4, 5, 6, 7, 0, 2, 3, 4, 5, 6, 7, 0, 1, 2, 3, 4, 5, 7, 0, 1, 2, 3, 4, 5,\n         6, 7, 0, 1, 2, 4, 5, 6, 0, 1, 3, 4, 5, 6, 7]))\n\n\n\nclass Net(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.ebdd1 = torch.nn.Embedding(9,2)\n        self.ebdd2 = torch.nn.Embedding(8,2)\n        self.b1 = torch.nn.Embedding(9,1)\n        self.b2 = torch.nn.Embedding(8,1)\n        self.sig = torch.nn.Sigmoid()\n    def forward(self,X):\n        X1,X2 = X[:,0],X[:,1]\n        W_features = self.ebdd1(X1)\n        M_features = self.ebdd2(X2)\n        W_bias = self.b1(X1)\n        M_bias = self.b2(X2)\n        score = (W_features * M_features).sum(axis=1).reshape(-1,1) + W_bias + M_bias\n        yhat = sig(score) * 5\n        return yhat\n\n\nnet = Net()\nloss_fn = torch.nn.MSELoss()\noptimizr = torch.optim.Adam(net.parameters(),lr=0.1) # 이게 편해요!!\n#--# \nfor epoc in range(100):\n    # 1\n    yhat = net(X) \n    # 2\n    loss = loss_fn(yhat,y)\n    # 3 \n    loss.backward()\n    # 4 \n    optimizr.step()\n    optimizr.zero_grad()\n\n\ntorch.concat([yhat.data,y],axis=1)[::5]\n\ntensor([[4.0690, 4.0200],\n        [0.6189, 0.4300],\n        [3.5540, 3.4300],\n        [3.4387, 3.4200],\n        [0.9671, 0.9900],\n        [0.6642, 0.5200],\n        [0.5653, 0.4300],\n        [0.0233, 0.9400],\n        [3.8030, 3.7800],\n        [0.7846, 0.8900],\n        [0.5607, 0.4800],\n        [3.8352, 3.8200],\n        [4.9609, 4.3900]])"
  },
  {
    "objectID": "posts/DL_Summary7.html#a.-nn-based-방식",
    "href": "posts/DL_Summary7.html#a.-nn-based-방식",
    "title": "Deep Learning 7",
    "section": "A. NN-based 방식",
    "text": "A. NN-based 방식\n\ndf_view = pd.read_csv('https://raw.githubusercontent.com/guebin/DL2024/main/posts/solo.csv',index_col=0)\ndf_view\n\n\n\n\n\n\n\n\n\n영식(IN)\n영철(IN)\n영호(IS)\n광수(IS)\n상철(EN)\n영수(EN)\n규빈(ES)\n다호(ES)\n\n\n\n\n옥순(IN)\nNaN\n4.02\n3.45\n3.42\n0.84\n1.12\n0.43\n0.49\n\n\n영자(IN)\n3.93\n3.99\n3.63\n3.43\n0.98\n0.96\n0.52\nNaN\n\n\n정숙(IS)\n3.52\n3.42\n4.05\n4.06\n0.39\nNaN\n0.93\n0.99\n\n\n영숙(IS)\n3.43\n3.57\nNaN\n3.95\n0.56\n0.52\n0.89\n0.89\n\n\n순자(EN)\n1.12\nNaN\n0.59\n0.43\n4.01\n4.16\n3.52\n3.38\n\n\n현숙(EN)\n0.94\n1.05\n0.32\n0.45\n4.02\n3.78\nNaN\n3.54\n\n\n서연(ES)\n0.51\n0.56\n0.88\n0.89\n3.50\n3.64\n4.04\n4.10\n\n\n보람(ES)\n0.48\n0.51\n1.03\nNaN\n3.52\n4.00\n3.82\nNaN\n\n\n하니(I)\n4.85\n4.82\nNaN\n4.98\n4.53\n4.39\n4.45\n4.52\n\n\n\n\n\n\n\n\n\ndf_train = df_view.stack().reset_index().set_axis(['W','M','y'],axis=1)\nw = {'옥순(IN)':0, '영자(IN)':1, '정숙(IS)':2, '영숙(IS)':3, '순자(EN)':4, '현숙(EN)':5, '서연(ES)':6, '보람(ES)':7, '하니(I)':8}\nm = {'영식(IN)':0, '영철(IN)':1, '영호(IS)':2, '광수(IS)':3, '상철(EN)':4, '영수(EN)':5, '규빈(ES)':6, '다호(ES)':7}\nX1 = torch.tensor(df_train['W'].map(w)) # length-n int vector \nX2 = torch.tensor(df_train['M'].map(m)) # length-n int vector \ny = torch.tensor(df_train['y']).float().reshape(-1,1) # (n,1) float vector\n\nNN-based 추천시스템을 설계하라\n(풀이1) - 실패\n- W_feature,M_feature,W_bias,M_bias을 모두 concat해서 (nx6)행렬로 만든 후에 linear를 태워볼까?\n\nclass Net(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        #--#\n        self.ebdd1 = torch.nn.Embedding(9,2)\n        self.ebdd2 = torch.nn.Embedding(8,2)\n        self.b1 = torch.nn.Embedding(9,1)\n        self.b2 = torch.nn.Embedding(8,1)\n        self.mlp = torch.nn.Sequential(\n            torch.nn.Linear(6,1),\n            torch.nn.Sigmoid()\n        )   \n    def forward(self,X1,X2):\n        W_feature = self.ebdd1(X1)\n        M_feature = self.ebdd2(X2)\n        W_bias = self.b1(X1)\n        M_bias = self.b2(X2)\n        Z = torch.concat([W_feature,M_feature,W_bias,M_bias],axis=1)\n        yhat = self.mlp(Z) * 5 \n        return yhat\n\n\nnet = Net()\nloss_fn = torch.nn.MSELoss()\noptimizr = torch.optim.Adam(net.parameters(),lr=0.1) # 이게 편해요!!\n#--# \nfor epoc in range(1000):\n    # 1\n    yhat = net(X1,X2) \n    # 2\n    loss = loss_fn(yhat,y)\n    # 3 \n    loss.backward()\n    # 4 \n    optimizr.step()\n    optimizr.zero_grad()\n\n\ntorch.concat([yhat.data,y],axis=1)[::5]\n\ntensor([[2.1565, 4.0200],\n        [1.6908, 0.4300],\n        [2.6466, 3.4300],\n        [2.7758, 3.4200],\n        [2.5814, 0.9900],\n        [2.3105, 0.5200],\n        [2.6402, 0.4300],\n        [1.7231, 0.9400],\n        [2.3893, 3.7800],\n        [2.4106, 0.8900],\n        [1.9774, 0.4800],\n        [2.0164, 3.8200],\n        [4.7447, 4.3900]])\n\n\n\n안 맞네..?\n\n(풀이2) - 모르겠다… 깊은 신경망 써보자\n\nclass Net(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        #--#\n        self.ebdd1 = torch.nn.Embedding(9,2)\n        self.ebdd2 = torch.nn.Embedding(8,2)\n        self.b1 = torch.nn.Embedding(9,1)\n        self.b2 = torch.nn.Embedding(8,1)\n        self.mlp = torch.nn.Sequential(\n            torch.nn.Linear(6,15),\n            torch.nn.ReLU(),\n            torch.nn.Linear(15,15),\n            torch.nn.ReLU(),\n            torch.nn.Linear(15,1),\n            torch.nn.Sigmoid()\n        )   \n    def forward(self,X1,X2):\n        W_feature = self.ebdd1(X1)\n        M_feature = self.ebdd2(X2)\n        W_bias = self.b1(X1)\n        M_bias = self.b2(X2)\n        Z = torch.concat([W_feature,M_feature,W_bias,M_bias],axis=1)\n        yhat = self.mlp(Z) * 5 \n        return yhat\n\n\ntorch.manual_seed(43052)\nnet = Net()\nloss_fn = torch.nn.MSELoss()\noptimizr = torch.optim.Adam(net.parameters(),lr=0.1) # 이게 편해요!!\n#--# \nfor epoc in range(1000):\n    # 1\n    yhat = net(X1,X2) \n    # 2\n    loss = loss_fn(yhat,y)\n    # 3 \n    loss.backward()\n    # 4 \n    optimizr.step()\n    optimizr.zero_grad()\n\n\ntorch.concat([yhat.data,y],axis=1)[::5]\n\ntensor([[4.0304, 4.0200],\n        [0.4593, 0.4300],\n        [3.4378, 3.4300],\n        [3.4342, 3.4200],\n        [1.0066, 0.9900],\n        [0.5503, 0.5200],\n        [0.4410, 0.4300],\n        [0.9358, 0.9400],\n        [3.8641, 3.7800],\n        [0.9078, 0.8900],\n        [0.4841, 0.4800],\n        [3.8569, 3.8200],\n        [4.4065, 4.3900]])\n\n\n- 잘 맞추는데 오버피팅 아니야?\n\ndf_view\n\n\n\n\n\n\n\n\n\n영식(IN)\n영철(IN)\n영호(IS)\n광수(IS)\n상철(EN)\n영수(EN)\n규빈(ES)\n다호(ES)\n\n\n\n\n옥순(IN)\nNaN\n4.02\n3.45\n3.42\n0.84\n1.12\n0.43\n0.49\n\n\n영자(IN)\n3.93\n3.99\n3.63\n3.43\n0.98\n0.96\n0.52\nNaN\n\n\n정숙(IS)\n3.52\n3.42\n4.05\n4.06\n0.39\nNaN\n0.93\n0.99\n\n\n영숙(IS)\n3.43\n3.57\nNaN\n3.95\n0.56\n0.52\n0.89\n0.89\n\n\n순자(EN)\n1.12\nNaN\n0.59\n0.43\n4.01\n4.16\n3.52\n3.38\n\n\n현숙(EN)\n0.94\n1.05\n0.32\n0.45\n4.02\n3.78\nNaN\n3.54\n\n\n서연(ES)\n0.51\n0.56\n0.88\n0.89\n3.50\n3.64\n4.04\n4.10\n\n\n보람(ES)\n0.48\n0.51\n1.03\nNaN\n3.52\n4.00\n3.82\nNaN\n\n\n하니(I)\n4.85\n4.82\nNaN\n4.98\n4.53\n4.39\n4.45\n4.52\n\n\n\n\n\n\n\n\n\nXX1 = torch.tensor([0,1,8])\nXX2 = torch.tensor([1,7,2])\n\n\nnet(XX1,XX2)\n\ntensor([[4.0098],\n        [0.5043],\n        [4.9727]], grad_fn=&lt;MulBackward0&gt;)\n\n\n잘 맞추네..오버피팅 아닌듯"
  }
]